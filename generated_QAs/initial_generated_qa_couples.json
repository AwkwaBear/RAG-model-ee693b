[
    {
        "context": "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Allow users to submit with enter in Interfaces with textbox / number inputs [@aliabid94](https://github.com/aliabid94) in [PR 4090](https://github.com/gradio-app/gradio/pull/4090).\n- Updates gradio's requirements.txt to requires uvicorn>=0.14.0 by [@abidlabs](https://github.com/abidlabs) in [PR 4086](https://github.com/gradio-app/gradio/pull/4086)\n- Updates some error messaging by [@abidlabs](https://github.com/abidlabs) in [PR 4086](https://github.com/gradio-app/gradio/pull/4086)\n- Renames simplified Chinese translation file from `zh-cn.json` to `zh-CN.json` by [@abidlabs](https://github.com/abidlabs) in [PR 4086](https://github.com/gradio-app/gradio/pull/4086)\n\n### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.28.3\n\n### New Features:\n\nNo changes to highlight.\n\n### Bug Fixes:\n\n- Fixes issue with indentation in `gr.Code()` component with streaming by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4043](https://github.com/gradio-app/gradio/pull/4043)\n\n### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.28.2\n\n### Bug Fixes\n\n- Code component visual updates by [@pngwn](https://github.com/pngwn) in [PR 4051](https://github.com/gradio-app/gradio/pull/4051)\n\n### New Features:",
        "question": "What was fixed in the Code component in version 3.28.2?\n",
        "answer": "The Code component had visual updates in version 3.28.2.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "Let's reuse our example above:\n\n```py\n>>> from huggingface_hub import get_collection, update_collection_item\n\n# Fetch collection\n>>> collection_slug = \"osanseviero/os-week-highlights-sept-18-24-650bfed7f795a59f491afb80\"\n>>> collection = get_collection(collection_slug)\n\n# Reorder to place the two `Wuerstchen` items together\n>>> update_collection_item(\n...     collection_slug=collection_slug,\n...     item_object_id=collection.items[3].item_object_id,\n...     position=2,\n... )\n```\n\n### Remove items\n\nFinally, you can also remove an item using [`delete_collection_item`].\n\n```py\n>>> from huggingface_hub import get_collection, update_collection_item\n\n# Fetch collection\n>>> collection_slug = \"osanseviero/os-week-highlights-sept-18-24-650bfed7f795a59f491afb80\"\n>>> collection = get_collection(collection_slug)\n\n# Remove `coqui/xtts` Space from the list\n>>> delete_collection_item(collection_slug=collection_slug, item_object_id=collection.items[0].item_object_id)\n```\n\n## Delete collection\n\nA collection can be deleted using [`delete_collection`].\n\n<Tip warning={true}>\n\nThis is a non-revertible action. A deleted collection cannot be restored.\n\n</Tip>\n\n```py\n>>> from huggingface_hub import delete_collection\n>>> collection = delete_collection(\"username/useless-collection-64f9a55bb3115b4f513ec026\", missing_ok=True)\n```",
        "question": "What is the function name to delete a collection item in Hugging Face Hub API?\n",
        "answer": "The function name to delete a collection item in Hugging Face Hub API is `delete_collection_item`.",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/collections.md"
    },
    {
        "context": "You can easily run `pipeline` on large models using 🤗 `accelerate`! First make sure you have installed `accelerate` with `pip install accelerate`. \n\nFirst load your model using `device_map=\"auto\"`! We will use `facebook/opt-1.3b` for our example.\n\n```py\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n```\n\nYou can also pass 8-bit loaded models if you install `bitsandbytes` and add the argument `load_in_8bit=True`\n\n```py\n# pip install accelerate bitsandbytes\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n```\n\nNote that you can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM!",
        "question": "How can I run a pipeline on a large model using 🤗 `accelerate`?\n",
        "answer": "First, make sure you have installed `accelerate` with `pip install accelerate`. Then, load your model using `device_map=\"auto\"` and pass the `torch_dtype=torch.bfloat16` argument to the `pipeline` function. You can also pass 8-bit loaded models if you install `bitsandbytes` and add the argument `load_in_8bit=True` to the `model_kwargs` parameter.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md"
    },
    {
        "context": "Gradio Demo: blocks_scroll\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    inp = gr.Textbox(placeholder=\"Enter text.\")\n    scroll_btn = gr.Button(\"Scroll\")\n    no_scroll_btn = gr.Button(\"No Scroll\")\n    big_block = gr.HTML(\"\"\"\n    <div style='height: 800px; width: 100px; background-color: pink;'></div>\n    \"\"\")\n    out = gr.Textbox()\n    \n    scroll_btn.click(lambda x: x, \n               inputs=inp, \n               outputs=out,\n                scroll_to_output=True)\n    no_scroll_btn.click(lambda x: x, \n               inputs=inp, \n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```",
        "question": "How do I scroll to the output in the Gradio demo?\n",
        "answer": "To scroll to the output in the Gradio demo, click the \"Scroll\" button.",
        "source_doc": "gradio-app/gradio/blob/main/demo/blocks_scroll/run.ipynb"
    },
    {
        "context": "TResNet\n\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, [Anti-Alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), In-Place Activated BatchNorm, Blocks selection and [squeeze-and-excitation layers](https://paperswithcode.com/method/squeeze-and-excitation-block).\n\n## How do I use this model on an image?\n\nTo load a pretrained model:\n\n```py\n>>> import timm\n>>> model = timm.create_model('tresnet_l', pretrained=True)\n>>> model.eval()\n```\n\nTo load and preprocess the image:\n\n```py \n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve_data_config\n>>> from timm.data.transforms_factory import create_transform\n\n>>> config = resolve_data_config({}, model=model)\n>>> transform = create_transform(**config)\n\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]",
        "question": "What is the name of the variant of ResNet called TResNet?\n",
        "answer": "TResNet",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/tresnet.mdx"
    },
    {
        "context": "`@gradio/button`\n\n```html\n<script>\n\timport { BaseChatBot } from \"@gradio/chatbot\";\n</script>\n```\n\n\nBaseChatBot\n```javascript\n\texport let value:\n\t\t| [\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null,\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null\n\t\t  ][]\n\t\t| null;\n\tlet old_value:\n\t\t| [\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null,\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null\n\t\t  ][]\n\t\t| null = null;\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[];\n\texport let pending_message = false;\n\texport let selectable = false;\n\texport let likeable = false;\n\texport let show_share_button = false;\n\texport let rtl = false;\n\texport let show_copy_button = false;\n\texport let avatar_images: [string | null, string | null] = [null, null];\n\texport let sanitize_html = true;\n\texport let bubble_full_width = true;\n\texport let render_markdown = true;\n\texport let line_breaks = true;\n\texport let root: string;\n\texport let root_url: null | string;\n\texport let i18n: I18nFormatter;\n\texport let layout: \"bubble\" | \"panel\" = \"bubble\";\n```",
        "question": "What is the default value of `bubble_full_width` in the `BaseChatBot` class?\n",
        "answer": "The default value of `bubble_full_width` in the `BaseChatBot` class is `true`.",
        "source_doc": "gradio-app/gradio/blob/main/js/chatbot/README.md"
    },
    {
        "context": "If you want to see how to load a specific model, you can click `Use in Adapter Transformers` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-transformers_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-transformers_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\nAt the moment there is no automatic method to upload your models to the Hub, but the process to upload them is documented in the [official guide](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md#share-your-models).\n\nAll the recipes create all the needed files to upload a model to the Hub. The process usually involves the following steps:\n1. Create and clone a model repository.\n2. Moving files from the recipe output to the repository (model card, model filte, TensorBoard traces).\n3. Push the files (`git add` + `git commit` + `git push`).\n\nOnce you do this, you can try out your model directly in the browser and share it with the rest of the community.\n\n## Additional resources\n\n* Asteroid [website](https://asteroid-team.github.io/).\n* Asteroid [library](https://github.com/asteroid-team/asteroid).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).",
        "question": "How can I upload my models to the Hub?\n",
        "answer": "The process to upload models to the Hub is documented in the official guide, which involves creating and cloning a model repository, moving files from the recipe output to the repository, and pushing the files using git commands.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/asteroid.md"
    },
    {
        "context": "Here's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.",
        "question": "What is the name of the csv file where the flagged data is stored?\n",
        "answer": "logs.csv",
        "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/using-flagging.md"
    },
    {
        "context": "--\ntitle: \"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\"\nthumbnail: /blog/assets/129_intel_sapphire_rapids_inference/01.png\nauthors:\n- user: juliensimon\n---\n\n# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2\n\n\nIn a [recent post](https://huggingface.co/blog/intel-sapphire-rapids), we introduced you to the fourth generation of Intel Xeon CPUs, code-named [Sapphire Rapids](https://en.wikipedia.org/wiki/Sapphire_Rapids), and its new Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) instruction set. Combining a cluster of Sapphire Rapids servers running on Amazon EC2 and Intel libraries like the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), we showed you how to efficiently run distributed training at scale, achieving an 8-fold speedup compared to the previous Xeon generation (Ice Lake) with near-linear scaling.\n\nIn this post, we're going to focus on inference. Working with popular HuggingFace transformers implemented with PyTorch, we'll first measure their performance on an Ice Lake server for short and long NLP token sequences. Then, we'll do the same with a Sapphire Rapids server and the latest version of Hugging Face [Optimum Intel](https://github.com/huggingface/optimum-intel), an open-source library dedicated to hardware acceleration for Intel platforms.\n\nLet's get started!\n\n\n## Why You Should Consider CPU-based Inference\n\nThere are several factors to consider when deciding whether to run deep learning inference on a CPU or GPU. The most important one is certainly the size of the model. In general, larger models may benefit more from the additional computational power provided by a GPU, while smaller models can run efficiently on a CPU.",
        "question": "What is the most important factor to consider when deciding whether to run deep learning inference on a CPU or GPU?\n",
        "answer": "The most important factor to consider when deciding whether to run deep learning inference on a CPU or GPU is the size of the model. In general, larger models may benefit more from the additional computational power provided by a GPU, while smaller models can run efficiently on a CPU.",
        "source_doc": "huggingface/blog/blob/main/intel-sapphire-rapids-inference.md"
    },
    {
        "context": "![snippet](assets/32_1b_sentence_embeddings/model.png)\n\n### Multiple Negative Ranking Loss\n\nThe parameters from the composition module are usually learned using a self-supervised objective. For the project, we used a contrastive training method illustrated in the figure below. We constitute a dataset with sentence pairs \\\\( (a_i, p_i) \\\\) such that sentences from the pair have a close meaning. For example, we consider pairs such as (query, answer-passage), (question, duplicate_question),(paper title, cited paper title). Our model is then trained to map pairs \\\\( (a_i , p_i) \\\\) to close vectors while assigning unmatched pairs \\\\( (a_i , p_j), i \\neq j \\\\) to distant vectors in the embedding space. This training method is also called training with in-batch negatives, InfoNCE or NTXentLoss.\n\n![snippet](assets/32_1b_sentence_embeddings/contrastive_1.png)\n\nFormally, given a batch of training samples, the model optimises the following [loss function](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py):\n\n$$-\\frac{1}{n}\\sum_{i=1}^n\\frac{exp(sim(a_i, p_i))}{\\sum_j exp(sim(a_i, p_j))}$$\n\nAn illustrative example can be seen below. The model first embeds each sentence from every pair in the batch. Then, we compute a similarity matrix between every possible pair \\\\( (a_i, p_j) \\\\). We then compare the similarity matrix with the ground truth, which indicates the original pairs. Finally, we perform the comparison using the cross entropy loss.\n\nIntuitively, the model should assign high similarity to the sentences « How many people live in Berlin? » and « Around 3.5 million people live in Berlin » and low similarity to other negative answers such as « The capital of France is Paris » as detailed in the Figure below.\n\n![snippet](assets/32_1b_sentence_embeddings/contrastive_2.png)",
        "question": "What is the loss function used in the contrastive training method?\n",
        "answer": "The loss function used in the contrastive training method is given by the formula: -1/n * Σ^n (exp(sim(a_i, p_i)) / Σ^n exp(sim(a_i, p_j))), where n is the number of training samples, a_i and p_i are the sentences from the pair, and sim is the similarity function.",
        "source_doc": "huggingface/blog/blob/main/1b-sentence-embeddings.md"
    },
    {
        "context": "prior_model_id = \"kakaobrain/karlo-v1-alpha\"\ndata_type = torch.float16\nprior = PriorTransformer.from_pretrained(prior_model_id, subfolder=\"prior\", torch_dtype=data_type)\n\nprior_text_model_id = \"openai/clip-vit-large-patch14\"\nprior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)\nprior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=data_type)\nprior_scheduler = UnCLIPScheduler.from_pretrained(prior_model_id, subfolder=\"prior_scheduler\")\nprior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)\n\nstable_unclip_model_id = \"stabilityai/stable-diffusion-2-1-unclip-small\"\n\npipe = StableUnCLIPPipeline.from_pretrained(\n    stable_unclip_model_id,\n    torch_dtype=data_type,\n    variant=\"fp16\",\n    prior_tokenizer=prior_tokenizer,\n    prior_text_encoder=prior_text_model,\n    prior=prior,\n    prior_scheduler=prior_scheduler,\n)\n\npipe = pipe.to(\"cuda\")\nwave_prompt = \"dramatic wave, the Oceans roar, Strong wave spiral across the oceans as the waves unfurl into roaring crests; perfect wave form; perfect wave shape; dramatic wave shape; wave shape unbelievable; wave; wave shape spectacular\"\n\nimage = pipe(prompt=wave_prompt).images[0]\nimage\n```\n<Tip warning={true}>\n\nFor text-to-image we use `stabilityai/stable-diffusion-2-1-unclip-small` as it was trained on CLIP ViT-L/14 embedding, the same as the Karlo model prior. [stabilityai/stable-diffusion-2-1-unclip](https://hf.co/stabilityai/stable-diffusion-2-1-unclip) was trained on OpenCLIP ViT-H, so we don't recommend its use.\n\n</Tip>\n\n### Text guided Image-to-Image Variation\n\n```python\nfrom diffusers import StableUnCLIPImg2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16, variation=\"fp16\"\n)\npipe = pipe.to(\"cuda\")",
        "question": "What is the pretrained model id used for the StableUnCLIPImg2ImgPipeline?\n",
        "answer": "stabilityai/stable-diffusion-2-1-unclip",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md"
    },
    {
        "context": "This speedup is due to the fact that this library avoids unnecessary copies by mapping the file directly. It is actually possible to do on [pure pytorch](https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282).\nThe currently shown speedup was gotten on:\n* OS: Ubuntu 18.04.6 LTS\n* CPU: Intel(R) Xeon(R) CPU @ 2.00GHz\n\n\n### GPU benchmark\n\n```py\n>>> # This is required because this feature hasn't been fully verified yet, but \n>>> # it's been tested on many different environments\n>>> os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n\n>>> # CUDA startup out of the measurement\n>>> torch.zeros((2, 2)).cuda()\n\n>>> start_st = datetime.datetime.now()\n>>> weights = load_file(sf_filename, device=\"cuda:0\")\n>>> load_time_st = datetime.datetime.now() - start_st\n>>> print(f\"Loaded safetensors {load_time_st}\")\n\n>>> start_pt = datetime.datetime.now()\n>>> weights = torch.load(pt_filename, map_location=\"cuda:0\")\n>>> load_time_pt = datetime.datetime.now() - start_pt\n>>> print(f\"Loaded pytorch {load_time_pt}\")\n\n>>> print(f\"on GPU, safetensors is faster than pytorch by: {load_time_pt/load_time_st:.1f} X\")\nLoaded safetensors 0:00:00.165206\nLoaded pytorch 0:00:00.353889\non GPU, safetensors is faster than pytorch by: 2.1 X\n```\n\nThe speedup works because this library is able to skip unnecessary CPU allocations. It is unfortunately not replicable in pure pytorch as far as we know. The library works by memory mapping the file, creating the tensor empty with pytorch and calling `cudaMemcpy` directly to move the tensor directly on the GPU.\nThe currently shown speedup was gotten on:\n* OS: Ubuntu 18.04.6 LTS.\n* GPU: Tesla T4\n* Driver Version: 460.32.03\n* CUDA Version: 11.2",
        "question": "How much faster is safetensors than pytorch on GPU?\n",
        "answer": "Safetensors is 2.1 times faster than pytorch on GPU.",
        "source_doc": "huggingface/safetensors/blob/main/docs/source/speed.mdx"
    },
    {
        "context": "6.  [ ] Successfully converted original checkpoint to Transformers\n    checkpoint\n\n7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original checkpoint\n\n8.  [ ] Finished model tests in Transformers\n\n9.  [ ] Successfully added Tokenizer in Transformers\n\n10. [ ] Run end-to-end integration tests\n\n11. [ ] Finished docs\n\n12. [ ] Uploaded model weights to the hub\n\n13. [ ] Submitted the pull request for review\n\n14. [ ] (Optional) Added a demo notebook\n\nTo begin with, we usually recommend to start by getting a good\ntheoretical understanding of `BigBird`. However, if you prefer to\nunderstand the theoretical aspects of the model *on-the-job*, then it is\ntotally fine to directly dive into the `BigBird`'s code-base. This\noption might suit you better, if your engineering skills are better than\nyour theoretical skill, if you have trouble understanding\n`BigBird`'s paper, or if you just enjoy programming much more than\nreading scientific papers.\n\n### 1. (Optional) Theoretical aspects of BigBird\n\nYou should take some time to read *BigBird's* paper, if such\ndescriptive work exists. There might be large sections of the paper that\nare difficult to understand. If this is the case, this is fine - don't\nworry! The goal is not to get a deep theoretical understanding of the\npaper, but to extract the necessary information required to effectively\nre-implement the model in 🤗 Transformers. That being said, you don't\nhave to spend too much time on the theoretical aspects, but rather focus\non the practical ones, namely:",
        "question": "What is the goal of reading BigBird's paper?\n",
        "answer": "The goal of reading BigBird's paper is to extract the necessary information required to effectively re-implement the model in 🤗 Transformers, rather than getting a deep theoretical understanding of the paper.",
        "source_doc": "huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md"
    },
    {
        "context": "Let's see how it does all of this!\n\n## Using a model for question answering[[using-a-model-for-question-answering]]\n\nLike with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the `question-answering` pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) (the \"squad\" in the name comes from the dataset on which the model was fine-tuned; we'll talk more about the SQuAD dataset in [Chapter 7](/course/chapter7/7)):\n\n{#if fw === 'pt'}\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\noutputs = model(**inputs)\n```\n\n{:else}\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\ninputs = tokenizer(question, context, return_tensors=\"tf\")\noutputs = model(**inputs)\n```\n\n{/if}\n\nNote that we tokenize the question and the context as a pair, with the question first.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg\" alt=\"An example of tokenization of question and context\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg\" alt=\"An example of tokenization of question and context\"/>\n</div>",
        "question": "What is the checkpoint used by default for the question-answering pipeline?\n",
        "answer": "The checkpoint used by default for the question-answering pipeline is `distilbert-base-cased-distilled-squad`.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter6/3b.mdx"
    },
    {
        "context": "wrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1)\n  format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1)\n  represents the position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on\n  a 0-1000 scale. To normalize, you can use the following function:",
        "question": "How should bounding boxes be formatted in pytesseract?\n",
        "answer": "Bounding boxes should be in (x0, y0, x1, y1) format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000 scale.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/layoutlmv2.md"
    },
    {
        "context": "We can leave the `postprocess` method as is and modify the `_postprocess_chat_messages`\n\n```python\ndef _postprocess_chat_messages(\n    self, chat_message: MultimodalMessage | dict | None\n) -> MultimodalMessage | None:\n    if chat_message is None:\n        return None\n    if isinstance(chat_message, dict):\n        chat_message = MultimodalMessage(**chat_message)\n    chat_message.text = inspect.cleandoc(chat_message.text or \"\")\n    for file_ in chat_message.files:\n        file_.file.mime_type = client_utils.get_mimetype(file_.file.path)\n    return chat_message\n```\n\nBefore we wrap up with the backend code, let's modify the `example_inputs` method to return a valid dictionary representation of the `ChatbotData`:\n\n```python\ndef example_inputs(self) -> Any:\n    return [[{\"text\": \"Hello!\", \"files\": []}, None]]\n```\n\nCongrats - the backend is complete!\n\n## Part 3a - The Index.svelte file\n\nThe frontend for the `Chatbot` component is divided into two parts - the `Index.svelte` file and the `shared/Chatbot.svelte` file.\nThe `Index.svelte` file applies some processing to the data received from the server and then delegates the rendering of the conversation to the `shared/Chatbot.svelte` file.\nFirst we will modify the `Index.svelte` file to apply processing to the new data type the backend will return.\n\nLet's begin by porting our custom types  from our python `data_model` to typescript.\nOpen `frontend/shared/utils.ts` and add the following type definitions at the top of the file:\n\n```ts\nexport type FileMessage = {\n\tfile: FileData;\n\talt_text?: string;\n};\n\n\nexport type MultimodalMessage = {\n\ttext: string;\n\tfiles?: FileMessage[];\n}\n```\n\nNow let's import them in `Index.svelte` and modify the type annotations for `value` and `_value`.\n\n```ts\nimport type { FileMessage, MultimodalMessage } from \"./shared/utils\";\n\nexport let value: [\n    MultimodalMessage | null,\n    MultimodalMessage | null\n][] = [];\n\nlet _value: [\n    MultimodalMessage | null,\n    MultimodalMessage | null\n][];\n```",
        "question": "What is the type of `value` in `Index.svelte`?\n",
        "answer": "The type of `value` in `Index.svelte` is `[MultimodalMessage | null, MultimodalMessage | null][]`.",
        "source_doc": "gradio-app/gradio/blob/main/guides/05_custom-components/08_multimodal-chatbot-part1.md"
    },
    {
        "context": "If you click on one of these issues you'll find it contains a title, a description, and a set of labels that characterize the issue. An example is shown in the screenshot below.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues-single.png\" alt=\"A typical GitHub issue in the 🤗 Datasets repository.\" width=\"80%\"/>\n</div>\n\nTo download all the repository's issues, we'll use the [GitHub REST API](https://docs.github.com/en/rest) to poll the [`Issues` endpoint](https://docs.github.com/en/rest/reference/issues#list-repository-issues). This endpoint returns a list of JSON objects, with each object containing a large number of fields that include the title and description as well as metadata about the status of the issue and so on.\n\nA convenient way to download the issues is via the `requests` library, which is the standard way for making HTTP requests in Python. You can install the library by running:\n\n```python\n!pip install requests\n```\n\nOnce the library is installed, you can make GET requests to the `Issues` endpoint by invoking the `requests.get()` function. For example, you can run the following command to retrieve the first issue on the first page:\n\n```py\nimport requests\n\nurl = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\nresponse = requests.get(url)\n```\n\nThe `response` object contains a lot of useful information about the request, including the HTTP status code:\n\n```py\nresponse.status_code\n```\n\n```python out\n200\n```\n\nwhere a `200` status means the request was successful (you can find a list of possible HTTP status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)). What we are really interested in, though, is the _payload_, which can be accessed in various formats like bytes, strings, or JSON. Since we know our issues are in JSON format, let's inspect the payload as follows:\n\n```py\nresponse.json()\n```",
        "question": "What is the HTTP status code for a successful request?\n",
        "answer": "200",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/5.mdx"
    },
    {
        "context": "by [@pngwn](https://github.com/pngwn) in [PR 3205](https://github.com/gradio-app/gradio/pull/3205)\n\n###### New `gr.BarPlot` component! 📊\n\nCreate interactive bar plots from a high-level interface with `gr.BarPlot`.\nNo need to remember matplotlib syntax anymore!\n\nExample usage:\n\n```python\nimport gradio as gr\nimport pandas as pd\n\nsimple = pd.DataFrame({\n    'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],\n    'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]\n})\n\nwith gr.Blocks() as demo:\n    gr.BarPlot(\n        simple,\n        x=\"a\",\n        y=\"b\",\n        title=\"Simple Bar Plot with made up data\",\n        tooltip=['a', 'b'],\n    )\n\ndemo.launch()\n```\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3157](https://github.com/gradio-app/gradio/pull/3157)\n\n###### Bokeh plots are back! 🌠\n\nFixed a bug that prevented bokeh plots from being displayed on the front end and extended support for both 2.x and 3.x versions of bokeh!\n\n![image](https://user-images.githubusercontent.com/41651716/219468324-0d82e07f-8fb4-4ff9-b40c-8250b29e45f7.png)\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3212](https://github.com/gradio-app/gradio/pull/3212)\n\n### Bug Fixes:\n\n- Adds ability to add a single message from the bot or user side. Ex: specify `None` as the second value in the tuple, to add a single message in the chatbot from the \"bot\" side.\n\n```python\ngr.Chatbot([(\"Hi, I'm DialoGPT. Try asking me a question.\", None)])\n```\n\nBy [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3165](https://github.com/gradio-app/gradio/pull/3165)",
        "question": "How to add a single message from the bot side in a gradio chatbot?\n",
        "answer": "You can add a single message from the bot side in a gradio chatbot by specifying `None` as the second value in the tuple when defining the chatbot. For example: `gr.Chatbot([(\"Hi, I'm DialoGPT. Try asking me a question.\", None)])`",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than supervised. They\n  outperform both the [original model (ViT)](vit) as well as [Data-efficient Image Transformers (DeiT)](deit) when fine-tuned on ImageNet-1K and CIFAR-100. You can check out demo notebooks regarding inference as well as\n  fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer) (you can just replace\n  [`ViTFeatureExtractor`] by [`BeitImageProcessor`] and\n  [`ViTForImageClassification`] by [`BeitForImageClassification`]).\n- There's also a demo notebook available which showcases how to combine DALL-E's image tokenizer with BEiT for\n  performing masked image modeling. You can find it [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).\n- As the BEiT models expect each image to be of the same size (resolution), one can use\n  [`BeitImageProcessor`] to resize (or rescale) and normalize images for the model.\n- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of\n  each checkpoint. For example, `microsoft/beit-base-patch16-224` refers to a base-sized architecture with patch\n  resolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit).\n- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/) (a collection of\n  14 million images and 22k classes) only, (2) also fine-tuned on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/) (also referred to as ILSVRC 2012, a collection of 1.3 million\n  images and 1,000 classes).\n- BEiT uses relative position embeddings, inspired by the T5 model. During pre-training, the authors shared the",
        "question": "What is the name of the base-sized architecture with patch resolution of 16x16 and fine-tuning resolution of 224x224?\n",
        "answer": "microsoft/beit-base-patch16-224",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/beit.md"
    },
    {
        "context": "## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n```bibtex\n@article{willmott2005advantages,\n  title={Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},\n  author={Willmott, Cort J and Matsuura, Kenji},\n  journal={Climate research},\n  volume={30},\n  number={1},\n  pages={79--82},\n  year={2005}\n}\n```\n\n## Further References\n- [Mean Absolute Error - Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)",
        "question": "What is the disadvantage of using the Root Mean Square Error (RMSE) over the Mean Absolute Error (MAE) in assessing average model performance?\n",
        "answer": "The disadvantage of using the Root Mean Square Error (RMSE) over the Mean Absolute Error (MAE) in assessing average model performance is that RMSE is more sensitive to outliers than MAE.",
        "source_doc": "huggingface/datasets/blob/main/metrics/mae/README.md"
    },
    {
        "context": "Take a look at the [AutoPeftModel](package_reference/auto_class) API reference to learn more about the [`AutoPeftModel`] classes.\n\n## Next steps\n\nWith the appropriate [`PeftConfig`], you can apply it to any pretrained model to create a [`PeftModel`] and train large powerful models faster on freely available GPUs! To learn more about PEFT configurations and models, the following guide may be helpful:\n\n* Learn how to configure a PEFT method for models that aren't from Transformers in the [Working with custom models](../developer_guides/custom_models) guide.",
        "question": "How can I apply a PEFT configuration to a pretrained model?\n",
        "answer": "You can apply a PEFT configuration to a pretrained model by creating a `PeftModel` using the appropriate `PeftConfig`.",
        "source_doc": "huggingface/peft/blob/main/docs/source/tutorial/peft_model_config.md"
    },
    {
        "context": "|                          [ByT5](model_doc/byt5)                          |       ✅        |         ✅         |      ✅      |\n|                     [CamemBERT](model_doc/camembert)                     |       ✅        |         ✅         |      ❌      |\n|                        [CANINE](model_doc/canine)                        |       ✅        |         ❌         |      ❌      |\n|                  [Chinese-CLIP](model_doc/chinese_clip)                  |       ✅        |         ❌         |      ❌      |\n|                          [CLAP](model_doc/clap)                          |       ✅        |         ❌         |      ❌      |\n|                          [CLIP](model_doc/clip)                          |       ✅        |         ✅         |      ✅      |\n|                       [CLIPSeg](model_doc/clipseg)                       |       ✅        |         ❌         |      ❌      |\n|                          [CLVP](model_doc/clvp)                          |       ✅        |         ❌         |      ❌      |\n|                       [CodeGen](model_doc/codegen)                       |       ✅        |         ❌         |      ❌      |\n|                    [CodeLlama](model_doc/code_llama)                     |       ✅        |         ❌         |      ✅      |\n|              [Conditional DETR](model_doc/conditional_detr)              |       ✅        |         ❌         |      ❌      |\n|                      [ConvBERT](model_doc/convbert)                      |       ✅        |         ✅         |      ❌      |\n|                      [ConvNeXT](model_doc/convnext)                      |       ✅        |         ✅         |      ❌      |\n|                    [ConvNeXTV2](model_doc/convnextv2)                    |       ✅        |         ✅         |      ❌      |\n|                           [CPM](model_doc/cpm)                           |       ✅        |         ✅         |      ✅      |",
        "question": "Which model is not supported by the platform?\n",
        "answer": "CANINE",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/index.md"
    },
    {
        "context": "dataset[\"train\"][0]\n```\n\n\n```python\n# data preprocessing\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n\nprocessed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n\ntrain_dataset = processed_datasets[\"train\"].shuffle()\neval_dataset = processed_datasets[\"validation\"]\n```\n\n\n```python\n# training and evaluation\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    correct = 0\n    total = 0\n    for pred, true in zip(preds, labels):\n        if pred.strip() == true.strip():\n            correct += 1\n        total += 1\n    accuracy = correct / total\n    return {\"accuracy\": accuracy}",
        "question": "What is the function used to compute metrics in the training and evaluation step?\n",
        "answer": "The function used to compute metrics in the training and evaluation step is `compute_metrics`.",
        "source_doc": "huggingface/peft/blob/main/examples/conditional_generation/peft_prompt_tuning_seq2seq_with_generate.ipynb"
    },
    {
        "context": "1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\n1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.",
        "question": "Which model was released by Microsoft Research with the paper \"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data\"?\n",
        "answer": "UniSpeech",
        "source_doc": "huggingface/transformers/blob/main/README_ru.md"
    },
    {
        "context": "# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=quantization_config)\n\n# enable BetterTransformer\nmodel = model.to_bettertransformer()\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# enable FlashAttention\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    outputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```",
        "question": "What is the quantization configuration used for the model?\n",
        "answer": "The model is loaded in 4-bit quantization with bnb_4bit_compute_dtype set to torch.float16.\n```",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md"
    },
    {
        "context": ". As you can see, these files are processed line-by-line, so empty lines in the raw text are also represented as a row in the dataset. For JSON files, there are two main formats to know about. The first one is called JSON Lines, where every row in the file is a separate JSON object. For these files, you can load the dataset by selecting the json loading script and pointing the data_files argument to the file or URL. In this example, we've loaded a JSON lines files based on Stack Exchange questions and answers.",
        "question": "What is the format of JSON Lines files?\n",
        "answer": "In JSON Lines files, every row in the file is a separate JSON object.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter5/02_custom-dataset.md"
    },
    {
        "context": "In case this code runs gracefully, congratulations, the installation is successful!\n\nIn case the above `assert` fails, or you encounter the following warning\n\n```\nFailed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\n```\n\nsomething is wrong with the TensorRT or ONNX Runtime installation.\n\n### TensorRT engine build and warmup\n\nTensorRT requires to build its [inference engine](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build-phase) ahead of inference, which takes some time due to the model optimization and nodes fusion. To avoid rebuilding the engine every time the model is loaded, ONNX Runtime provides a pair of options to save the engine: `trt_engine_cache_enable` and `trt_engine_cache_path`.\n\nWe recommend setting these two provider options when using the TensorRT execution provider. The usage is as follows, where [`optimum/gpt2`](https://huggingface.co/optimum/gpt2) is an ONNX model converted from PyTorch using the [Optimum ONNX exporter](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model):\n\n```python\n>>> from optimum.onnxruntime import ORTModelForCausalLM\n\n>>> provider_options = {\n...     \"trt_engine_cache_enable\": True,\n...     \"trt_engine_cache_path\": \"tmp/trt_cache_gpt2_example\"\n... }\n\n# the TensorRT engine is not built here, it will be when doing inference\n>>> ort_model = ORTModelForCausalLM.from_pretrained(\n...     \"optimum/gpt2\",\n...     use_cache=False,\n...     provider=\"TensorrtExecutionProvider\",\n...     provider_options=provider_options\n... )\n```",
        "question": "How to avoid rebuilding the TensorRT engine every time the model is loaded?\n",
        "answer": "To avoid rebuilding the engine every time the model is loaded, ONNX Runtime provides a pair of options to save the engine: `trt_engine_cache_enable` and `trt_engine_cache_path`. These options should be set when using the TensorRT execution provider.",
        "source_doc": "huggingface/optimum/blob/main/docs/source/onnxruntime/usage_guides/gpu.mdx"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Load pretrained instances with an AutoClass\n\nWith so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of 🤗 Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n\n<Tip>\n\nRemember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, [BERT](https://huggingface.co/bert-base-uncased) is an architecture, while `bert-base-uncased` is a checkpoint. Model is a general term that can mean either architecture or checkpoint.\n\n</Tip>\n\nIn this tutorial, learn to:",
        "question": "What is the difference between architecture and checkpoint?\n",
        "answer": "Architecture refers to the skeleton of the model, while checkpoints are the weights for a given architecture. For example, BERT is an architecture, while `bert-base-uncased` is a checkpoint. Model is a general term that can mean either architecture or checkpoint.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/autoclass_tutorial.md"
    },
    {
        "context": "# MM-IMDb\n\nBased on the script [`run_mmimdb.py`](https://github.com/huggingface/transformers/blob/main/examples/research_projects/mm-imdb/run_mmimdb.py).\n\n[MM-IMDb](http://lisi1.unal.edu.co/mmimdb/) is a Multimodal dataset with around 26,000 movies including images, plots and other metadata.\n\n### Training on MM-IMDb\n\n```\npython run_mmimdb.py \\\n    --data_dir /path/to/mmimdb/dataset/ \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --output_dir /path/to/save/dir/ \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_len 512 \\\n    --gradient_accumulation_steps 20 \\\n    --num_image_embeds 3 \\\n    --num_train_epochs 100 \\\n    --patience 5\n```",
        "question": "How many movies are in the MM-IMDb dataset?\n",
        "answer": "The MM-IMDb dataset contains around 26,000 movies.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/mm-imdb/README.md"
    },
    {
        "context": "If you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix` argument.\n\nFor the aforementioned group of T5 models it's important to remember that if you switch to a different language pair, make sure to adjust the source and target values in all 3 language-specific command line argument: `--source_lang`, `--target_lang` and `--source_prefix`.\n\nMBart models require a different format for `--source_lang` and `--target_lang` values, e.g. instead of `en` it expects `en_XX`, for `ro` it expects `ro_RO`. The full MBart specification for language codes can be found [here](https://huggingface.co/facebook/mbart-large-cc25). For example:\n\n```bash\npython run_translation.py \\\n    --model_name_or_path facebook/mbart-large-en-ro  \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name wmt16 \\\n    --dataset_config_name ro-en \\\n    --source_lang en_XX \\\n    --target_lang ro_RO \\\n    --output_dir /tmp/tst-translation \\\n    --per_device_train_batch_size=16 \\\n    --per_device_eval_batch_size=16 \\\n    --overwrite_output_dir\n ```",
        "question": "What should you check if you get a terrible BLEU score with T5 models?\n",
        "answer": "You should check if you forgot to use the `--source_prefix` argument.",
        "source_doc": "huggingface/transformers/blob/main/examples/tensorflow/translation/README.md"
    },
    {
        "context": "## Core ML inference in Swift\n\nRunning inference in Swift is slightly faster than in Python because the models are already compiled in the `mlmodelc` format. This is noticeable on app startup when the model is loaded but shouldn’t be noticeable if you run several generations afterward.\n\n### Download\n\nTo run inference in Swift on your Mac, you need one of the `compiled` checkpoint versions. We recommend you download them locally using Python code similar to the previous example, but with one of the `compiled` variants:\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/compiled\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n### Inference[[swift-inference]]\n\nTo run inference, please clone Apple's repo:\n\n```bash\ngit clone https://github.com/apple/ml-stable-diffusion\ncd ml-stable-diffusion\n```\n\nAnd then use Apple's command line tool, [Swift Package Manager](https://www.swift.org/package-manager/#):\n\n```bash\nswift run StableDiffusionSample --resource-path models/coreml-stable-diffusion-v1-4_original_compiled --compute-units all \"a photo of an astronaut riding a horse on mars\"\n```\n\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step, so please make sure it contains compiled Core ML bundles with the extension `.mlmodelc`. The `--compute-units` has to be one of these values: `all`, `cpuOnly`, `cpuAndGPU`, `cpuAndNeuralEngine`.\n\nFor more details, please refer to the [instructions in Apple's repo](https://github.com/apple/ml-stable-diffusion).\n\n## Supported Diffusers Features",
        "question": "What is the recommended way to download the compiled checkpoint versions for Swift inference?\n",
        "answer": "The recommended way to download the compiled checkpoint versions for Swift inference is by using Python code similar to the previous example, but with one of the `compiled` variants.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md"
    },
    {
        "context": "We have not experimented with Varuna and SageMaker but their papers report that they have overcome the list of problems \nmentioned above and that they require smaller changes to the user's model.\n\nImplementations:\n- [PyTorch](https://pytorch.org/docs/stable/pipeline.html) (initial support in pytorch-1.8, and progressively getting improved in 1.9 and more so in 1.10). Some [examples](https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py)\n- [DeepSpeed](https://www.deepspeed.ai/tutorials/pipeline/)\n- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) has an internal implementation - no API.\n- [Varuna](https://github.com/microsoft/varuna)\n- [SageMaker](https://arxiv.org/abs/2111.05972) - this is a proprietary solution that can only be used on AWS.\n- [OSLO](https://github.com/tunib-ai/oslo) - this is implemented based on the Hugging Face Transformers.\n\n🤗 Transformers status: as of this writing none of the models supports full-PP. GPT2 and T5 models have naive MP support. \nThe main obstacle is being unable to convert the models to `nn.Sequential` and have all the inputs to be Tensors. This \nis because currently the models include many features that make the conversion very complicated, and will need to be removed to accomplish that.\n\nDeepSpeed and Megatron-LM integrations are available in [🤗 Accelerate](https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed)\n\nOther approaches:\n\nDeepSpeed, Varuna and SageMaker use the concept of an [Interleaved Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-sagemaker-interleaved-pipeline.png\" alt=\"Interleaved pipeline execution\"/>\n</div>",
        "question": "Which ML frameworks use the concept of an Interleaved Pipeline?\n",
        "answer": "DeepSpeed, Varuna and SageMaker",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_many.md"
    },
    {
        "context": "<Tip>\n\n🚨 Many issues in the 🤗 Transformers repository are unsolved because the data used to reproduce them is not accessible.\n\n</Tip>\n\nOnce you have something that is self-contained, you can try to reduce it into even less lines of code, building what we call a _minimal reproducible example_. While this requires a bit more work on your side, you will almost be guaranteed to get help and a fix if you provide a nice, short bug reproducer.\n\nIf you feel comfortable enough, go inspect the source code where your bug happens. You might find a solution to your problem (in which case you can even suggest a pull request to fix it), but more generally, this can help the maintainers better understand the source when they read your report.\n\n## Filling out the issue template[[filling-out-the-issue-template]]\n\nWhen you file your issue, you will notice there is a template to fill out. We will follow the one for [🤗 Transformers issues](https://github.com/huggingface/transformers/issues/new/choose) here, but the same kind of information will be required if you report an issue in another repository. Don't leave the template blank: taking the time to fill it in will maximize your chances of getting an answer and solving your problem.\n\nIn general, when filing an issue, always stay courteous. This is an open source project, so you are using free software, and no one has any obligation to help you. You may include what you feel is justified criticism in your issue, but then the maintainers may very well take it badly and not be in a rush help you. Make sure you read the [code of conduct](https://github.com/huggingface/transformers/blob/master/CODE_OF_CONDUCT.md) of the project.\n\n### Including your environment information[[including-your-environment-information]]\n\n🤗 Transformers provides a utility to get all the information we need about your environment. Just type the following in your terminal:\n\n```\ntransformers-cli env\n```\n\nand you should get something like this:",
        "question": "What command should be run to get the environment information for the Transformers library?\n",
        "answer": "The command to get the environment information for the Transformers library is `transformers-cli env`.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter8/5.mdx"
    },
    {
        "context": "--\ntitle: \"Evaluating Language Model Bias with 🤗 Evaluate\"\nthumbnail: /blog/assets/112_evaluating-llm-bias/thumbnail.png\nauthors:\n- user: sasha\n- user: meg\n- user: mathemakitten\n- user: lvwerra\n- user: douwekiela\n---\n\n# Evaluating Language Model Bias with 🤗 Evaluate\n\n\nWhile the size and capabilities of large language models have drastically increased over the past couple of years, so too has the concern around biases imprinted into these models and their training data. In fact, many popular language models have been found to be biased against specific [religions](https://www.nature.com/articles/s42256-021-00359-2?proof=t) and [genders](https://aclanthology.org/2021.nuse-1.5.pdf), which can result in the promotion of discriminatory ideas and the perpetuation of harms against marginalized groups.\n\nTo help the community explore these kinds of biases and strengthen our understanding of the social issues that language models encode, we have been working on adding bias metrics and measurements to the [🤗 Evaluate library](https://github.com/huggingface/evaluate). In this blog post, we will present a few examples of the new additions and how to use them. We will focus on the evaluation of [causal language models (CLMs)](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) like [GPT-2](https://huggingface.co/gpt2) and [BLOOM](https://huggingface.co/bigscience/bloom-560m), leveraging their ability to generate free text based on prompts.\n\nIf you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!\n\nThe workflow has two main steps:\n- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))\n- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))",
        "question": "What is the name of the library used to evaluate language model bias?\n",
        "answer": "The name of the library used to evaluate language model bias is 🤗 Evaluate.",
        "source_doc": "huggingface/blog/blob/main/evaluating-llm-bias.md"
    },
    {
        "context": "| A1111/k-diffusion    | 🤗 Diffusers                         | Usage                                                                                                         |\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\n| DPM++ 2M            | [`DPMSolverMultistepScheduler`]     |                                                                                                               |\n| DPM++ 2M Karras     | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True`                                                                            |\n| DPM++ 2M SDE        | [`DPMSolverMultistepScheduler`]     | init with `algorithm_type=\"sde-dpmsolver++\"`                                                                  |\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True` and `algorithm_type=\"sde-dpmsolver++\"`                                     |\n| DPM++ 2S a          | N/A                                 | very similar to  `DPMSolverSinglestepScheduler`                         |\n| DPM++ 2S a Karras   | N/A                                 | very similar to  `DPMSolverSinglestepScheduler(use_karras_sigmas=True, ...)` |\n| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                                                                                               |\n| DPM++ SDE Karras    | [`DPMSolverSinglestepScheduler`]    | init with `use_karras_sigmas=True`                                                                            |\n| DPM2                | [`KDPM2DiscreteScheduler`]          |                                                                                                               |",
        "question": "What scheduler is used for DPM++ 2M SDE Karras?\n",
        "answer": "The `DPMSolverMultistepScheduler` is used for DPM++ 2M SDE Karras.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md"
    },
    {
        "context": "**3. The Text-encoder**\n\nThe text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronaut riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.\n\nInspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).\n\n**Why is latent diffusion fast and efficient?**\n\nSince latent diffusion operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8. This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 × 8 = 64` times less memory.\n\nThis is why it's possible to generate `512 × 512` images so quickly, even on 16GB Colab GPUs!\n\n**Stable Diffusion during inference**\n\nPutting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow.\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n</p>\n\nThe stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size \\\\( 64 \\times 64 \\\\) where as the text prompt is transformed to text embeddings of size \\\\( 77 \\times 768 \\\\) via CLIP's text encoder.",
        "question": "What is the reduction factor of the autoencoder used in Stable Diffusion?\n",
        "answer": "The reduction factor of the autoencoder used in Stable Diffusion is 8.",
        "source_doc": "huggingface/blog/blob/main/stable_diffusion.md"
    },
    {
        "context": "Run the following command to authenticate your token\n\n```bash\nhuggingface-cli login\n```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\n#### Hardware\nWith `gradient_checkpointing` and `mixed_precision` it should be possible to fine tune the model on a single 24GB GPU. For higher `batch_size` and faster training it's better to use GPUs with >30GB memory.\n\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.___**\n<!-- accelerate_snippet_start -->\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\"\n```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset according to the format required by `datasets`, you can find the instructions for how to do that in this [document](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata).\nIf you wish to use custom loading logic, you should modify the script, we have left pointers for that in the training script.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"",
        "question": "How to prepare the dataset for training?\n",
        "answer": "To prepare the dataset for training, you need to follow the format required by `datasets`. You can find the instructions for how to do that in this [document](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata). If you wish to use custom loading logic, you should modify the script.",
        "source_doc": "huggingface/diffusers/blob/main/examples/text_to_image/README.md"
    },
    {
        "context": "### Fixes\n\n- [#5459](https://github.com/gradio-app/gradio/pull/5459) [`bd2fda77`](https://github.com/gradio-app/gradio/commit/bd2fda77fc98d815f4fb670f535af453ebee9b80) - Dispatch `stop_recording` event in Audio.  Thanks [@hannahblair](https://github.com/hannahblair)!\n- [#5508](https://github.com/gradio-app/gradio/pull/5508) [`05715f55`](https://github.com/gradio-app/gradio/commit/05715f5599ae3e928d3183c7b0a7f5291f843a96) - Adds a `filterable` parameter to `gr.Dropdown` that controls whether user can type to filter choices.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5470](https://github.com/gradio-app/gradio/pull/5470) [`a4e010a9`](https://github.com/gradio-app/gradio/commit/a4e010a96f1d8a52b3ac645e03fe472b9c3cbbb1) - Fix share button position.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n- [#5496](https://github.com/gradio-app/gradio/pull/5496) [`82ec4d26`](https://github.com/gradio-app/gradio/commit/82ec4d2622a43c31b248b78e9410e2ac918f6035) - Allow interface with components to be run inside blocks.  Thanks [@abidlabs](https://github.com/abidlabs)!\n\n## 3.43.2\n\n### Fixes\n\n- [#5456](https://github.com/gradio-app/gradio/pull/5456) [`6e381c4f`](https://github.com/gradio-app/gradio/commit/6e381c4f146cc8177a4e2b8e39f914f09cd7ff0c) - ensure dataframe doesn't steal focus.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 3.43.1\n\n### Fixes\n\n- [#5445](https://github.com/gradio-app/gradio/pull/5445) [`67bb7bcb`](https://github.com/gradio-app/gradio/commit/67bb7bcb6a95b7a00a8bdf612cf147850d919a44) - ensure dataframe doesn't scroll unless needed.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5447](https://github.com/gradio-app/gradio/pull/5447) [`7a4a89e5`](https://github.com/gradio-app/gradio/commit/7a4a89e5ca1dedb39e5366867501584b0c636bbb) - ensure iframe is correct size on spaces.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 3.43.0\n\n### Features",
        "question": "Which user added the filterable parameter to gr.Dropdown?\n",
        "answer": "@abidlabs",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "Turning typed, handwritten, or printed text into machine-encoded text is known as Optical Character Recognition (OCR). It's a widely studied problem with many well-established open-source and commercial offerings. The figure shows an example of converting handwriting into text.\n\n![png](assets/112_document-ai/ocr.png)\n\nOCR is a backbone of Document AI use cases as it's essential to transform the text into something readable by a computer. Some widely available OCR models that operate at the document level are [EasyOCR](https://huggingface.co/spaces/tomofi/EasyOCR) or [PaddleOCR](https://huggingface.co/spaces/PaddlePaddle/PaddleOCR). There are also models like [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/docs/transformers/model_doc/trocr), which runs on single-text line images. This model works with a text detection model like CRAFT which first identifies the individual \"pieces\" of text in a document in the form of bounding boxes. The relevant metrics for OCR are Character Error Rate (CER) and word-level precision, recall, and F1. Check out [this Space](https://huggingface.co/spaces/tomofi/CRAFT-TrOCR) to see a demonstration of CRAFT and TrOCR.  \n</div>\n    </div>\n        </div>\n\n <html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n    <a id=\"2-what-is-doc_class\"><strong itemprop=\"name\"> What is Document Image Classification?</strong></a>\n    <div itemscope itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\">\n      <div itemprop=\"text\">         \n    \nClassifying documents into the appropriate category, such as forms, invoices, or letters, is known as document image classification. Classification may use either one or both of the document's image and text. The recent addition of multimodal models that use the visual structure and the underlying text has dramatically increased classifier performance.",
        "question": "What is document image classification?\n",
        "answer": "Document image classification is the process of categorizing documents into appropriate categories, such as forms, invoices, or letters, based on the document's image and/or text.\n\n</div>\n    </div>\n  </div>\n\n</html>",
        "source_doc": "huggingface/blog/blob/main/document-ai.md"
    },
    {
        "context": "There are many libraries available which provide such higher-level features to accelerate the development of algorithms. \nAmong the most common, one can look at OpenMP, Thread Building Blocks and directly from the C++ when targeting a recent version of the standard. \nIn the following part of this blog post, we will restrict ourselves to OpenMP and especially comparing the GNU, open source and community-based implementation, to the Intel OpenMP one. \nThe latter especially targets Intel CPUs and is optimized to provide best of class performances when used as a drop-in replacement against the GNU OpenMP one.\n\nOpenMP exposes [many environment variables](https://www.openmp.org/spec-html/5.0/openmpch6.html) to automatically configure the underlying resources which will be involved in the computations, \nsuch as the number of threads to use to dispatch computation to (intra-op threads), the way the system scheduler should bind each of these threads with respect to the CPU resources (threads, cores, sockets) \nand some other variables which bring further control to the user. \nIntel OpenMP exposes [more of these environment variables](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compilation/supported-environment-variables.html) to provide the user even more flexibility to adjust the performance of its software.",
        "question": "How many environment variables does Intel OpenMP expose?\n",
        "answer": "Intel OpenMP exposes more of these environment variables.",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-2.md"
    },
    {
        "context": "Beam Datasets\n\nSome datasets are too large to be processed on a single machine. Instead, you can process them with [Apache Beam](https://beam.apache.org/), a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/), or [Google Cloud Dataflow](https://cloud.google.com/dataflow).\n\nWe have already created Beam pipelines for some of the larger datasets like [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b). You can load these normally with [`load_dataset`]. But if you want to run your own Beam pipeline with Dataflow, here is how:\n\n1. Specify the dataset and configuration you want to process:\n\n```\nDATASET_NAME=your_dataset_name  # ex: wikipedia\nCONFIG_NAME=your_config_name    # ex: 20220301.en\n```\n\n2. Input your Google Cloud Platform information:\n\n```\nPROJECT=your_project\nBUCKET=your_bucket\nREGION=your_region\n```\n\n3. Specify your Python requirements:\n\n```\necho \"datasets\" > /tmp/beam_requirements.txt\necho \"apache_beam\" >> /tmp/beam_requirements.txt\n```\n\n4. Run the pipeline:\n\n```\ndatasets-cli run_beam datasets/$DATASET_NAME \\\n--name $CONFIG_NAME \\\n--save_info \\\n--cache_dir gs://$BUCKET/cache/datasets \\\n--beam_pipeline_options=\\\n\"runner=DataflowRunner,project=$PROJECT,job_name=$DATASET_NAME-gen,\"\\\n\"staging_location=gs://$BUCKET/binaries,temp_location=gs://$BUCKET/temp,\"\\\n\"region=$REGION,requirements_file=/tmp/beam_requirements.txt\"\n```\n\n<Tip>\n\nWhen you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.\n\n</Tip>",
        "question": "How do I specify the dataset and configuration to process with Apache Beam?\n",
        "answer": "You can specify the dataset and configuration to process with Apache Beam by setting the `DATASET_NAME` and `CONFIG_NAME` variables. For example, `DATASET_NAME=wikipedia` and `CONFIG_NAME=20220301.en`.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/beam.mdx"
    },
    {
        "context": "</Tip>\n\n## DiTPipeline\n[[autodoc]] DiTPipeline\n\t- all\n\t- __call__\n\n## ImagePipelineOutput\n[[autodoc]] pipelines.ImagePipelineOutput",
        "question": "What is the name of the class that is the output of the ImagePipeline?\n",
        "answer": "pipelines.ImagePipelineOutput",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/dit.md"
    },
    {
        "context": "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XGLM\n\n## Overview\n\nThe XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)\nby Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, \nShruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, \nJeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n\nThe abstract from the paper is the following:",
        "question": "Is the XGLM model open-source?\n",
        "answer": "Yes, the XGLM model is open-source and is available for use and modification under the Apache 2.0 license.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/xglm.md"
    },
    {
        "context": "```bash\n# First, clone repo locally\ngit clone https://github.com/huggingface/huggingface_hub.git\n\n# Then, install with -e flag\ncd huggingface_hub\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths.\nPython will now look inside the folder you cloned to in addition to the normal library paths.\nFor example, if your Python packages are typically installed in `./.venv/lib/python3.11/site-packages/`,\nPython will also search the folder you cloned `./huggingface_hub/`.\n\n## Install with conda\n\nIf you are more familiar with it, you can install `huggingface_hub` using the [conda-forge channel](https://anaconda.org/conda-forge/huggingface_hub):\n\n\n```bash\nconda install -c conda-forge huggingface_hub\n```\n\nOnce done, [check installation](#check-installation) is working correctly.\n\n## Check installation\n\nOnce installed, check that `huggingface_hub` works properly by running the following command:\n\n```bash\npython -c \"from huggingface_hub import model_info; print(model_info('gpt2'))\"\n```\n\nThis command will fetch information from the Hub about the [gpt2](https://huggingface.co/gpt2) model.\nOutput should look like this:\n\n```text\nModel Name: gpt2\nTags: ['pytorch', 'tf', 'jax', 'tflite', 'rust', 'safetensors', 'gpt2', 'text-generation', 'en', 'doi:10.57967/hf/0039', 'transformers', 'exbert', 'license:mit', 'has_space']\nTask: text-generation\n```\n\n## Windows limitations\n\nWith our goal of democratizing good ML everywhere, we built `huggingface_hub` to be a\ncross-platform library and in particular to work correctly on both Unix-based and Windows\nsystems. However, there are a few cases where `huggingface_hub` has some limitations when\nrun on Windows. Here is an exhaustive list of known issues. Please let us know if you\nencounter any undocumented problem by opening [an issue on Github](https://github.com/huggingface/huggingface_hub/issues/new/choose).",
        "question": "What is the command to check if huggingface_hub is installed correctly?\n",
        "answer": "The command to check if huggingface_hub is installed correctly is `python -c \"from huggingface_hub import model_info; print(model_info('gpt2'))\"`.",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/installation.md"
    },
    {
        "context": "## TFElectraForTokenClassification\n\n[[autodoc]] TFElectraForTokenClassification\n    - call\n\n## TFElectraForQuestionAnswering\n\n[[autodoc]] TFElectraForQuestionAnswering\n    - call\n\n</tf>\n<jax>\n\n## FlaxElectraModel\n\n[[autodoc]] FlaxElectraModel\n    - __call__\n\n## FlaxElectraForPreTraining\n\n[[autodoc]] FlaxElectraForPreTraining\n    - __call__\n\n## FlaxElectraForCausalLM\n\n[[autodoc]] FlaxElectraForCausalLM\n    - __call__\n\n## FlaxElectraForMaskedLM\n\n[[autodoc]] FlaxElectraForMaskedLM\n    - __call__\n\n## FlaxElectraForSequenceClassification\n\n[[autodoc]] FlaxElectraForSequenceClassification\n    - __call__\n\n## FlaxElectraForMultipleChoice\n\n[[autodoc]] FlaxElectraForMultipleChoice\n    - __call__\n\n## FlaxElectraForTokenClassification\n\n[[autodoc]] FlaxElectraForTokenClassification\n    - __call__\n\n## FlaxElectraForQuestionAnswering\n\n[[autodoc]] FlaxElectraForQuestionAnswering\n    - __call__\n\n</jax>\n</frameworkcontent>",
        "question": "What is the name of the class for causal language modeling in FlaxElectra?\n",
        "answer": "FlaxElectraForCausalLM",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/electra.md"
    },
    {
        "context": "### 8. Fixing a \"Good second issue\"\n\n*Good second issues* are marked by the [Good second issue](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22Good+second+issue%22) label. Good second issues are\nusually more complicated to solve than [Good first issues](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).\nThe issue description usually gives less guidance on how to fix the issue and requires\na decent understanding of the library by the interested contributor.\nIf you are interested in tackling a good second issue, feel free to open a PR to fix it and link the PR to the issue. If you see that a PR has already been opened for this issue but did not get merged, have a look to understand why it wasn't merged and try to open an improved PR.\nGood second issues are usually more difficult to get merged compared to good first issues, so don't hesitate to ask for help from the core maintainers. If your PR is almost finished the core maintainers can also jump into your PR and commit to it in order to get it merged.\n\n### 9. Adding pipelines, models, schedulers\n\nPipelines, models, and schedulers are the most important pieces of the Diffusers library.\nThey provide easy access to state-of-the-art diffusion technologies and thus allow the community to\nbuild powerful generative AI applications.\n\nBy adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of the user interfaces relying on Diffusers which can be of immense value for the whole generative AI ecosystem.",
        "question": "How can I add a new model, pipeline, or scheduler to the Diffusers library?\n",
        "answer": "To add a new model, pipeline, or scheduler to the Diffusers library, you can contribute by opening a PR and linking it to the relevant issue. The PR should ideally improve upon any existing PRs that have not been merged. If your PR is almost finished, the core maintainers can assist in getting it merged. Adding new models, pipelines, or schedulers can enable new powerful use cases for user interfaces relying on Diffusers, contributing significantly to the generative AI ecosystem.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md"
    },
    {
        "context": "## Create a Space to react to your Webhook\n\nWe now need a way to react to your Webhook events. An easy way to do this is to use a [Space](https://huggingface.co/docs/hub/spaces-overview)!\n\nYou can find an example Space [here](https://huggingface.co/spaces/huggingface-projects/auto-retrain/tree/main).\n\nThis Space uses Docker, Python, [FastAPI](https://fastapi.tiangolo.com/), and [uvicorn](https://www.uvicorn.org) to run a simple HTTP server. Read more about Docker Spaces [here](https://huggingface.co/docs/hub/spaces-sdks-docker).\n\nThe entry point is [src/main.py](https://huggingface.co/spaces/huggingface-projects/auto-retrain/blob/main/src/main.py). Let's walk through this file and detail what it does:\n\n1. It spawns a FastAPI app that will listen to HTTP `POST` requests on `/webhook`:\n\n```python\nfrom fastapi import FastAPI\n\n# [...]\n@app.post(\"/webhook\")\nasync def post_webhook(\n\t# ...\n):\n\n# ...\n```\n\n2.  2. This route checks that the `X-Webhook-Secret` header is present and that its value is the same as the one you set in your Webhook's settings. The `WEBHOOK_SECRET` secret must be set in the Space's settings and be the same as the secret set in your Webhook.\n\n```python\n# [...]\n\nWEBHOOK_SECRET = os.getenv(\"WEBHOOK_SECRET\")\n\n# [...]\n\n@app.post(\"/webhook\")\nasync def post_webhook(\n\t# [...]\n\tx_webhook_secret:  Optional[str] = Header(default=None),\n\t# ^ checks for the X-Webhook-Secret HTTP header\n):\n\tif x_webhook_secret is None:\n\t\traise HTTPException(401)\n\tif x_webhook_secret != WEBHOOK_SECRET:\n\t\traise HTTPException(403)\n\t# [...]\n```\n\n3. The event's payload is encoded as JSON. Here, we'll be using pydantic models to parse the event payload. We also specify that we will run our Webhook only when:\n- the event concerns the input dataset\n- the event is an update on the repo's content, i.e., there has been a new commit\n\n\n```python\n# defined in src/models.py\nclass WebhookPayloadEvent(BaseModel):\n\taction: Literal[\"create\", \"update\", \"delete\"]\n\tscope: str",
        "question": "What is the name of the file that contains the FastAPI app?\n",
        "answer": "The name of the file that contains the FastAPI app is src/main.py.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md"
    },
    {
        "context": "AR image generative models have evolved architecturally with much work towards making transformers computationally feasible. Prior to transformer based models, [PixelRNN](https://arxiv.org/abs/1601.06759), [PixelCNN](https://arxiv.org/abs/1606.05328), and [PixelCNN++](https://arxiv.org/abs/1701.05517) were the state of the art. \n\n[Image Transformer](https://arxiv.org/abs/1802.05751) provides a good discussion on the non-transformer based models and the transition to transformer based models (see paper for omitted citations).\n\n> Training recurrent neural networks to sequentially predict each pixel of even a small image is computationally very challenging. Thus, parallelizable models that use convolutional neural networks such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality. \n>\n> One disadvantage of CNNs compared to RNNs is their typically fairly limited receptive field. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. \n>\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions.\n\n[Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels.",
        "question": "What is a disadvantage of CNNs compared to RNNs in image processing?\n",
        "answer": "A disadvantage of CNNs compared to RNNs in image processing is their typically fairly limited receptive field, which can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers.",
        "source_doc": "huggingface/blog/blob/main/vq-diffusion.md"
    },
    {
        "context": "Those API utilities are also exposed through the `huggingface-cli` CLI:\n\n```bash\nhuggingface-cli login\nhuggingface-cli logout\nhuggingface-cli whoami\nhuggingface-cli repo create\n```\n\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tags (e.g. if you want to list models compatible with your library):\n- **Models**:\n  - `list_models()`\n  - `model_info()`\n  - `get_model_tags()`\n- **Datasets**:\n  - `list_datasets()`\n  - `dataset_info()`\n  - `get_dataset_tags()`\n- **Spaces**:\n  - `list_spaces()`\n  - `space_info()`\n\nThese lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```",
        "question": "How do I clone a Hugging Face repository using the `Repository` class?\n",
        "answer": "You can clone a Hugging Face repository by passing the model ID as a string to the `clone_from` parameter of the `Repository` class. For example, `Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")`.",
        "source_doc": "huggingface/huggingface_hub/blob/main/src/huggingface_hub/README.md"
    },
    {
        "context": "And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.\n\nSo one thing that we've been working on is to develop a way that you can evaluate models and datasets directly through the Hub. We're still trying to experiment there with the direction. But I'm hoping that we have something cool to show later this year. \n\nAnd there's another side to this which is that a large part of the measuring progress in machine learning is through the use of benchmarks. These benchmarks are traditionally a set of datasets with some tasks but what's been maybe missing is that a lot of researchers speak to us and say, “Hey, I've got this cool idea for a benchmark, but I don't really want to implement all of the nitty-gritty infrastructure for the submissions, and the maintenance, and all those things.”\n\nAnd so we've been working with some really cool partners on hosting benchmarks on the Hub directly. So that then people in the research community can use the tooling that we have and then simplify the evaluation of these models. \n\n### That is super interesting and powerful.\n\n**Lewis:** Maybe one thing to mention is that the whole evaluation question is a very subtle one. We know from previous benchmarks, such as SQuAD, a famous benchmark to measure how good models are at question answering, that many of these transformer models are good at taking shortcuts.\n\nWell, that's the aim but it turns out that many of these transformer models are really good at taking shortcuts. So, what they’re actually doing is they're getting a very high score on a benchmark which doesn't necessarily translate into the actual thing you were interested in which was answering questions.",
        "question": "How are some researchers addressing the issue of transformer models taking shortcuts in benchmarks?\n",
        "answer": "Some researchers are addressing the issue of transformer models taking shortcuts in benchmarks by developing new benchmarks that require more complex reasoning and discourage shortcut-taking. They are also working on hosting these benchmarks on the Hub to simplify the evaluation of models and discourage shortcut-taking in the research community.",
        "source_doc": "huggingface/blog/blob/main/lewis-tunstall-interview.md"
    },
    {
        "context": "Assistant 2's code was much better in terms of structure and readability. The code was accurate, efficient, and included helpful comments. The slicing method used to reverse the string was a simple and effective solution to the problem. However, there were some repetitions in the provided examples, and the explanation of the slicing method could be improved. Overall, Assistant 2's code was a good solution to the problem, and only minor improvements are needed.\n```\n\nWe can compare this to ChatGPT’s response, which seems to miss the fact that the Assistant 1 does not actually accomplish the task. In its response it says the second is better, but gives it a lower score. \n\nChatGPT Evaluation:\n\n```\n8 7\n\nAssistant 1's solution attempts to use a variable assignment operator, :=, which is a new feature added to Python 3.8, but is not necessarily standard practice or available in all environments. Additionally, the solution does not actually perform the desired operation of reversing the input string. Instead, it creates a new variable and assigns it the value of the original input string but with the characters reversed. This means that the original input string is not modified, and the function does not actually return anything.\n\nAssistant 2's solution correctly implements the desired operation of reversing the input string using a list to store each character, then joining the list back into a string in reverse order. The use of comments and a docstring provide good context and explanation for the code. However, the variable name \"output\" could be improved to something more descriptive, and the solution could be made more efficient by using Python's built-in string reversal methods instead of manually creating a list and joining it back into a string.",
        "question": "Which assistant's solution correctly reverses the input string?\n",
        "answer": "Assistant 2's solution correctly reverses the input string.\n```\n\n```",
        "source_doc": "huggingface/blog/blob/main/starchat-alpha.md"
    },
    {
        "context": "<!--\nType: model-index\nCollections:\n- Name: EfficientNet Pruned\n  Paper:\n    Title: Knapsack Pruning with Inner Distillation\n    URL: https://paperswithcode.com/paper/knapsack-pruning-with-inner-distillation\nModels:\n- Name: efficientnet_b1_pruned\n  In Collection: EfficientNet Pruned\n  Metadata:\n    FLOPs: 489653114\n    Parameters: 6330000\n    File Size: 25595162\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: efficientnet_b1_pruned\n    Crop Pct: '0.882'\n    Image Size: '240'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/efficientnet.py#L1208\n  Weights: https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb1_pruned_9ebb3fe6.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78.25%\n      Top 5 Accuracy: 93.84%\n- Name: efficientnet_b2_pruned\n  In Collection: EfficientNet Pruned\n  Metadata:\n    FLOPs: 878133915\n    Parameters: 8310000\n    File Size: 33555005\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: efficientnet_b2_pruned\n    Crop Pct: '0.89'\n    Image Size: '260'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/efficientnet.py#L1219\n  Weights: https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb2_pruned_203f55bc.pth\n  Results:",
        "question": "What is the number of parameters in the efficientnet_b1_pruned model?\n",
        "answer": "6330000",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/efficientnet-pruned.md"
    },
    {
        "context": "```python\nimport numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport os\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm\n```\n\nWe're now ready to code our Q-Learning algorithm 🔥\n\n# Part 1: Frozen Lake ⛄ (non slippery version)\n\n## Create and understand [FrozenLake environment ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n---\n\n💡 A good habit when you start to use an environment is to check its documentation\n\n👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n\n---\n\nWe're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n\nWe can have two sizes of environment:\n\n- `map_name=\"4x4\"`: a 4x4 grid version\n- `map_name=\"8x8\"`: a 8x8 grid version\n\n\nThe environment has two modes:\n\n- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).\n\nFor now let's keep it simple with the 4x4 map and non-slippery.\nWe add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n\nAs [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) “rgb_array”: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n\n```python\n# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\nenv = gym.make()  # TODO use the correct parameters\n```\n\n### Solution",
        "question": "What is the name of the environment created?\n",
        "answer": "The name of the environment created is FrozenLake-v1.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx"
    },
    {
        "context": "Create a new file called `PdfUploadText.svelte` and copy the following code.\nIts creating a new div to display our \"upload text\" with some custom styling.\n\nTip: Notice that we're leveraging Gradio core's existing css variables here: `var(--size-60)` and `var(--body-text-color-subdued)`. This allows our component to work nicely in light mode and dark mode, as well as with Gradio's built-in themes.\n\n\n```ts\n<script lang=\"ts\">\n\timport { Upload as UploadIcon } from \"@gradio/icons\";\n\texport let hovered = false;\n\n</script>\n\n<div class=\"wrap\">\n\t<span class=\"icon-wrap\" class:hovered><UploadIcon /> </span>\n    Drop PDF\n    <span class=\"or\">- or -</span>\n    Click to Upload\n</div>\n\n<style>\n\t.wrap {\n\t\tdisplay: flex;\n\t\tflex-direction: column;\n\t\tjustify-content: center;\n\t\talign-items: center;\n\t\tmin-height: var(--size-60);\n\t\tcolor: var(--block-label-text-color);\n\t\tline-height: var(--line-md);\n\t\theight: 100%;\n\t\tpadding-top: var(--size-3);\n\t}\n\n\t.or {\n\t\tcolor: var(--body-text-color-subdued);\n\t\tdisplay: flex;\n\t}\n\n\t.icon-wrap {\n\t\twidth: 30px;\n\t\tmargin-bottom: var(--spacing-lg);\n\t}\n\n\t@media (--screen-md) {\n\t\t.wrap {\n\t\t\tfont-size: var(--text-lg);\n\t\t}\n\t}\n\n\t.hovered {\n\t\tcolor: var(--color-accent);\n\t}\n</style>\n```\n\nNow import `PdfUploadText.svelte` in your `<script>` and pass it to the `Upload` component!\n\n```ts\n\timport PdfUploadText from \"./PdfUploadText.svelte\";\n\n...\n\n    <Upload\n        filetype={\"application/pdf\"}\n        file_count=\"single\"\n        {root}\n    >\n        <PdfUploadText />\n    </Upload>\n```\n\nAfter saving your code, the frontend should now look like this:\n\n![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/better_upload.png)\n\n## Step 6: PDF Rendering logic\n\nThis is the most advanced javascript part.\nIt took me a while to figure it out!\nDo not worry if you have trouble, the important thing is to not be discouraged 💪\nAsk for help in the gradio [discord](https://discord.gg/hugging-face-879548962464493619) if you need and ask for help.",
        "question": "What is the filetype that the Upload component accepts?\n",
        "answer": "The Upload component accepts the filetype \"application/pdf\".",
        "source_doc": "gradio-app/gradio/blob/main/guides/05_custom-components/07_pdf-component-example.md"
    },
    {
        "context": "1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\n1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.",
        "question": "Which model was released by Meta AI?\n",
        "answer": "X-MOD",
        "source_doc": "huggingface/transformers/blob/main/README_te.md"
    },
    {
        "context": "Our user studies provided further clarity on the sections that different user profiles/stakeholders would find more challenging or easier to write. \n\nThe results illustrated below show that while the Bias, Risks and Limitations section ranks second for both model card writers and model card readers for *In what order do you write the model card and What section do you look at first*, respectively, it is also noted as the most challenging/longest section to write. This favoured/endorsed the need to further evaluate the Bias, Risks and Limitations sections in order to assist with writing this decisive/imperative section.\n\nThese templates were then used to generate model cards for the top 200 most downloaded Hugging Face (HF) models. \n\n* We first began by pulling all Hugging Face model's on the hub and, in particular, subsections on Limitations and Bias (\"Risks\" subsections were largely not present).\n* Based on inputs that were the most continuously used with a higher number of model downloads, grouped by model typed, the tool provides prompted text within the Bias, Risks and Limitations sections. We also prompt a default text if the model type is not specified.\n\nUsing this information, we returned back to our analysis of all model cards on the hub, coupled with suggestions from other researchers and peers at HF and additional research on the type of prompted information we could provide to users while they are creating model cards. These defaulted prompted text allowed us to satisfy the aims:\n\n1) For those who have not created model cards before or who do not usually make a model card or any other type of model documentation for their model’s, the prompted text enables these users to easily create a model card. This in turn increased the number of model cards created.\n   \n2) Users who already write model cards, the prompted text invites them to add more to their model card, further developing the content/standard of model cards. \n\n## User Study Details",
        "question": "What is the most challenging section to write in a model card?\n",
        "answer": "The Bias, Risks and Limitations section is the most challenging section to write in a model card.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/model-cards-user-studies.md"
    },
    {
        "context": "## Upgrading your Hardware (GPUs, TPUs, etc.)\n\nIf you have done everything above, and your demo is still not fast enough, you can upgrade the hardware that your model is running on. Changing the model from running on CPUs to running on GPUs will usually provide a 10x-50x increase in inference time for deep learning models.\n\nIt is particularly straightforward to upgrade your Hardware on Hugging Face Spaces. Simply click on the \"Settings\" tab in your Space and choose the Space Hardware you'd like.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings.png)\n\nWhile you might need to adapt portions of your machine learning inference code to run on a GPU (here's a [handy guide](https://cnvrg.io/pytorch-cuda/) if you are using PyTorch), Gradio is completely agnostic to the choice of hardware and will work completely fine if you use it with CPUs, GPUs, TPUs, or any other hardware!\n\nNote: your GPU memory is different than your CPU memory, so if you upgrade your hardware,\nyou might need to adjust the value of the `default_concurrency_limit` parameter described above.\n\n## Conclusion\n\nCongratulations! You know how to set up a Gradio demo for maximum performance. Good luck on your next viral demo!",
        "question": "How much faster is running a deep learning model on GPUs compared to CPUs?\n",
        "answer": "Running a deep learning model on GPUs is usually 10x-50x faster than running it on CPUs.",
        "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/setting-up-a-demo-for-maximum-performance.md"
    },
    {
        "context": "---\n\n\n# **SageMaker Hugging Face Inference Toolkit ⚙️**\n\nIn addition to the Hugging Face Transformers-optimized Deep Learning Containers for inference, we have created a new[ Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) for Amazon SageMaker. This new Inference Toolkit leverages the `pipelines` from the `transformers` library to allow zero-code deployments of models without writing any code for pre- or post-processing. In the \"Getting Started\" section below you find two examples of how to deploy your models to Amazon SageMaker.\n\nIn addition to the zero-code deployment, the Inference Toolkit supports \"bring your own code\" methods, where you can override the default methods. You can learn more about \"bring your own code\" in the documentation[ here](https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules) or you can check out the sample notebook \"deploy custom inference code to Amazon SageMaker\".\n\n\n## **API - Inference Toolkit Description**\n\nUsing the` transformers pipelines`, we designed an API, which makes it easy for you to benefit from all `pipelines` features. The API has a similar interface than the[ 🤗 Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. Below you can find examples for requests.",
        "question": "What is the name of the new Inference Toolkit for Amazon SageMaker?\n",
        "answer": "The name of the new Inference Toolkit for Amazon SageMaker is SageMaker Hugging Face Inference Toolkit.",
        "source_doc": "huggingface/blog/blob/main/deploy-hugging-face-models-easily-with-amazon-sagemaker.md"
    },
    {
        "context": "🤗 Evaluate also uses `flake8` and a few custom scripts to check for coding mistakes. Quality\n   control runs in CI, however you can also run the same checks with:\n\n   ```bash\n   $ make quality\n   ```\n\n   If you're modifying documents under `docs/source`, make sure to validate that\n   they can still be built. This check also runs in CI. To run a local check\n   make sure you have installed the documentation builder requirements. First you will need to clone the\n   repository containing our tools to build the documentation:\n   \n   ```bash\n   $ pip install git+https://github.com/huggingface/doc-builder\n   ```\n\n   Then, make sure you have all the dependencies to be able to build the doc with:\n   \n   ```bash\n   $ pip install \".[docs]\"\n   ```\n\n   Finally, run the following command from the root of the repository:\n\n   ```bash\n   $ doc-builder build evaluate docs/source/ --build_dir ~/tmp/test-build\n   ```\n\n   This will build the documentation in the `~/tmp/test-build` folder where you can inspect the generated\n   Markdown files with your favorite editor. You won't be able to see the final rendering on the website\n   before your PR is merged, we are actively working on adding a tool for this.\n\n   Once you're happy with your changes, add changed files using `git add` and\n   make a commit with `git commit` to record your changes locally:\n\n   ```bash\n   $ git add modified_file.py\n   $ git commit\n   ```\n\n   Please write [good commit\n   messages](https://chris.beams.io/posts/git-commit/).\n\n   It is a good idea to sync your copy of the code with the original\n   repository regularly. This way you can quickly account for changes:\n\n   ```bash\n   $ git fetch upstream\n   $ git rebase upstream/main\n   ```\n\n   Push the changes to your account using:\n\n   ```bash\n   $ git push -u origin a-descriptive-name-for-my-changes\n   ```\n\n6. Once you are satisfied, go to the webpage of your fork on GitHub. Click on 'Pull request' to send your changes\n   to the project maintainers for review.",
        "question": "How can I run a local check on the documentation of the repository?\n",
        "answer": "You can run a local check on the documentation of the repository by first installing the documentation builder requirements, cloning the repository containing the tools to build the documentation, and then running the `doc-builder build evaluate docs/source/ --build_dir ~/tmp/test-build` command from the root of the repository. The generated Markdown files can be inspected in the `~/tmp/test-build` folder.",
        "source_doc": "huggingface/evaluate/blob/main/CONTRIBUTING.md"
    },
    {
        "context": "1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (来自 CNRS) 伴随论文 [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) 由 Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab 发布。\n1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (来自 Facebook AI) 伴随论文 [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) 由 Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela 发布。\n1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (来自 Google Research) 伴随论文 [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) 由 James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon 发布。\n1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (来自 Microsoft Research) 伴随论文 [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) 由 Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao 发布。\n1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (来自 CMU/Google Brain) 伴随论文 [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) 由 Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le 发布。\n1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (来自 ADEPT) 伴随论文 [blog post](https://www.adept.ai/blog/fuyu-8b 由 Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar 发布。)",
        "question": "Which model is released by ADEPT?\n",
        "answer": "Fuyu",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hans.md"
    },
    {
        "context": "To enable DeepSpeed ZeRO Stage-2 without any code changes, please run `accelerate config` and leverage the [Accelerate DeepSpeed Plugin](https://huggingface.co/docs/accelerate/deepspeed#accelerate-deepspeed-plugin). \n\n**ZeRO Stage-2 DeepSpeed Plugin Example**\n```bash\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n gradient_accumulation_steps: 1\n gradient_clipping: 1.0\n offload_optimizer_device: none\n offload_param_device: none\n zero3_init_flag: false\n zero_stage: 2\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\nNow, run below command for training:\n```bash\naccelerate launch run_cls_no_trainer.py \\\n  --model_name_or_path \"microsoft/deberta-v2-xlarge-mnli\" \\\n  --task_name \"mrpc\" \\\n  --ignore_mismatched_sizes \\\n  --max_length 128 \\\n  --per_device_train_batch_size 40 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir \"/tmp/mrpc/deepspeed_stage2/\" \\\n  --with_tracking \\\n  --report_to \"wandb\" \\\n```\n\nIn our Single-Node Multi-GPU setup, the maximum batch size that DDP supports without OOM error is 8. In contrast, DeepSpeed Zero-Stage 2 enables batch size of 40 without running into OOM errors. Therefore, DeepSpeed enables to fit **5X** more data per GPU when compared to DDP. Below is the snapshot of the plots from wandb [run](https://wandb.ai/smangrul/DDP_vs_DeepSpeed_cls_task?workspace=user-smangrul) along with benchmarking table comparing DDP vs DeepSpeed. \n\n![Wandb Run](./assets/83_accelerate_deepspeed/cls_run.png)\n\n---\n| Method | Batch Size Max | Train time per epoch (seconds) | Eval time  per epoch (seconds) | F1 score | Accuracy |\n| --- | --- | --- | --- | --- | --- |\n| DDP (Distributed Data Parallel) | 8 | 103.57 | 2.04 | 0.931 | 0.904 |\n| DeepSpeed ZeRO Stage 2 | **40** | **28.98** | **1.79** | **0.936** | **0.912** |",
        "question": "What is the maximum batch size that DeepSpeed ZeRO Stage 2 supports without OOM errors in the Single-Node Multi-GPU setup?\n",
        "answer": "The maximum batch size that DeepSpeed ZeRO Stage 2 supports without OOM errors in the Single-Node Multi-GPU setup is 40.",
        "source_doc": "huggingface/blog/blob/main/accelerate-deepspeed.md"
    },
    {
        "context": "## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViTMatte.\n\n- A demo notebook regarding inference with [`VitMatteForImageMatting`], including background replacement, can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViTMatte).\n\n<Tip>\n\nThe model expects both the image and trimap (concatenated) as input. Use [`ViTMatteImageProcessor`] for this purpose.\n</Tip>\n\n## VitMatteConfig\n\n[[autodoc]] VitMatteConfig\n\n## VitMatteImageProcessor\n\n[[autodoc]] VitMatteImageProcessor\n    - preprocess\n\n## VitMatteForImageMatting\n\n[[autodoc]] VitMatteForImageMatting\n    - forward",
        "question": "What is the name of the demo notebook for inference with VitMatteForImageMatting?\n",
        "answer": "The name of the demo notebook for inference with VitMatteForImageMatting is [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViTMatte).",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/vitmatte.md"
    },
    {
        "context": "Latent Consistency Distillation Example:\n\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill stable-diffusion-v1.5 for inference with few timesteps.\n\n## Full model distillation\n\n### Running locally with PyTorch\n\n#### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\nThen cd in the example folder and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [🤗 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\nOr for a default accelerate configuration without answering questions about your environment\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell e.g. a notebook\n\n```python\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups.\n\n\n#### Example\n\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example, and for illustrative purposes only. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/). You may also need to search the hyperparameter space according to the dataset you use.",
        "question": "What is the recommended dataset for best results in the example?\n",
        "answer": "The recommended dataset for best results in the example is large and high-quality text-image datasets such as LAION.",
        "source_doc": "huggingface/diffusers/blob/main/examples/consistency_distillation/README.md"
    },
    {
        "context": "```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n\n### Training on a 16GB GPU:\n\nWith the help of gradient checkpointing and the 8-bit optimizer from bitsandbytes it's possible to run train dreambooth on a 16GB GPU.\n\nTo install `bitsandbytes` please refer to this [readme](https://github.com/TimDettmers/bitsandbytes#requirements--installation).\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n\n### Training on a 12GB GPU:",
        "question": "What is the batch size for training on a 12GB GPU?\n",
        "answer": "The batch size for training on a 12GB GPU is 1.",
        "source_doc": "huggingface/diffusers/blob/main/examples/dreambooth/README.md"
    },
    {
        "context": "This model was added by [Juarez Bochi](https://huggingface.co/jbochi). The original checkpoints can be found [here](https://github.com/google-research/google-research/tree/master/madlad_400). \n\nThis is a machine translation model that supports many low-resource languages, and that is competitive with models that are significantly larger.\n\nOne can directly use MADLAD-400 weights without finetuning the model:\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/madlad400-3b-mt\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/madlad400-3b-mt\")\n\n>>> inputs = tokenizer(\"<2pt> I love pizza!\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Eu amo pizza!']\n```\n\nGoogle has released the following variants:\n\n- [google/madlad400-3b-mt](https://huggingface.co/google/madlad400-3b-mt)\n\n- [google/madlad400-7b-mt](https://huggingface.co/google/madlad400-7b-mt)\n\n- [google/madlad400-7b-mt-bt](https://huggingface.co/google/madlad400-7b-mt-bt)\n\n- [google/madlad400-10b-mt](https://huggingface.co/google/madlad400-10b-mt)\n\nThe original checkpoints can be found [here](https://github.com/google-research/google-research/tree/master/madlad_400).\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for all API references, code examples, and notebooks. For more details regarding training and evaluation of the MADLAD-400, refer to the model card.\n\n</Tip>",
        "question": "Who added this model to Hugging Face?\n",
        "answer": "Juarez Bochi",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/madlad-400.md"
    },
    {
        "context": "A: Yes, you can download your trained model from S3 and directly use it with transformers or upload it to the [Hugging Face Model Hub](https://huggingface.co/models).\n\n_Q: How is my data and code secured by Amazon SageMaker?_\n\nA: Amazon SageMaker provides numerous security mechanisms including [encryption at rest](https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-at-rest-nbi.html) and [in transit](https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-in-transit.html), [Virtual Private Cloud (VPC) connectivity](https://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html) and [Identity and Access Management (IAM)](https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html). To learn more about security in the AWS cloud and with Amazon SageMaker, you can visit [Security in Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html) and [AWS Cloud Security](https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html).\n\n_Q: Is this available in my region?_\n\nA: For a list of the supported regions, please visit the [AWS region table](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/) for all AWS global infrastructure.\n\n_Q: Do I need to pay for a license from Hugging Face to use the DLCs?_\n\nA: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\n\n_Q: How can I run inference on my trained models?_",
        "question": "How are the Hugging Face DLCs licensed?\n",
        "answer": "The Hugging Face DLCs are open source and licensed under Apache 2.0.",
        "source_doc": "huggingface/blog/blob/main/the-partnership-amazon-sagemaker-and-hugging-face.md"
    },
    {
        "context": "1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (हुआवेई नूह के आर्क लैब से) साथ में कागज़ [NEZHA: चीनी भाषा समझ के लिए तंत्रिका प्रासंगिक प्रतिनिधित्व](https :/ /arxiv.org/abs/1909.00204) जुन्किउ वेई, ज़ियाओज़े रेन, ज़िआओगुआंग ली, वेनयोंग हुआंग, यी लियाओ, याशेंग वांग, जियाशू लिन, शिन जियांग, जिओ चेन और कुन लियू द्वारा।\n1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (फ्रॉम मेटा) साथ में पेपर [नो लैंग्वेज लेफ्ट बिहाइंड: स्केलिंग ह्यूमन-सेंटेड मशीन ट्रांसलेशन] (https://arxiv.org/abs/2207.04672) एनएलएलबी टीम द्वारा प्रकाशित।\n1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (Meta से) the NLLB team. द्वाराअनुसंधान पत्र [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) के साथ जारी किया गया\n1. **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (Meta AI से) Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. द्वाराअनुसंधान पत्र [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) के साथ जारी किया गया\n1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (विस्कॉन्सिन विश्वविद्यालय - मैडिसन से) साथ में कागज [Nyströmformer: A Nyström- आधारित एल्गोरिथम आत्म-ध्यान का अनुमान लगाने के लिए ](https://arxiv.org/abs/2102.03902) युनयांग ज़िओंग, झानपेंग ज़ेंग, रुद्रसिस चक्रवर्ती, मिंगक्सिंग टैन, ग्लेन फंग, यिन ली, विकास सिंह द्वारा पोस्ट किया गया।",
        "question": "Which model was released by SHI Labs with the paper Neighborhood Attention Transformer?\n",
        "answer": "NAT",
        "source_doc": "huggingface/transformers/blob/main/README_hd.md"
    },
    {
        "context": "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## Tips\n\nStable unCLIP takes  `noise_level` as input during inference which determines how much noise is added to the image embeddings. A higher `noise_level` increases variation in the final un-noised images. By default, we do not add any additional noise to the image embeddings (`noise_level = 0`).\n\n### Text-to-Image Generation\nStable unCLIP can be leveraged for text-to-image generation by pipelining it with the prior model of KakaoBrain's open source DALL-E 2 replication [Karlo](https://huggingface.co/kakaobrain/karlo-v1-alpha):\n\n```python\nimport torch\nfrom diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline\nfrom diffusers.models import PriorTransformer\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\nprior_model_id = \"kakaobrain/karlo-v1-alpha\"\ndata_type = torch.float16\nprior = PriorTransformer.from_pretrained(prior_model_id, subfolder=\"prior\", torch_dtype=data_type)",
        "question": "What is the name of the prior model used in Stable unCLIP for text-to-image generation?\n",
        "answer": "The name of the prior model used in Stable unCLIP for text-to-image generation is KakaoBrain's open source DALL-E 2 replication Karlo.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md"
    },
    {
        "context": "|      |                                                                            |[keyfan/vicuna-chinese-replication-beta](https://huggingface.co/keyfan/vicuna-chinese-replication-beta)|0           |6                        |llama-license                                                                                 |https://huggingface.co/keyfan/vicuna-chinese-replication-beta/blob/main/LICENSE               |[LICENSE](https://huggingface.co/keyfan/vicuna-chinese-replication-beta/blob/main/LICENSE)         |                                                                                                                     |                                                                                   |\n|      |                                                                            |[khachdallak/llama-13b-hf-new-tok](https://huggingface.co/khachdallak/llama-13b-hf-new-tok)|0           |0                        |llama-license                                                                                 |https://huggingface.co/khachdallak/llama-13b-hf-new-tok/blob/main/LICENSE                     |[LICENSE](https://huggingface.co/khachdallak/llama-13b-hf-new-tok/blob/main/LICENSE)               |                                                                                                                     |                                                                                   |",
        "question": "What license is used for the model keyfan/vicuna-chinese-replication-beta?\n",
        "answer": "The model keyfan/vicuna-chinese-replication-beta is licensed under the llama-license.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "### What you need to remember\n\n* Gradio will use the interactive version (if available) of a component if that component is used as the **input** to any event; otherwise, the static version will be used.\n\n* When you design custom components, you **must** accept the boolean interactive keyword in the constructor of your Python class. In the frontend, you **may** accept the `interactive` property, a `bool` which represents whether the component should be static or interactive. If you do not use this property in the frontend, the component will appear the same in interactive or static mode.\n\n## The value and how it is preprocessed/postprocessed\n\nThe most important attribute of a component is its `value`.\nEvery component has a `value`.\nThe value that is typically set by the user in the frontend (if the component is interactive) or displayed to the user (if it is static). \nIt is also this value that is sent to the backend function when a user triggers an event, or returned by the user's function e.g. at the end of a prediction.\n\nSo this value is passed around quite a bit, but sometimes the format of the value needs to change between the frontend and backend. \nTake a look at this example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\ndemo.launch()\n```",
        "question": "What is the attribute of a component that is passed around quite a bit?\n",
        "answer": "The value attribute of a component is passed around quite a bit.",
        "source_doc": "gradio-app/gradio/blob/main/guides/05_custom-components/02_key-component-concepts.md"
    },
    {
        "context": "Replace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\n\n## How do I finetune this model?\n\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n\n```py\n>>> model = timm.create_model('selecsls42b', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@article{Mehta_2020,\n   title={XNect},\n   volume={39},\n   ISSN={1557-7368},\n   url={http://dx.doi.org/10.1145/3386569.3392410},\n   DOI={10.1145/3386569.3392410},\n   number={4},\n   journal={ACM Transactions on Graphics},\n   publisher={Association for Computing Machinery (ACM)},\n   author={Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},\n   year={2020},\n   month={Jul}\n}\n```",
        "question": "How do I extract image features with the selecsls42b model?\n",
        "answer": "To extract image features with the selecsls42b model, follow the timm feature extraction examples, just change the name of the model you want to use.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/selecsls.mdx"
    },
    {
        "context": "Tips:\n\n- All ASR models accept a float array corresponding to the raw waveform of the speech signal. The raw waveform should be pre-processed with [`Wav2Vec2FeatureExtractor`].\n- The models were trained using connectionist temporal classification (CTC) so the model output has to be decoded using\n  [`Wav2Vec2CTCTokenizer`].\n- You can load different language adapter weights for different languages via [`~Wav2Vec2PreTrainedModel.load_adapter`]. Language adapters only consists of roughly 2 million parameters \n  and can therefore be efficiently loaded on the fly when needed.\n\n#### Loading\n\nBy default MMS loads adapter weights for English. If you want to load adapter weights of another language \nmake sure to specify `target_lang=<your-chosen-target-lang>` as well as `\"ignore_mismatched_sizes=True`.\nThe `ignore_mismatched_sizes=True` keyword has to be passed to allow the language model head to be resized according\nto the vocabulary of the specified language.\nSimilarly, the processor should be loaded with the same target language\n\n```py\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\n\nmodel_id = \"facebook/mms-1b-all\"\ntarget_lang = \"fra\"\n\nprocessor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)\n```\n\n<Tip>\n\nYou can safely ignore a warning such as:\n\n```text\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated\n- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\n</Tip>",
        "question": "How should the processor be loaded to use a language other than English?\n",
        "answer": "The processor should be loaded with the same target language as the model by specifying `target_lang=<your-chosen-target-lang>` in the `AutoProcessor.from_pretrained()` method.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md"
    },
    {
        "context": "Note: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a \nlarge model and a small batch size, the memory use will be larger.\n\n</Tip>\n\nYou can combine the above methods to get a cumulative effect. These techniques are available to you whether you are \ntraining your model with [`Trainer`] or writing a pure PyTorch loop, in which case you can [configure these optimizations \nwith 🤗 Accelerate](#using-accelerate).\n\nIf these methods do not result in sufficient gains, you can explore the following options: \n* [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\n* [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\n* [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)\n\nFinally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \nto a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \ntechniques outlined in the [multi-GPU section](perf_train_gpu_many). \n\n## Batch size choice\n\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \ninput/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \nhigher depending on the hardware being used and the model's dtype.\n\nFor reference, check out NVIDIA's recommendation for [input/output neuron counts](\nhttps://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and \n[batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for \nfully connected layers (which are involved in GEMMs (General Matrix Multiplications)).",
        "question": "What is the recommended batch size and input/output neuron count for optimal performance?\n",
        "answer": "The recommended batch size and input/output neuron count are of size 2^N, often a multiple of 8, but it can be higher depending on the hardware being used and the model's dtype.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md"
    },
    {
        "context": "Before we start, let's refresh our knowledge about Inference Endpoints.\n\n## 1. What is Hugging Face Inference Endpoints?\n\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.\n\nHere are some of the most important features:",
        "question": "What is the purpose of Hugging Face Inference Endpoints?\n",
        "answer": "The purpose of Hugging Face Inference Endpoints is to offer an easy and secure way to deploy Machine Learning models for use in production. It empowers developers and data scientists to create Generative AI applications without managing infrastructure, simplifying the deployment process to a few clicks, handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.",
        "source_doc": "huggingface/blog/blob/main/inference-endpoints-embeddings.md"
    },
    {
        "context": "To get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]\n\n>>> # Print top categories per image\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5_prob.size(0)):\n...     print(categories[top5_catid[i]], top5_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\n\n## How do I finetune this model?\n\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n\n```py\n>>> model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation",
        "question": "How do I extract image features with this model?\n",
        "answer": "To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/noisy-student.mdx"
    },
    {
        "context": "We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:\n\n```bash\ngit add .\n```\n\nWe can then have a look at the files that are currently staged:\n\n```bash\ngit status\n```\n\n{#if fw === 'pt'}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n  modified:   .gitattributes\n\tnew file:   config.json\n\tnew file:   pytorch_model.bin\n\tnew file:   sentencepiece.bpe.model\n\tnew file:   special_tokens_map.json\n\tnew file:   tokenizer.json\n\tnew file:   tokenizer_config.json\n```\n{:else}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n  modified:   .gitattributes\n  \tnew file:   config.json\n\tnew file:   sentencepiece.bpe.model\n\tnew file:   special_tokens_map.json\n\tnew file:   tf_model.h5\n\tnew file:   tokenizer.json\n\tnew file:   tokenizer_config.json\n```\n{/if}\n\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its `status` command:\n\n```bash\ngit lfs status\n```\n\n{#if fw === 'pt'}\n```bash\nOn branch main\nObjects to be pushed to origin/main:\n\n\nObjects to be committed:\n\n\tconfig.json (Git: bc20ff2)\n\tpytorch_model.bin (LFS: 35686c2)\n\tsentencepiece.bpe.model (LFS: 988bc5a)\n\tspecial_tokens_map.json (Git: cb23931)\n\ttokenizer.json (Git: 851ff3e)\n\ttokenizer_config.json (Git: f0f7783)\n\nObjects not staged for commit:\n\n\n```\n\nWe can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiece.bpe.model*, which have `LFS`. Great!\n\n{:else}\n```bash\nOn branch main\nObjects to be pushed to origin/main:\n\n\nObjects to be committed:\n\n\tconfig.json (Git: bc20ff2)\n\tsentencepiece.bpe.model (LFS: 988bc5a)\n\tspecial_tokens_map.json (Git: cb23931)\n\ttf_model.h5 (LFS: 86fce29)\n\ttokenizer.json (Git: 851ff3e)\n\ttokenizer_config.json (Git: f0f7783)",
        "question": "What command is used to check the status of files being tracked by git-lfs?\n",
        "answer": "The command used to check the status of files being tracked by git-lfs is `git lfs status`.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter4/3.mdx"
    },
    {
        "context": "The solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```",
        "question": "How to define a Textbox outside of the Blocks() scope in Gradio?\n",
        "answer": "Define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.",
        "source_doc": "gradio-app/gradio/blob/main/guides/03_building-with-blocks/02_controlling-layout.md"
    },
    {
        "context": "</Tip>\n\nNext, let's create the config and models as we did before:\n\n```py\nresnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\nresnet50d = ResnetModelForImageClassification(resnet50d_config)\n\npretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\nresnet50d.model.load_state_dict(pretrained_model.state_dict())\n```\n\nNow to send the model to the Hub, make sure you are logged in. Either run in your terminal:\n\n```bash\nhuggingface-cli login\n```\n\nor from a notebook:\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nYou can then push to your own namespace (or an organization you are a member of) like this:\n\n```py\nresnet50d.push_to_hub(\"custom-resnet50d\")\n```\n\nOn top of the modeling weights and the configuration in json format, this also copied the modeling and\nconfiguration `.py` files in the folder `custom-resnet50d` and uploaded the result to the Hub. You can check the result\nin this [model repo](https://huggingface.co/sgugger/custom-resnet50d).\n\nSee the [sharing tutorial](model_sharing) for more information on the push to Hub method.\n\n## Using a model with custom code\n\nYou can use any configuration, model or tokenizer with custom code files in its repository with the auto-classes and\nthe `from_pretrained` method. All files and code uploaded to the Hub are scanned for malware (refer to the [Hub security](https://huggingface.co/docs/hub/security#malware-scanning) documentation for more information), but you should still \nreview the model code and author to avoid executing malicious code on your machine. Set `trust_remote_code=True` to use\na model with custom code:\n\n```py\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\"sgugger/custom-resnet50d\", trust_remote_code=True)\n```",
        "question": "What is the name of the model that was pushed to the Hugging Face Model Hub?\n",
        "answer": "The model was pushed to the Hugging Face Model Hub with the name \"custom-resnet50d\".",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/custom_models.md"
    },
    {
        "context": "![VS Code extension](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_codellama/vscode.png \"VS Code extension\")\n\n## Evaluation\n\nLanguage models for code are typically benchmarked on datasets such as HumanEval. It consists of programming challenges where the model is presented with a function signature and a docstring and is tasked to complete the function body. The proposed solution is then verified by running a set of predefined unit tests. Finally, a pass rate is reported which describes how many solutions passed all tests. The pass@1 rate describes how often the model generates a passing solution when having one shot whereas pass@10 describes how often at least one solution passes out of 10 proposed candidates.\n\nWhile HumanEval is a Python benchmark there have been significant efforts to translate it to more programming languages and thus enable a more holistic evaluation. One such approach is [MultiPL-E](https://github.com/nuprl/MultiPL-E) which translates HumanEval to over a dozen languages. We are hosting a [multilingual code leaderboard](https://huggingface.co/spaces/bigcode/multilingual-code-evals) based on it to allow the community to compare models across different languages to evaluate which model fits their use-case best.",
        "question": "What is the name of the benchmark used for evaluating language models for code?\n",
        "answer": "HumanEval",
        "source_doc": "huggingface/blog/blob/main/codellama.md"
    },
    {
        "context": "Search index\n\n[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.elastic.co/elasticsearch/) enables searching for examples in a dataset. This can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task. For example, if you are working on a Open Domain Question Answering task, you may want to only return examples that are relevant to answering your question.\n\nThis guide will show you how to build an index for your dataset that will allow you to search it.\n\n## FAISS\n\nFAISS retrieves documents based on the similarity of their vector representations. In this example, you will generate the vector representations with the [DPR](https://huggingface.co/transformers/model_doc/dpr.html) model.\n\n1. Download the DPR model from 🤗 Transformers:\n\n```py\n>>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n>>> import torch\n>>> torch.set_grad_enabled(False)\n>>> ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n>>> ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n```\n\n2. Load your dataset and compute the vector representations:\n\n```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset('crime_and_punish', split='train[:100]')\n>>> ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\n```\n\n3. Create the index with [`Dataset.add_faiss_index`]:\n\n```py\n>>> ds_with_embeddings.add_faiss_index(column='embeddings')\n```\n\n4. Now you can query your dataset with the `embeddings` index. Load the DPR Question Encoder, and search for a question with [`Dataset.get_nearest_examples`]:",
        "question": "What is the indexing method used by FAISS?\n",
        "answer": "FAISS retrieves documents based on the similarity of their vector representations.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/faiss_es.mdx"
    },
    {
        "context": "🧨 Diffusers JAX integration offers a convenient way to run SDXL on TPU via [XLA](https://github.com/openxla/xla), and we built a demo to showcase it. You can try it out in [this Space](https://huggingface.co/spaces/google/sdxl) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.45.1/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"google/sdxl\"></gradio-app>\n\nUnder the hood, this demo runs on several TPU v5e-4 instances (each instance has 4 TPU chips) and takes advantage of parallelization to serve four large 1024×1024 images in about 4 seconds. This time includes format conversions, communications time, and frontend processing; the actual generation time is about 2.3s, as we'll see below!\n\nIn this blog post,\n1. [We describe why JAX + TPU + Diffusers is a powerful framework to run SDXL](#why-jax--tpu-v5e-for-sdxl)\n2. [Explain how you can write a simple image generation pipeline with Diffusers and JAX](#how-to-write-an-image-generation-pipeline-in-jax)\n3. [Show benchmarks comparing different TPU settings](#benchmark)\n\n## Why JAX + TPU v5e for SDXL?\n\nServing SDXL with JAX on Cloud TPU v5e with high performance and cost-efficiency is possible thanks to the combination of purpose-built TPU hardware and a software stack optimized for performance. Below we highlight two key factors: JAX just-in-time (jit) compilation and XLA compiler-driven parallelism with JAX pmap.\n\n#### JIT compilation",
        "question": "What is JIT compilation in JAX?\n",
        "answer": "JIT compilation in JAX is a feature that compiles JAX functions to machine code at runtime, allowing for faster execution.",
        "source_doc": "huggingface/blog/blob/main/sdxl_jax.md"
    },
    {
        "context": "Navigate to the directory containing the training scripts for fine-tuning Dreambooth with LoRA:\n\n```bash\ncd peft/examples/lora_dreambooth\n```\n\nSet up your environment: install PEFT, and all the required libraries. At the time of writing this guide we recommend \ninstalling PEFT from source.  \n\n```bash\npip install -r requirements.txt\npip install git+https://github.com/huggingface/peft\n```\n\n## Fine-tuning DreamBooth\n\nPrepare the images that you will use for fine-tuning the model. Set up a few environment variables: \n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\" \nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n```\n\nHere: \n- `INSTANCE_DIR`: The directory containing the images that you intend to use for training your model.\n- `CLASS_DIR`: The directory containing class-specific images. In this example, we use prior preservation to avoid overfitting and language-drift. For prior preservation, you need other images of the same class as part of the training process. However, these images can be generated and the training script will save them to a local path you specify here.\n- `OUTPUT_DIR`: The destination folder for storing the trained model's weights.\n\nTo learn more about DreamBooth fine-tuning with prior-preserving loss, check out the [Diffusers documentation](https://huggingface.co/docs/diffusers/training/dreambooth#finetuning-with-priorpreserving-loss).\n\nLaunch the training script with `accelerate` and pass hyperparameters, as well as LoRa-specific arguments to it such as:\n\n- `use_lora`: Enables LoRa in the training script. \n- `lora_r`:  The dimension used by the LoRA update matrices.\n- `lora_alpha`: Scaling factor.\n- `lora_text_encoder_r`: LoRA rank for text encoder.\n- `lora_text_encoder_alpha`: LoRA alpha (scaling factor) for text encoder.\n\nHere's what the full set of script arguments may look like:",
        "question": "What is the environment variable for the directory containing the images used for training the model in DreamBooth fine-tuning?\n",
        "answer": "INSTANCE_DIR",
        "source_doc": "huggingface/peft/blob/main/docs/source/task_guides/dreambooth_lora.md"
    },
    {
        "context": "mkdir coco_dataset\nmv train2014 coco_dataset/\nmv annotations coco_dataset/\n```\n\n### Prepare dataset files and split the dataset.\n\n```python\nimport json\nimport collections\n\nimages_dir = \"coco_dataset/train2014\"\nannotation_file = \"coco_dataset/annotations/captions_train2014.json\"\nwith open(annotation_file, \"r\") as f:\n    annotations = json.load(f)[\"annotations\"]\n\nimage_path_to_caption = collections.defaultdict(list)\nfor element in annotations:\n    caption = f\"{element['caption'].lower().rstrip('.')}\"\n    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n    image_path_to_caption[image_path].append(caption)\n\nlines = []\nfor image_path, captions in image_path_to_caption.items():\n    lines.append(json.dumps({\"image_path\": image_path, \"captions\": captions}))\n\ntrain_lines = lines[:-8000]\nvalid_line = lines[-8000:]\nwith open(\"coco_dataset/train_dataset.json\", \"w\") as f:\n    f.write(\"\\n\".join(train_lines))\n\nwith open(\"coco_dataset/valid_dataset.json\", \"w\") as f:\n    f.write(\"\\n\".join(valid_line))\n```\n\n> Note: The data loading and processing part of this script can still be improved for maximum performance. In particular one should decode the images beforehand and use those instead decoding them each time. If the dataset is small or if you have huge disk space the you could also pre-process all the dataset beforehand and then use it.\n\n## Train the model\nNext we can run the example script to train the model:",
        "question": "What is the name of the file where the annotations are stored?\n",
        "answer": "coco_dataset/annotations/captions_train2014.json",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/hybrid_clip/README.md"
    },
    {
        "context": "# take the first half of the audio sample\nsample[\"array\"] = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\ninputs = processor(\n    audio=sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    text=[\"80s blues track with groovy saxophone\"],\n    padding=True,\n    return_tensors=\"pt\",\n)\naudio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nLet's give it a listen:\n\n<audio controls>\n<source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ie_musicgen/musicgen_out_melody_minified.wav\" type=\"audio/wav\"> \nYour browser does not support the audio element. \n</audio> \n\nIn both the cases the `model.generate` method produces the audio and follows the same principles as text generation. You can read more about it in our [how to generate](https://huggingface.co/blog/how-to-generate) blog post.\n\nAlright! With the basic usage outlined above, let's deploy MusicGen for fun and profit!\n\nFirst, we'll define a custom handler in `handler.py`. We can use the [Inference Endpoints template](https://huggingface.co/docs/inference-endpoints/guides/custom_handler#3-customize-endpointhandler) and override the `__init__` and `__call__` methods with our custom inference code. `__init__` will initialize the model and the processor, and `__call__` will take the data and return the generated music. You can find the modified `EndpointHandler` class below. 👇 \n\n```python\nfrom typing import Dict, List, Any\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nimport torch\n\nclass EndpointHandler:\n    def __init__(self, path=\"\"):\n        # load model and processor from path\n        self.processor = AutoProcessor.from_pretrained(path)\n        self.model = MusicgenForConditionalGeneration.from_pretrained(path, torch_dtype=torch.float16).to(\"cuda\")",
        "question": "What is the torch data type used in the model?\n",
        "answer": "The torch data type used in the model is torch.float16.",
        "source_doc": "huggingface/blog/blob/main/run-musicgen-as-an-api.md"
    },
    {
        "context": "Let's take a look at alternatives and why this format is deemed interesting.\nThis is my very personal and probably biased view:\n\n| Format                  | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\n| pickle (PyTorch)        | ✗ | ✗ | ✗ | 🗸 | ✗ | 🗸 | 🗸 |\n| H5 (Tensorflow)         | 🗸 | ✗ | 🗸 | 🗸 | ~ | ~ | ✗ |\n| SavedModel (Tensorflow) | 🗸 | ✗ | ✗ | 🗸 | 🗸 | ✗ | 🗸 |\n| MsgPack (flax)          | 🗸 | 🗸 | ✗ | 🗸 | ✗ | ✗ | 🗸 |\n| Protobuf (ONNX)         | 🗸 | ✗ | ✗ | ✗ | ✗ | ✗ | 🗸 |\n| Cap'n'Proto             | 🗸 | 🗸 | ~ | 🗸 | 🗸 | ~ | ✗ |\n| Arrow                   | ? | ? | ? | ? | ? | ? | ✗ |\n| Numpy (npy,npz)         | 🗸 | ? | ? | ✗ | 🗸 | ✗ | ✗ |\n| pdparams (Paddle)       | ✗ | ✗ | ✗ | 🗸 | ✗ | 🗸 | 🗸 |\n| SafeTensors             | 🗸 | 🗸 | 🗸 | 🗸 | 🗸 | ✗ | 🗸 |\n\n- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)\n- Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are\nnecessary)? This is becoming increasingly important in the ML world.\n\n\n### Main oppositions",
        "question": "What is the difference between H5 and SavedModel in terms of lazy loading?\n",
        "answer": "H5 supports lazy loading, while SavedModel does not.",
        "source_doc": "huggingface/safetensors/blob/main/README.md"
    },
    {
        "context": "his demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has a separate button for explaining the prediction.",
        "question": "How many inputs does the demo take in?\n",
        "answer": "The demo takes in 12 inputs from the user.",
        "source_doc": "gradio-app/gradio/blob/main/demo/xgboost-income-prediction-with-explainability/DESCRIPTION.md"
    },
    {
        "context": "## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n\n## Further References",
        "question": "Who are the authors of the Scikit-learn: Machine Learning in Python article?\n",
        "answer": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay",
        "source_doc": "huggingface/evaluate/blob/main/metrics/accuracy/README.md"
    },
    {
        "context": "The victory is a testament to the power of deep learning, and to the incredible work of our\nresearch team, which has been at the forefront of AI research for the past five years. AlphaGo\nis one of the most advanced Go programs ever created, and its performance is an important step\ntowards the goal of human-level AI.\n\n\"This is the culmination of a decade of hard work,\" said Andy Ng, co-founder and CTO of DeepMind.\n\"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that\ncan be used in a wide range of applications and to help people live better lives.\"\n\nDeepMind's work on Go began in 2010, when it began to train a neural network to play Go using\nmillions of games played by top Go players around the world. Since then, the team has refined the\nalgorithm, adding more and more layers of reinforcement learning to make it better at recognizing\npatterns and making decisions based on those patterns. In the past year and a half, the team has\nmade significant progress in the game, winning a record-tying 13 games in a row to move into the\ntop four of the world rankings.\n\n\"The game of Go is a complex game in which players have to be very careful not to overextend their\nterritory, and this is something that we have been able to improve over and over again,\" said\nDr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. \"We are very proud of our\nteam's work, and we hope that it will inspire others to take the next step in their research and\napply the same techniques to other problems.\"",
        "question": "When did DeepMind's work on Go begin?\n",
        "answer": "DeepMind's work on Go began in 2010.",
        "source_doc": "huggingface/blog/blob/main/introducing-csearch.md"
    },
    {
        "context": "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# What 🤗 Transformers can do\n\n🤗 Transformers is a library of pretrained state-of-the-art models for natural language processing (NLP), computer vision, and audio and speech processing tasks. Not only does the library contain Transformer models, but it also has non-Transformer models like modern convolutional networks for computer vision tasks. If you look at some of the most popular consumer products today, like smartphones, apps, and televisions, odds are that some kind of deep learning technology is behind it. Want to remove a background object from a picture taken by your smartphone? This is an example of a panoptic segmentation task (don't worry if you don't know what this means yet, we'll describe it in the following sections!). \n\nThis page provides an overview of the different speech and audio, computer vision, and NLP tasks that can be solved with the 🤗 Transformers library in just three lines of code!\n\n## Audio",
        "question": "What is an example of a task that can be solved with the 🤗 Transformers library in the audio domain?\n",
        "answer": "An example of a task that can be solved with the 🤗 Transformers library in the audio domain is removing a background object from a picture taken by a smartphone. This is an example of a panoptic segmentation task.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/task_summary.md"
    },
    {
        "context": "<!--\nType: model-index\nCollections:\n- Name: Ensemble Adversarial\n  Paper:\n    Title: Adversarial Attacks and Defences Competition\n    URL: https://paperswithcode.com/paper/adversarial-attacks-and-defences-competition\nModels:\n- Name: ens_adv_inception_resnet_v2\n  In Collection: Ensemble Adversarial\n  Metadata:\n    FLOPs: 16959133120\n    Parameters: 55850000\n    File Size: 223774238\n    Architecture:\n    - 1x1 Convolution\n    - Auxiliary Classifier\n    - Average Pooling\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inception-v3 Module\n    - Max Pooling\n    - ReLU\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: ens_adv_inception_resnet_v2\n    Crop Pct: '0.897'\n    Image Size: '299'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/inception_resnet_v2.py#L351\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 1.0%\n      Top 5 Accuracy: 17.32%\n-->",
        "question": "What is the top 1 accuracy of ens_adv_inception_resnet_v2 on ImageNet?\n",
        "answer": "The top 1 accuracy of ens_adv_inception_resnet_v2 on ImageNet is 1.0%.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md"
    },
    {
        "context": "The same works for methods so you can either use \\[\\`XXXClass.method\\`\\] or\n\\[~\\`XXXClass.method\\`\\].\n\n### Defining arguments in a method\n\nArguments should be defined with the `Args:` (or `Arguments:` or `Parameters:`)\nprefix, followed by a line return and an indentation. The argument should be\nfollowed by its type, with its shape if it is a tensor, a colon and its\ndescription:\n\n```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nIf the description is too long to fit in one line, another indentation is\nnecessary before writing the description after the argument.\n\nHere's an example showcasing everything so far:\n\n```\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AlbertTokenizer`]. See [`~PreTrainedTokenizer.encode`] and\n            [`~PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n```\n\nFor optional arguments or arguments with defaults we follow the following\nsyntax: imagine we have a function with the following signature:\n\n```\ndef my_function(x: str = None, a: float = 1):\n```\n\nthen its documentation should look like this:\n\n```\n    Args:\n        x (`str`, *optional*):\n            This argument controls ...\n        a (`float`, *optional*, defaults to 1):\n            This argument is used to ...\n```\n\nNote that we always omit the \"defaults to \\`None\\`\" when None is the default for\nany argument. Also note that even if the first line describing your argument\ntype and its default gets long, you can't break it on several lines. You can\nhowever write as many lines as you want in the indented description (see the\nexample above with `input_ids`).\n\n### Writing a multi-line code block\n\nMulti-line code blocks can be useful for displaying examples. They are done\nbetween two lines of three backticks as usual in Markdown:",
        "question": "How should multi-line code blocks be written in Markdown?\n",
        "answer": "Multi-line code blocks can be written between two lines of three backticks in Markdown.",
        "source_doc": "huggingface/optimum/blob/main/docs/README.md"
    },
    {
        "context": "ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizations. The memory optimizations enable ONNX Runtime to maximize the batch size and utilize the available memory efficiently whereas the compute optimizations speed up the training time. These optimizations include, but are not limited to, efficient memory planning, kernel optimizations, multi tensor apply for Adam Optimizer (which batches the elementwise updates applied to all the model’s parameters into one or a few kernel launches), FP16 optimizer (which eliminates a lot of device to host memory copies), mixed precision training and graph optimizations like node fusions and node eliminations. ONNX Runtime Training supports both [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) and [AMD GPUs](https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/), and offers extensibility with custom operators.\n\nIn short, it empowers AI developers to take full advantage of the ecosystem they are familiar with, like PyTorch and Hugging Face, and use acceleration from ONNX Runtime on the target device of their choice to save both time and resources.\n\n## ONNX Runtime Training in Optimum",
        "question": "What does ONNX Runtime Training support in terms of hardware?\n",
        "answer": "ONNX Runtime Training supports both NVIDIA and AMD GPUs.",
        "source_doc": "huggingface/blog/blob/main/optimum-onnxruntime-training.md"
    },
    {
        "context": "Pre-tokenizers\n\n<tokenizerslangcontent>\n<python>\n## BertPreTokenizer\n\n[[autodoc]] tokenizers.pre_tokenizers.BertPreTokenizer\n\n## ByteLevel\n\n[[autodoc]] tokenizers.pre_tokenizers.ByteLevel\n\n## CharDelimiterSplit\n\n[[autodoc]] tokenizers.pre_tokenizers.CharDelimiterSplit\n\n## Digits\n\n[[autodoc]] tokenizers.pre_tokenizers.Digits\n\n## Metaspace\n\n[[autodoc]] tokenizers.pre_tokenizers.Metaspace\n\n## PreTokenizer\n\n[[autodoc]] tokenizers.pre_tokenizers.PreTokenizer\n\n## Punctuation\n\n[[autodoc]] tokenizers.pre_tokenizers.Punctuation\n\n## Sequence\n\n[[autodoc]] tokenizers.pre_tokenizers.Sequence\n\n## Split\n\n[[autodoc]] tokenizers.pre_tokenizers.Split\n\n## UnicodeScripts\n\n[[autodoc]] tokenizers.pre_tokenizers.UnicodeScripts\n\n## Whitespace\n\n[[autodoc]] tokenizers.pre_tokenizers.Whitespace\n\n## WhitespaceSplit\n\n[[autodoc]] tokenizers.pre_tokenizers.WhitespaceSplit\n</python>\n<rust>\nThe Rust API Reference is available directly on the [Docs.rs](https://docs.rs/tokenizers/latest/tokenizers/) website.\n</rust>\n<node>\nThe node API has not been documented yet.\n</node>\n</tokenizerslangcontent>",
        "question": "What is the name of the pre-tokenizer that splits text by whitespace?\n",
        "answer": "WhitespaceSplit",
        "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/api/pre-tokenizers.mdx"
    },
    {
        "context": "### `To use the type as a Parameter, please correct the detach() semantics defined by __torch_dispatch__() implementation.`\n\nUse the latest version of `accelerate` with a command such as: `pip install -U accelerate` and the problem should be solved.\n\n### `Parameter has no attribue .CB` \n\nSame solution as above.\n\n### `RuntimeError: CUDA error: an illegal memory access was encountered ... consider passing CUDA_LAUNCH_BLOCKING=1`\n\nRun your script by pre-pending `CUDA_LAUNCH_BLOCKING=1` and you should observe an error as described in the next section.\n\n### `CUDA illegal memory error: an illegal memory access at line...`:\n\nCheck the CUDA verisons with:\n```\nnvcc --version\n```\nand confirm it is the same version as the one detected by `bitsandbytes`. If not, run:\n```\nls -l $CONDA_PREFIX/lib/libcudart.so\n```\nor \n```\nls -l $LD_LIBRARY_PATH\n```\nCheck if `libcudart.so` has a correct symlink that is set. Sometimes `nvcc` detects the correct CUDA version but `bitsandbytes` doesn't. You have to make sure that the symlink that is set for the file `libcudart.so` is redirected to the correct CUDA file. \n\nHere is an example of a badly configured CUDA installation:\n\n`nvcc --version` gives:\n\n![Screenshot 2022-08-15 at 15.12.23.png](https://cdn-uploads.huggingface.co/production/uploads/1660569220888-62441d1d9fdefb55a0b7d12c.png)\n\nwhich means that the detected CUDA version is 11.3 but `bitsandbytes` outputs:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1660569284243-62441d1d9fdefb55a0b7d12c.png)\n\nFirst check:\n\n```bash\necho $LD_LIBRARY_PATH\n```\n\nIf this contains multiple paths separated by `:`. Then you have to make sure that the correct CUDA version is set. By doing:\n\n```bash\nls -l $path/libcudart.so\n```\n\nOn each path (`$path`) separated by `:`.\nIf not, simply run\n```bash\nls -l $LD_LIBRARY_PATH/libcudart.so\n```\n\nand you can see",
        "question": "How to solve the error \"CUDA illegal memory error: an illegal memory access at line...\"?\n",
        "answer": "To solve this error, check the CUDA versions with `nvcc --version` and confirm it is the same version as the one detected by `bitsandbytes`. If not, run `ls -l $CONDA_PREFIX/lib/libcudart.so` or `ls -l $LD_LIBRARY_PATH` to check if `libcudart.so` has a correct symlink that is set. Sometimes `nvcc` detects the correct CUDA version but `bitsandbytes` doesn't. You have to make sure that the symlink that is set for the file `libcudart.so` is redirected to the correct CUDA file.",
        "source_doc": "huggingface/transformers/blob/main/tests/quantization/bnb/README.md"
    },
    {
        "context": "Enterprise Hub\n\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant and managed collaboration for companies and teams on Hugging Face.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/storage-regions/enterprise-hub.png)\n\nIn this section we will document the following Enterprise Hub features:\n\n- [SSO](./enterprise-sso)\n- [Audit Logs](./audit-logs)\n- [Storage Regions](./storage-regions)",
        "question": "What is one feature of Enterprise Hub?\n",
        "answer": "One feature of Enterprise Hub is SSO (Single Sign-On).",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/enterprise-hub.md"
    },
    {
        "context": "|      |      |[aipicasso/cool-japan-diffusion-2-1-0-beta](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-0-beta)                                      |597         |30      |                         |                                                                                   |[MODEL-LICENSE](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-0-beta/blob/main/MODEL-LICENSE)               |                                                                                                    |             |\n|      |      |[aipicasso/cool-japan-diffusion-2-1-1](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-1)                                                |547         |20      |                         |                                                                                   |[MODEL-LICENSE](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-1/blob/main/MODEL-LICENSE)                    |                                                                                                    |             |\n|      |      |[TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ)                                          |540         |15      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ/blob/main/LICENSE.txt)                     |                                                                                                    |             |",
        "question": "What is the license of the model aipicasso/cool-japan-diffusion-2-1-0-beta?\n",
        "answer": "The license of the model aipicasso/cool-japan-diffusion-2-1-0-beta is MODEL-LICENSE.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md"
    },
    {
        "context": "### Prompt weighting\n\nPrompt weighting provides a quantifiable way to scale the representation of concepts in a prompt. You can use it to increase or decrease the magnitude of the text embedding vector for each concept in the prompt, which subsequently determines how much of each concept is generated. The [Compel](https://github.com/damian0815/compel) library offers an intuitive syntax for scaling the prompt weights and generating the embeddings. Learn how to create the embeddings in the [Prompt weighting](../using-diffusers/weighted_prompts) guide.\n\nOnce you've generated the embeddings, pass them to the `prompt_embeds` (and `negative_prompt_embeds` if you're using a negative prompt) parameter in the [`AutoPipelineForInpainting`]. The embeddings replace the `prompt` parameter:\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16,\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage = pipeline(prompt_embeds=prompt_embeds, # generated from Compel\n    negative_prompt_embeds=negative_prompt_embeds, # generated from Compel\n    image=init_image,\n    mask_image=mask_image\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n### ControlNet\n\nControlNet models are used with other diffusion models like Stable Diffusion, and they provide an even more flexible and accurate way to control how an image is generated. A ControlNet accepts an additional conditioning image input that guides the diffusion model to preserve the features in it.\n\nFor example, let's condition an image with a ControlNet pretrained on inpaint images:",
        "question": "What is a ControlNet used for in the context of diffusion models?\n",
        "answer": "A ControlNet is used with other diffusion models like Stable Diffusion to provide a more flexible and accurate way to control how an image is generated. It accepts an additional conditioning image input that guides the diffusion model to preserve the features in it.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md"
    },
    {
        "context": "```python\nfrom transformers import BertGenerationTokenizer\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\n\ntokenizer = BertGenerationTokenizer.from_pretrained(\"/path/big/bird/folder\")\n\ninput_ids = tokenizer(input_str).input_ids\n```\n\nWhen both `input_ids` yield the same values, as a final step a tokenizer\ntest file should also be added.\n\nSince BigBird is most likely fully based on `BertGenerationTokenizer`, \nyou should only add a couple of \"slow\" integration tests. However, in this \ncase you do **not** need to add any `BigBirdTokenizationTest`.\n\n**10. Run End-to-end integration tests**\n\nHaving added the tokenizer, you should also add a couple of end-to-end\nintegration tests using both the model and the tokenizer to\n`tests/test_modeling_big_bird.py` in 🤗 Transformers. Such a test\nshould show on a meaningful text-to-text sample that the 🤗 Transformers\nimplementation works as expected. A meaningful text-to-text sample can\ninclude, *e.g.*, a source-to-target-translation pair, an\narticle-to-summary pair, a question-to-answer pair, etc... If none of\nthe ported checkpoints has been fine-tuned on a downstream task it is\nenough to simply rely on the model tests. In a final step to ensure that\nthe model is fully functional, it is advised that you also run all tests\non GPU. It can happen that you forgot to add some `.to(self.device)`\nstatements to internal tensors of the model, which in such a test would\nshow in an error. In case you have no access to a GPU, the Hugging Face\nteam can take care of running those tests for you.\n\n**11. Add Docstring**",
        "question": "What should be added to 🤗 Transformers as a final step to ensure that the model is fully functional?\n",
        "answer": "As a final step to ensure that the model is fully functional, it is advised that you run all tests on GPU.",
        "source_doc": "huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md"
    },
    {
        "context": "\"plot\": \"The film is centered on Mortal Kombat, a fighting tournament between the representatives of the realms of Earth and Outworld conceived by the Elder Gods amid looming invasion of the Earth by Outworld. If the realm of Outworld wins Mortal Kombat ten consecutive times, its Emperor Shao Kahn will be able to invade and conquer the Earth realm.\\nShaolin monk Liu Kang and his comrades, movie star Johnny Cage and military officer Sonya Blade were handpicked by Raiden, the god of thunder and defender of the Earth realm, to overcome their powerful adversaries in order to prevent Outworld from winning their tenth straight Mortal Kombat tournament. Each of the three has his or her own reason for competing: Liu seeks revenge against the tournament host Shang Tsung for killing his brother Chan; Sonya seeks revenge on an Australian crime lord Kano; and Cage, having been branded as a fake by the media, seeks to prove otherwise.\\nAt Shang Tsung's island, Liu is attracted to Princess Kitana, Shao Kahn's adopted daughter. Aware that Kitana is a dangerous adversary because she is the rightful heir to Outworld and that she will attempt to ally herself with the Earth warriors, Tsung orders the creature Reptile to spy on her. Liu defeats his first opponent and Sonya gets her revenge on Kano by snapping his neck. Cage encounters and barely beats Scorpion. Liu engages in a brief duel with Kitana, who secretly offers him cryptic advice for his next battle. Liu's next opponent is Sub-Zero, whose defense seems untouched because of his freezing abilities, until Liu recalls Kitana's advice and uses it to kill Sub-Zero.\\nPrince Goro enters the tournament and mercilessly crushes every opponent he faces. One of Cage's peers, Art Lean, is defeated by Goro as well and has his soul taken by Shang Tsung. Sonya worries that they may not win against Goro, but Raiden disagrees. He reveals their own fears and egos are preventing them from winning the tournament",
        "question": "Who is the host of the Mortal Kombat tournament?\n",
        "answer": "Shang Tsung",
        "source_doc": "huggingface/datasets-server/blob/main/docs/source/rows.mdx"
    },
    {
        "context": "```python\nextends Node3D\n\n@export var rotation_speed = 3.0\n@onready var ball = get_node(\"../Ball\")\n@onready var ai_controller = $AIController3D\n\nfunc _ready():\n\tai_controller.init(self)\n\nfunc game_over():\n\tai_controller.done = true\n\tai_controller.needs_reset = true\n\nfunc _physics_process(delta):\n\tif ai_controller.needs_reset:\n\t\tai_controller.reset()\n\t\tball.reset()\n\t\treturn\n\n\tvar movement : float\n\tif ai_controller.heuristic == \"human\":\n\t\tmovement = Input.get_axis(\"rotate_anticlockwise\", \"rotate_clockwise\")\n\telse:\n\t\tmovement = ai_controller.move_action\n\trotate_y(movement*delta*rotation_speed)\n\nfunc _on_area_3d_body_entered(body):\n\tai_controller.reward += 1.0\n```\n\nWe now need to synchronize between the game running in Godot and the neural network being trained in Python. Godot RL agents provides a node that does just that. Open the train.tscn scene, right click on the root node, and click “Add child node”. Then, search for “sync” and add a Godot RL Agents Sync node. This node handles the communication between Python and Godot over TCP.\n\nYou can run training live in the the editor, by first launching the python training with `gdrl`\n\nIn this simple example, a reasonable policy is learned in several minutes. You may wish to speed up training, click on the Sync node in the train scene and you will see there is a “Speed Up” property exposed in the editor:\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/godot6.png\" alt=\"Godot\">\n\nTry setting this property up to 8 to speed up training. This can be a great benefit on more complex environments, like the multi-player FPS we will learn about in the next chapter.\n\n### There’s more!\n\nWe have only scratched the surface of what can be achieved with Godot RL Agents, the library includes custom sensors and cameras to enrich the information available to the agent. Take a look at the [examples](https://github.com/edbeeching/godot_rl_agents_examples) to find out more!\n\n## Author",
        "question": "What is the name of the node that handles the communication between Python and Godot over TCP?\n",
        "answer": "Godot RL Agents Sync",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx"
    },
    {
        "context": "### Training with gradient checkpointing and 8-bit optimizer:\n\nWith the help of gradient checkpointing and the 8-bit optimizer from bitsandbytes it's possible to run train dreambooth on a 16GB GPU.\n\nTo install `bitandbytes` please refer to this [readme](https://github.com/TimDettmers/bitsandbytes#requirements--installation).\n\n```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n### Fine-tune text encoder with the UNet.\n\nThe script also allows to fine-tune the `text_encoder` along with the `unet`. It's been observed experimentally that fine-tuning `text_encoder` gives much better results especially on faces. \nPass the `--train_text_encoder` argument to the script to enable training `text_encoder`.\n\n___Note: Training text encoder requires more memory, with this option the training won't fit on 16GB GPU. It needs at least 24GB VRAM.___\n\n```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"",
        "question": "How much VRAM is required to fine-tune the text encoder along with the UNet?\n",
        "answer": "At least 24GB VRAM is required to fine-tune the text encoder along with the UNet.",
        "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/README.md"
    },
    {
        "context": "<Tip>\n\n✏️ **Try it out!** Run the code snippet above several times to see the random masking happen in front of your very eyes! Also replace the `tokenizer.decode()` method with `tokenizer.convert_ids_to_tokens()` to see that the tokens from a given word are always masked together.\n\n</Tip>\n\nNow that we have two data collators, the rest of the fine-tuning steps are standard. Training can take a while on Google Colab if you're not lucky enough to score a mythical P100 GPU 😭, so we'll first downsample the size of the training set to a few thousand examples. Don't worry, we'll still get a pretty decent language model! A quick way to downsample a dataset in 🤗 Datasets is via the `Dataset.train_test_split()` function that we saw in [Chapter 5](/course/chapter5):\n\n```python\ntrain_size = 10_000\ntest_size = int(0.1 * train_size)\n\ndownsampled_dataset = lm_datasets[\"train\"].train_test_split(\n    train_size=train_size, test_size=test_size, seed=42\n)\ndownsampled_dataset\n```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n        num_rows: 1000\n    })\n})\n```\n\nThis has automatically created new `train` and `test` splits, with the training set size set to 10,000 examples and the validation set to 10% of that -- feel free to increase this if you have a beefy GPU! The next thing we need to do is log in to the Hugging Face Hub. If you're running this code in a notebook, you can do so with the following utility function:\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nwhich will display a widget where you can enter your credentials. Alternatively, you can run: \n\n```\nhuggingface-cli login\n```\n\nin your favorite terminal and log in there. \n\n{#if fw === 'tf'}",
        "question": "What is the name of the function used to log in to the Hugging Face Hub in a terminal?\n",
        "answer": "The name of the function used to log in to the Hugging Face Hub in a terminal is `huggingface-cli login`.\n\n{#endif}",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/3.mdx"
    },
    {
        "context": "The results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.** We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds.\n\n<center>\n\n|   | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\n|:-:|:-------:|:--------------:|:------------:|\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\n| Speedup | / | x2.44 | x1.0 |\n\n</center>\n\n*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpointing enabled. All runs were *distributed* runs on *8 devices*.\n\n\n## Conclusion\n\nIn this article, we discuss our first experience with Gaudi2. The transition from first generation Gaudi to Gaudi2 is completely seamless since SynapseAI, Habana's SDK, is fully compatible with both. This means that new optimizations proposed by future releases will benefit both of them.\n\nYou have seen that Habana Gaudi2 significantly improves performance over first generation Gaudi and delivers about twice the throughput speed as Nvidia A100 80GB for both training and inference.\n\nYou also know now how to setup a Gaudi2 instance through the Intel Developer Zone. Check out the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples) you can easily run on it with 🤗 Optimum Habana.",
        "question": "How much faster is Gaudi2 than A100 80GB?\n",
        "answer": "Gaudi2 is x2.44 faster than A100 80GB.",
        "source_doc": "huggingface/blog/blob/main/habana-gaudi-2-benchmark.md"
    },
    {
        "context": "pipe.enable_model_cpu_offload()\nprompt='A robot pokemon, 4k photo'\nimages = pipe(prompt=prompt, negative_prompt=negative_prompt).images\nimages[0]\n```\n\nIf you want to use a fine-tuned decoder checkpoint along with your fine-tuned prior checkpoint, you can simply replace the \"kandinsky-community/kandinsky-2-2-decoder\" in above code with your custom model repo name. Note that in order to be able to create a `KandinskyV22CombinedPipeline`, your model repository need to have a prior tag. If you have created your model repo using our training script, the prior tag is automatically included. \n\n#### Training with multiple GPUs\n\n`accelerate` allows for seamless multi-GPU training. Follow the instructions [here](https://huggingface.co/docs/accelerate/basic_tutorials/launch)\nfor running distributed training with `accelerate`. Here is an example command:\n\n```bash\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu  train_text_to_image_decoder.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot pokemon, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-decoder-pokemon-model\" \n```\n\n\n#### Training with Min-SNR weighting\n\nWe support training with the Min-SNR weighting strategy proposed in [Efficient Diffusion Training via Min-SNR Weighting Strategy](https://arxiv.org/abs/2303.09556) which helps achieve faster convergence\nby rebalancing the loss. Enable the `--snr_gamma` argument and set it to the recommended\nvalue of 5.0.\n\n\n## Training with LoRA",
        "question": "What is the recommended value for the snr_gamma argument in the context?\n",
        "answer": "The recommended value for the snr_gamma argument is 5.0.",
        "source_doc": "huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md"
    },
    {
        "context": "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UniSpeech-SAT\n\n## Overview\n\nThe UniSpeech-SAT model was proposed in [UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware\nPre-Training](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen,\nShujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu .\n\nThe abstract from the paper is the following:",
        "question": "What are the benchmark datasets that the UniSpeech-SAT model achieves state-of-the-art performance",
        "answer": "The UniSpeech-SAT model can be fine-tuned for various downstream tasks, such as automatic speech recognition (ASR), speaker identification (SID), and speaker verification (SV).\n\nFactoid question: What are the benchmark datasets that the UniSpeech-SAT model achieves state-of-the-art performance",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/unispeech-sat.md"
    },
    {
        "context": "processor = AutoProcessor.from_pretrained(model_id)\n```\n\nLet's load the English speech transcription dataset that we will use for benchmarking. We'll load a small dataset \nconsisting of 73 samples from the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) validation-clean \ndataset. This amounts to ~9MB of data, so it's very lightweight and quick to download on device:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n```\n\nFor the benchmark, we only want to measure the generation time, so let's write a short helper function that measures \nthis step. The following function will return both the decoded tokens and the time it took to run the model:\n\n```python\nimport time\n\ndef generate_with_time(model, inputs, **kwargs):\n    start_time = time.time()\n    outputs = model.generate(**inputs, **kwargs)\n    generation_time = time.time() - start_time\n    return outputs, generation_time\n```\n\nWe can now iterate over the audio samples in our dataset and sum up the overall generation time:\n\n```python\nfrom tqdm import tqdm\n\nall_time = 0\npredictions = []\nreferences = []\n\nfor sample in tqdm(dataset):\n    audio = sample[\"audio\"]\n    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n    inputs = inputs.to(device=device, dtype=torch.float16)\n    \n    output, gen_time = generate_with_time(model, inputs)\n    all_time += gen_time\n    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])\n    references.append(processor.tokenizer._normalize(sample[\"text\"]))\n\nprint(all_time)\n```\n\n**Output:**\n```\n100%|██████████| 73/73 [01:37<00:00,  1.33s/it]\n72.99542546272278\n```\n\nAlright! We see that transcribing the 73 samples took 73 seconds. Let's check the WER of the predictions:\n\n```python\nfrom evaluate import load",
        "question": "How long did it take to transcribe the 73 samples?\n",
        "answer": "It took 73 seconds to transcribe the 73 samples.\n```",
        "source_doc": "huggingface/blog/blob/main/whisper-speculative-decoding.md"
    },
    {
        "context": "### Other Changes:\n\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](https://github.com/aliabid94) in [PR 4349](https://github.com/gradio-app/gradio/pull/4349).\n- Performance optimization in the frontend's Blocks code by [@akx](https://github.com/akx) in [PR 4334](https://github.com/gradio-app/gradio/pull/4334)\n- Upgrade the pnpm lock file format version from v6.0 to v6.1 by [@whitphx](https://github.com/whitphx) in [PR 4393](https://github.com/gradio-app/gradio/pull/4393)\n\n### Breaking Changes:\n\n- The `/file=` route no longer allows accessing dotfiles or files in \"dot directories\" by [@akx](https://github.com/akx) in [PR 4303](https://github.com/gradio-app/gradio/pull/4303)\n\n## 3.32.0\n\n### New Features:\n\n- `Interface.launch()` and `Blocks.launch()` now accept an `app_kwargs` argument to allow customizing the configuration of the underlying FastAPI app, by [@akx](https://github.com/akx) in [PR 4282](https://github.com/gradio-app/gradio/pull/4282)\n\n### Bug Fixes:",
        "question": "What is the new feature added in version 3.32.0 of gradio?\n",
        "answer": "The new feature added in version 3.32.0 of gradio is that `Interface.launch()` and `Blocks.launch()` now accept an `app_kwargs` argument to allow customizing the configuration of the underlying FastAPI app.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Super-resolution\n\nThe Stable Diffusion upscaler diffusion model was created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), and [LAION](https://laion.ai/). It is used to enhance the resolution of input images by a factor of 4.\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\nIf you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionUpscalePipeline\n\n[[autodoc]] StableDiffusionUpscalePipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput",
        "question": "Who created the Stable Diffusion upscaler diffusion model?\n",
        "answer": "The Stable Diffusion upscaler diffusion model was created by the researchers and engineers from CompVis, Stability AI, and LAION.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/upscale.md"
    },
    {
        "context": "--\ntitle: Deprecation of Git Authentication using password\nthumbnail: /blog/assets/password-git-deprecation/thumbnail.png\nauthors:\n- user: Sylvestre\n- user: pierric\n- user: sbrandeis\n\n---\n\n# Hugging Face Hub: Important Git Authentication Changes\n\n\nBecause we are committed to improving the security of our services, we are making changes to the way you authenticate when interacting with the Hugging Face Hub through Git.\nStarting from **October 1st, 2023**, we will no longer accept passwords as a way to authenticate your command-line Git operations. Instead, we recommend using more secure authentication methods, such as replacing the password with a personal access token or using an SSH key.\n\n## Background\n\nIn recent months, we have implemented various security enhancements, including sign-in alerts and support for SSH keys in Git. However, users have still been able to authenticate Git operations using their username and password. To further improve security, we are now transitioning to token-based or SSH key authentication.\nToken-based and SSH key authentication offer several advantages over traditional password authentication, including unique, revocable, and random features that enhance security and control.\n## Action Required Today\n\nIf you currently use your HF account password to authenticate with Git, please switch to using a personal access token or SSH keys before **October 1st, 2023**.\n\n### Switching to personal access token\nYou will need to generate an access token for your account; you can follow https://huggingface.co/docs/hub/security-tokens#user-access-tokens to generate one.\n\nAfter generating your access token, you can update your Git repository using the following commands:",
        "question": "When will password authentication be deprecated for Git operations on the Hugging Face Hub?\n",
        "answer": "Password authentication will be deprecated for Git operations on the Hugging Face Hub starting from October 1st, 2023.",
        "source_doc": "huggingface/blog/blob/main/password-git-deprecation.md"
    },
    {
        "context": "1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (来自 the University of Wisconsin - Madison) 伴随论文 [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) 由 Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh 发布。\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (来自 SHI Labs)  伴随论文 [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) 由 Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi 发布。\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (来自 [s-JoL](https://huggingface.co/s-JoL)) 由 GitHub (现已删除).\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (来自 Meta AI) 伴随论文 [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) 由 Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al 发布。\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (来自 Google AI) 伴随论文 [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) 由 Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby 发布。\n1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (来自 Google AI) 伴随论文 [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) 由 Matthias Minderer, Alexey Gritsenko, Neil Houlsby 发布。",
        "question": "Which model is developed by Meta AI?\n",
        "answer": "OPT",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hans.md"
    },
    {
        "context": "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large applications.\n\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\n- Corrected an issue that was causing markdown to re-render infinitely.\n- Ensured that the `gr.3DModel` does re-render prematurely.\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 0.2.0\n\n### Features\n\n- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 0.1.0\n\n### Features\n\n- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app/gradio/commit/3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf) - Implement left and right click in `Gallery` component and show implicit images in `Gallery` grid. Thanks [@hannahblair](https://github.com/hannahblair)!",
        "question": "What is the performance improvement for large applications in the recent version of Gradio?\n",
        "answer": "The performance improvement for large applications in the recent version of Gradio is around twice as fast due to delegated events instead of manual attachment.",
        "source_doc": "gradio-app/gradio/blob/main/js/gallery/CHANGELOG.md"
    },
    {
        "context": "```python\nimport matplotlib.pyplot as plt\n\ncolor_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\npalette = np.array(ade_palette())\n\nfor label, color in enumerate(palette):\n    color_seg[pred_seg == label, :] = color\ncolor_seg = color_seg[..., ::-1]  # convert to BGR\n\nimg = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()\n```\n\nAs you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of \nfine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art \nresults. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same \nmodel variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total \ntrainable parameters and in less time.\n\nIf you wish to use this example and improve the results, here are some things that you can try:\n\n* Increase the number of training samples.\n* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).\n* Try different values for the arguments available in `LoraConfig`.\n* Tune the learning rate and batch size.",
        "question": "What is the purpose of the example?\n",
        "answer": "The purpose of the example is to illustrate the end-to-end workflow of fine-tuning a semantic segmentation model with LoRa technique.",
        "source_doc": "huggingface/peft/blob/main/docs/source/task_guides/semantic_segmentation_lora.md"
    },
    {
        "context": "Drawing samples from a probability distribution for the next token will cause our greedy assistant to fail more often, reducing its latency benefits. However, we can control how sharp the probability distribution for the next tokens is, using the temperature coefficient that’s present in most sampling-based applications. At one extreme, with temperatures close to 0, sampling will approximate greedy decoding, favoring the most likely token. At the other extreme, with the temperature set to values much larger than 1, sampling will be chaotic, drawing from a uniform distribution. Low temperatures are, therefore, more favorable to your assistant model, retaining most of the latency benefits from assisted generation, as we can see below.\n\n\n<!-- [TEMPERATURE RESULTS, SHOW THAT LATENCY INCREASES STEADILY WITH TEMP] -->\n<div align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/temperature.png\"/>\n</div>\n\n\nWhy don't you see it for yourself, so get a feeling of assisted generation?\n\n\n<!-- [DEMO] -->\n<gradio-app theme_mode=\"light\" space=\"joaogante/assisted_generation_demo\"></gradio-app>\n\n\n## Future directions\n\nAssisted generation shows that modern text generation strategies are ripe for optimization. Understanding that it is currently a memory-bound problem, not a compute-bound problem, allows us to apply simple heuristics to get the most out of the available memory bandwidth, alleviating the bottleneck. We believe that further refinement of the use of assistant models will get us even bigger latency reductions - for instance, we may be able to skip a few more forward passes if we request the assistant to generate several candidate continuations. Naturally, releasing high-quality small models to be used as assistants will be critical to realizing and amplifying the benefits.",
        "question": "What happens to the sampling when the temperature is set to values much larger than 1?\n",
        "answer": "When the temperature is set to values much larger than 1, sampling will be chaotic, drawing from a uniform distribution.",
        "source_doc": "huggingface/blog/blob/main/assisted-generation.md"
    },
    {
        "context": "It is worth spending some time to create fitting model cards for each\ncheckpoint. The model cards should highlight the specific\ncharacteristics of this particular checkpoint, *e.g.*, On which dataset\nwas the checkpoint pretrained/fine-tuned on? On what down-stream task\nshould the model be used? And also include some code on how to correctly\nuse the model.\n\n**13. (Optional) Add notebook**\n\nIt is very helpful to add a notebook that showcases in-detail how\n*[camelcase name of model]* can be used for inference and/or fine-tuned on a\ndownstream task. This is not mandatory to merge your PR, but very useful\nfor the community.\n\n**14. Submit your finished PR**\n\nYou're done programming now and can move to the last step, which is\ngetting your PR merged into main. Usually, [name of mentor]\nshould have helped you already at this point, but it is worth taking\nsome time to give your finished PR a nice description and eventually add\ncomments to your code, if you want to point out certain design choices\nto your reviewer.\n\n### Share your work!!\n\nNow, it's time to get some credit from the community for your work!\nHaving completed a model addition is a major contribution to\nTransformers and the whole NLP community. Your code and the ported\npre-trained models will certainly be used by hundreds and possibly even\nthousands of developers and researchers. You should be proud of your\nwork and share your achievement with the community.\n\n**You have made another model that is super easy to access for everyone\nin the community! 🤯**",
        "question": "What is the purpose of creating model cards for each checkpoint?\n",
        "answer": "The purpose of creating model cards for each checkpoint is to highlight the specific characteristics of that particular checkpoint, such as the dataset it was pretrained/fine-tuned on and the downstream task it should be used for. It should also include some code on how to correctly use the model.",
        "source_doc": "huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md"
    },
    {
        "context": "image = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1216))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    unet=unet,\n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\", \n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=5,\n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\ngrid = make_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_full_sdxl_t2iadapter.png)",
        "question": "What is the name of the image processing technique used in the context?\n",
        "answer": "Canny edge detection",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md"
    },
    {
        "context": "def collate_fn(examples):\n    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n\n\n# Instantiate dataloaders.\ntrain_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n)\n```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\nmodel\n```\n\n\n```python\noptimizer = AdamW(params=model.parameters(), lr=lr)\n\n# Instantiate scheduler\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```\n\n\n```python\nmodel.to(device)\nfor epoch in range(num_epochs):\n    model.train()\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch.to(device)\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch.to(device)\n        with torch.no_grad():\n            outputs = model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        predictions, references = predictions, batch[\"labels\"]\n        metric.add_batch(\n            predictions=predictions,\n            references=references,\n        )\n\n    eval_metric = metric.compute()\n    print(f\"epoch {epoch}:\", eval_metric)\n```\n\n## Share adapters on the 🤗 Hub\n\n\n```python\nmodel.push_to_hub(\"smangrul/roberta-large-peft-lora\", use_auth_token=True)\n```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the commands below:",
        "question": "What is the name of the model that was pushed to the 🤗 Hub?\n",
        "answer": "smangrul/roberta-large-peft-lora",
        "source_doc": "huggingface/peft/blob/main/examples/sequence_classification/LoRA.ipynb"
    },
    {
        "context": "Send Requests to Endpoints\n\nYou can send requests to Inference Endpoints using the UI leveraging the Inference Widget or programmatically, e.g. with cURL, `@huggingface/inference`, `huggingface_hub` or any REST client. The Endpoint overview not only provides a interactive widget for you to test the Endpoint, but also generates code for `python`, `javascript` and `curl`. You can use this code to quickly get started with your Endpoint in your favorite programming language.\n\nBelow are also examples on how to use the `@huggingface/inference` library to call an inference endpoint.\n\n## Use the UI to send requests\n\nThe Endpoint overview provides access to the Inference Widget which can be used to send requests (see step 6 of [Create an Endpoint](/docs/inference-endpoints/guides/create_endpoint)). This allows you to quickly test your Endpoint with different inputs and share it with team members.\n\n## Use cURL to send requests\n\nThe cURL command for the request above should look like this. You'll need to provide your user token which can be found in your Hugging Face [account settings](https://huggingface.co/settings/tokens):\n\nExample Request:\n\n```bash\ncurl https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/distilbert-sentiment \\\n\t-X POST \\\n\t-d '{\"inputs\": \"Deploying my first endpoint was an amazing experience.\"}' \\\n\t-H \"Authorization: Bearer <Token>\"\n```\n\nThe Endpoints API offers the same API definitions as the [Inference API](https://huggingface.co/docs/api-inference/detailed_parameters) and the [SageMaker Inference Toolkit](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api). All the request payloads are documented in the [Supported Tasks](/docs/inference-endpoints/supported_tasks) section.",
        "question": "How can I use cURL to send requests to an Inference Endpoint?\n",
        "answer": "To send requests to an Inference Endpoint using cURL, you can use the following command:\n\n```bash\ncurl <ENDPOINT_URL> \\\n\t-X POST \\\n\t-d '{\"inputs\": \"<INPUT_DATA>\"}' \\\n\t-H \"Authorization: Bearer <TOKEN>\"\n```\n\nReplace `<ENDPOINT_URL>` with the URL of your Inference Endpoint, `<INPUT_DATA>` with the data you want to send, and `<TOKEN>` with your user token from your Hugging Face account settings.",
        "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/test_endpoint.mdx"
    },
    {
        "context": "metric = load_metric(\"squad_v2\")\ndataset = load_dataset(\"squad_v2\")[\"validation\"]\n\nprint(f\"length of dataset {len(dataset)}\")\n#length of dataset 11873\n```\n\nWe can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)\n\n*This can take quite a while (1.5h)*\n\n```python\ndef evaluate(example):\n  default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n  optimized = opt_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n  quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n  return {\n      'reference': {'id': example['id'], 'answers': example['answers']},\n      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n      'optimized': {'id': example['id'],'prediction_text': optimized['answer'], 'no_answer_probability': 0.},\n      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n      }\n\nresult = dataset.map(evaluate)\n# COMMENT IN to run evaluation on 2000 subset of the dataset\n# result = dataset.shuffle().select(range(2000)).map(evaluate)\n```\n\nNow lets compare the results\n\n```python\ndefault_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\noptimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\nquantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])",
        "question": "What is the length of the result dataset after mapping the evaluate function?\n",
        "answer": "The length of the result dataset after mapping the evaluate function is the same as the length of the original dataset, which is 11873.\n```",
        "source_doc": "huggingface/blog/blob/main/optimum-inference.md"
    },
    {
        "context": "If you answer yes, the new model will have files for all the frameworks implemented by the model you're cloning.\nOtherwise, you will get a new question to select the frameworks you want.\n\nOnce the command has finished, you will see a new subfolder in the `src/transformers/models/` folder, with the\nnecessary files (configuration and modeling files for all frameworks requested, and maybe the processing files,\ndepending on your choices).\n\nYou will also see a doc file and tests for your new models. First you should run\n\n```\nmake style\nmake fix-copies\n```\n\nand then you can start tweaking your model. You should:\n- fill the doc file at `docs/source/model_doc/model_name.md`\n- tweak the configuration and modeling files to your need\n\nOnce you're done, you can run the tests to ensure that they all pass:\n\n```\npython -m pytest ./tests/test_*<model_name>*.py\n```\n\n⚠ You should be careful about the classes preceded by the following line:️ \n\n```python\n# Copied from transformers.[...]\n```\n\nThis line ensures that the copy does not diverge from the source. If it *should* diverge, because the implementation\nis different, this line needs to be deleted. If you don't delete this line and run `make fix-copies`,\nyour changes will be overwritten.\n\nOnce you have edited the files to fit your architecture, simply re-run the tests (and edit them if a change \nis needed!) afterwards to make sure everything works as expected. \n\nOnce the files are generated and you are happy with your changes, here's a checklist to ensure that your contribution\nwill be merged quickly:\n\n- You should run the `make fixup` utility to fix the style of the files and to ensure the code quality meets the\n  library's standards.\n- You should add your model to the main README then run `make fix-copies`.",
        "question": "What should I do if I want the new model to have files for all the frameworks implemented by the model I'm cloning?\n",
        "answer": "If you answer yes to a question during the command, the new model will have files for all the frameworks implemented by the model you're cloning.",
        "source_doc": "huggingface/transformers/blob/main/templates/adding_a_new_model/README.md"
    },
    {
        "context": "Training a masked language model end-to-end from scratch on TPUs\n\nIn this example, we're going to demonstrate how to train a TensorFlow model from 🤗 Transformers from scratch. If you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check out our \n[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\nIf you're interested in smaller-scale TPU training from a pre-trained checkpoint, you can also check out the  [TPU fine-tuning example](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb).\n\nThis example will demonstrate pre-training language models at the 100M-1B parameter scale, similar to BERT or GPT-2. More concretely, we will show how to train a [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) (base model) from scratch on the [WikiText dataset (v1)](https://huggingface.co/datasets/wikitext).\n\nWe've tried to ensure that all the practices we show you here are scalable, though - with relatively few changes, the code could be scaled up to much larger models. \n\nGoogle's gargantuan [PaLM model](https://arxiv.org/abs/2204.02311), with\nover 500B parameters, is a good example of how far you can go with pure TPU training, though gathering the dataset and the budget to train at that scale is not an easy task!\n\n### Table of contents \n\n- [Setting up a TPU-VM](#setting-up-a-tpu-vm)\n- [Training a tokenizer](#training-a-tokenizer)\n- [Preparing the dataset](#preparing-the-dataset)\n- [Training the model](#training-the-model)\n- [Inference](#inference)\n\n## Setting up a TPU-VM\n\nSince this example focuses on using TPUs, the first step is to set up access to TPU hardware. For this example, we chose to use a TPU v3-8 VM. Follow [this guide](https://cloud.google.com/tpu/docs/run-calculation-tensorflow) to quickly create a TPU VM with TensorFlow pre-installed.",
        "question": "What is the name of the TPU used in this example?\n",
        "answer": "The TPU used in this example is a TPU v3-8 VM.",
        "source_doc": "huggingface/transformers/blob/main/examples/tensorflow/language-modeling-tpu/README.md"
    },
    {
        "context": "Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [`DistilBertConfig`] to inspect it's attributes:\n\n```py\n>>> from transformers import DistilBertConfig\n\n>>> config = DistilBertConfig()\n>>> print(config)\nDistilBertConfig {\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```\n\n[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel`]. All attributes are customizable, creating space for experimentation. For example, you can customize a default model to:\n\n- Try a different activation function with the `activation` parameter.\n- Use a higher dropout ratio for the attention probabilities with the `attention_dropout` parameter.\n\n```py\n>>> my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n>>> print(my_config)\nDistilBertConfig {\n  \"activation\": \"relu\",\n  \"attention_dropout\": 0.4,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```\n\nPretrained model attributes can be modified in the [`~PretrainedConfig.from_pretrained`] function:\n\n```py\n>>> my_config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n```\n\nOnce you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:",
        "question": "What is the default activation function in DistilBERT?\n",
        "answer": "The default activation function in DistilBERT is gelu.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/create_a_model.md"
    },
    {
        "context": "```python\nfrom huggingface_hub import DatasetCard, ModelCard\nfrom huggingface_hub.utils import EntryNotFoundError \n\ndef load_repo_card_metadata(repo_type, repo_name):\n    if repo_type == \"dataset\":\n        try:\n            return DatasetCard.load(repo_name).data.to_dict()\n        except EntryNotFoundError:\n            return {}\n    if repo_type == \"model\":\n        try:\n            return ModelCard.load(repo_name).data.to_dict()\n        except EntryNotFoundError:\n            return {}\n```\n\nThis function will return a Python dictionary containing the metadata associated with the repository (or an empty dictionary if there is no metadata).\n\n```python\n{'license': 'afl-3.0'}\n```\n\n## Creating our metadata review report\n\nOnce we have a Python dictionary containing the metadata associated with a repository, we'll create a 'report card' for our metadata review. In this particular instance, we'll review our metadata by defining some metadata fields for which we want values. For example, we may want to ensure that the `license` field has always been completed. To rate our metadata, we'll count which metadata fields are present out of our desired fields and return a percentage score based on the coverage of the required metadata fields we want to see values.\n\nSince we have a Python dictionary containing our metadata, we can loop through this dictionary to check if our desired keys are there. If a desired metadata field (a key in our dictionary) is missing, we'll assign the value as `None`.\n\n```python\ndef create_metadata_key_dict(card_data, repo_type: str):\n    shared_keys = [\"tags\", \"license\"]\n    if repo_type == \"model\":\n        model_keys = [\"library_name\", \"datasets\", \"metrics\", \"co2\", \"pipeline_tag\"]\n        shared_keys.extend(model_keys)\n        keys = shared_keys\n        return {key: card_data.get(key) for key in keys}\n    if repo_type == \"dataset\":\n        # [...]\n```",
        "question": "What are the keys in the dictionary returned by the `create_metadata_key_dict` function for a model repository?\n",
        "answer": "The keys in the dictionary returned by the `create_metadata_key_dict` function for a model repository are 'tags', 'license', 'library\\_name', 'datasets', 'metrics', 'co2', and 'pipeline\\_tag'.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md"
    },
    {
        "context": "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Performance and Scalability\n\nTraining large transformer models and deploying them to production present various challenges.  \nDuring training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment \nphase, the model can struggle to handle the required throughput in a production environment.\n\nThis documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. \nThe guides are divided into training and inference sections, as each comes with different challenges and solutions. \nWithin each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU \nfor training or CPU vs. GPU for inference.\n\nUse this document as your starting point to navigate further to the methods that match your scenario.\n\n## Training",
        "question": "What are the two challenges of training large transformer models?\n",
        "answer": "The two challenges of training large transformer models are that the model may require more GPU memory than available and that it can exhibit slow training speed.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/performance.md"
    },
    {
        "context": "Inference Examples\n\n**The inference examples folder is deprecated and will be removed in a future version**.\n**Officially supported inference examples can be found in the [Pipelines folder](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines)**.\n\n- For `Image-to-Image text-guided generation with Stable Diffusion`, please have a look at the official [Pipeline examples](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines#examples)\n- For `In-painting using Stable Diffusion`, please have a look at the official [Pipeline examples](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines#examples)\n- For `Tweak prompts reusing seeds and latents`, please have a look at the official [Pipeline examples](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines#examples)",
        "question": "Where can I find the official examples for Image-to-Image text-guided generation with Stable Diffusion?\n",
        "answer": "The official examples for Image-to-Image text-guided generation with Stable Diffusion can be found in the [Pipelines folder](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines).",
        "source_doc": "huggingface/diffusers/blob/main/examples/inference/README.md"
    },
    {
        "context": "1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n1. ¿Quieres aportar un nuevo modelo? Hemos agregado una **guía detallada y plantillas** para guiarte en el proceso de agregar un nuevo modelo. Puedes encontrarlos en la carpeta de [`templates`](./templates) del repositorio. Asegúrate de revisar las [pautas de contribución](./CONTRIBUTING.md) y comunícate con los mantenedores o abra un problema para recopilar comentarios antes de comenzar su PR.",
        "question": "What is the name of the model released by Facebook AI with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?\n",
        "answer": "XLS-R",
        "source_doc": "huggingface/transformers/blob/main/README_es.md"
    },
    {
        "context": "new_audio = acapellify(old_audio)\n\n    new_video = f\"acap_{video_path}\"\n    subprocess.call(['ffmpeg', '-y', '-i', video_path, '-i', new_audio, '-map', '0:v', '-map', '1:a', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental', f\"static/{new_video}\"])\n    return new_video\n```\n\nYou can read up on [ffmpeg documentation](https://ffmpeg.org/ffmpeg.html) if you'd like to understand all of the command line parameters, as they are beyond the scope of this tutorial.\n\n## Step 2: Create a FastAPI app (Backend Routes)\n\nNext up, we'll create a simple FastAPI app. If you haven't used FastAPI before, check out [the great FastAPI docs](https://fastapi.tiangolo.com/). Otherwise, this basic template, which we add to `main.py`, will look pretty familiar:\n\n```python\nimport os\nfrom fastapi import FastAPI, File, UploadFile, Request\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\nos.makedirs(\"static\", exist_ok=True)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\nvideos = []\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    return templates.TemplateResponse(\n        \"home.html\", {\"request\": request, \"videos\": videos})\n\n@app.post(\"/uploadvideo/\")\nasync def upload_video(video: UploadFile = File(...)):\n    new_video = process_video(video.filename)\n    videos.append(new_video)\n    return RedirectResponse(url='/', status_code=303)\n```\n\nIn this example, the FastAPI app has two routes: `/` and `/uploadvideo/`.\n\nThe `/` route returns an HTML template that displays a gallery of all uploaded videos.",
        "question": "What is the name of the function that processes the uploaded video file?\n",
        "answer": "process_video",
        "source_doc": "gradio-app/gradio/blob/main/guides/08_gradio-clients-and-lite/fastapi-app-with-the-gradio-client.md"
    },
    {
        "context": "```python\ncommon_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\ncommon_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n```\n\nLet\\'s take a look at `\"audio\"` again.\n\n```python\ncommon_voice_train[0][\"audio\"]\n```\n\n```bash\n    {'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n            -7.4556941e-05, -1.4621433e-05, -5.7861507e-05], dtype=float32),\n     'path': '/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3',\n     'sampling_rate': 16000}\n```\n\nThis seemed to have worked! Let\\'s listen to a couple of audio files to\nbetter understand the dataset and verify that the audio was correctly\nloaded.\n\n```python\nimport IPython.display as ipd\nimport numpy as np\nimport random\n\nrand_int = random.randint(0, len(common_voice_train)-1)\n\nprint(common_voice_train[rand_int][\"sentence\"])\nipd.Audio(data=common_voice_train[rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000)\n```\n\n**Print Output:**\n\n```bash\n    sunulan bütün teklifler i̇ngilizce idi\n```\nIt seems like the data is now correctly loaded and resampled.\n\nIt can be heard, that the speakers change along with their speaking\nrate, accent, and background environment, etc. Overall, the recordings\nsound acceptably clear though, which is to be expected from a\ncrowd-sourced read speech corpus.\n\nLet\\'s do a final check that the data is correctly prepared, by printing\nthe shape of the speech input, its transcription, and the corresponding\nsampling rate.\n\n```python\nrand_int = random.randint(0, len(common_voice_train)-1)\n\nprint(\"Target text:\", common_voice_train[rand_int][\"sentence\"])\nprint(\"Input array shape:\", common_voice_train[rand_int][\"audio\"][\"array\"].shape)\nprint(\"Sampling rate:\", common_voice_train[rand_int][\"audio\"][\"sampling_rate\"])\n```\n\n**Print Output:**",
        "question": "What is the sampling rate of the speech input?\n",
        "answer": "The sampling rate of the speech input is 16000.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-xlsr-wav2vec2.md"
    },
    {
        "context": "SWSL ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-block) that aggregates a set of transformations with the same topology. Compared to a [ResNet](https://paperswithcode.com/method/resnet), it exposes a new dimension,  *cardinality* (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width. \n\nThe models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. \n\nPlease note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.\n\n## How do I use this model on an image?\nTo load a pretrained model:\n\n```python\nimport timm\nmodel = timm.create_model('swsl_resnext101_32x16d', pretrained=True)\nmodel.eval()\n```\n\nTo load and preprocess the image:\n```python \nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n```python\nimport torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]",
        "question": "How do I get the top-5 predictions class names from the model?\n",
        "answer": "You can get the top-5 predictions class names from the model by first getting the imagenet class mappings and then using the `torch.nn.functional.softmax` function to get the probabilities of each class. The top-5 class names can then be obtained by sorting the probabilities in descending order and selecting the indices of the top-5 probabilities.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/swsl-resnext.md"
    },
    {
        "context": "And we can also have a look at the decoder input IDs, to see that they are shifted versions of the labels:\n\n```py\nbatch[\"decoder_input_ids\"]\n```\n\n```python out\ntensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n         59513, 59513, 59513, 59513, 59513, 59513],\n        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n           817,   550,  7032,  5821,  7907, 12649]])\n```\n\nHere are the labels for the first and second elements in our dataset:\n\n```py\nfor i in range(1, 3):\n    print(tokenized_datasets[\"train\"][i][\"labels\"])\n```\n\n```python out\n[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]\n```\n\n{#if fw === 'pt'}\n\nWe will pass this `data_collator` along to the `Seq2SeqTrainer`. Next, let's have a look at the metric.\n\n{:else}\n\nWe can now use this `data_collator` to convert each of our datasets to a `tf.data.Dataset`, ready for training:\n\n```python\ntf_train_dataset = model.prepare_tf_dataset(\n    tokenized_datasets[\"train\"],\n    collate_fn=data_collator,\n    shuffle=True,\n    batch_size=32,\n)\ntf_eval_dataset = model.prepare_tf_dataset(\n    tokenized_datasets[\"validation\"],\n    collate_fn=data_collator,\n    shuffle=False,\n    batch_size=16,\n)\n```\n\n{/if}\n\n\n### Metrics[[metrics]]\n\n<Youtube id=\"M05L1DhFqcw\"/>\n\n{#if fw === 'pt'}\n\nThe feature that `Seq2SeqTrainer` adds to its superclass `Trainer` is the ability to use the `generate()` method during evaluation or prediction. During training, the model will use the `decoder_input_ids` with an attention mask ensuring it does not use the tokens after the token it's trying to predict, to speed up training. During inference we won't be able to use those since we won't have labels, so it's a good idea to evaluate our model with the same setup.",
        "question": "How will the model be saved after training?\n",
        "answer": "The model will be saved after training by using the `save_model()` method.\n\n{/if}\n\n### Loading the model from a checkpoint[[load]]\n\n<Youtube id=\"M05L1DhFqcw\"/>\n\n{#if fw === 'pt'}\n\nWe",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/4.mdx"
    },
    {
        "context": "response = endpoint_service_client.delete_endpoint(name=endpoint)\n    print(\"running delete_endpoint operation:\", response.operation.name)\n    print(response.result())\n\n    response = model_service_client.delete_model(name=model_name)\n    print(\"running delete_model operation:\", response.operation.name)\n    print(response.result())\n\n\ncleanup(tf28_gpu_endpoint, tf28_gpu_model, tf28_gpu_deployed_model.deployed_model.id)\n```\n\n\n```python\n!gsutil rm -r $GCS_BUCKET\n```",
        "question": "What is the command used to remove a resource from Google Cloud Storage?\n",
        "answer": "The command used to remove a resource from Google Cloud Storage is `gsutil rm -r $GCS_BUCKET`.",
        "source_doc": "huggingface/blog/blob/main/notebooks/112_vertex_ai_vision.ipynb"
    },
    {
        "context": "#### 更多复杂性\n\n这里有一个应用程序可以让你感受一下`Blocks`的更多可能：\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tabs():\n        with gr.TabItem(\"Flip Text\"):\n            text_input = gr.Textbox()\n            text_output = gr.Textbox()\n            text_button = gr.Button(\"Flip\")\n        with gr.TabItem(\"Flip Image\"):\n            with gr.Row():\n                image_input = gr.Image()\n                image_output = gr.Image()\n            image_button = gr.Button(\"Flip\")\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n```\n\n![`blocks_flipper` demo](../../demo/blocks_flipper/screenshot.gif)\n\n还有很多事情可以做！我们将在[使用blocks构建](https://gradio.app/building_with_blocks)部分为您介绍如何创建像这样复杂的 `Blocks` 应用程序。\n\n恭喜你，你现在已经熟悉了Gradio的基础使用！🥳 去我们的[下一章](https://gradio.app/key_features) 了解Gradio的更多功能。\n\n## 开源栈\n\nGradio是由许多很棒的开源库构建的，请一并支持它们!\n\n[<img src=\"../huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"../python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"../fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"../encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"../svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"../vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"../pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"../tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n\n## 协议\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## 引用\n\n另外请参阅论文 _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_，如果您在工作中使用Gradio请引用它。",
        "question": "What is the function that flips the text input in the `blocks_flipper` demo?\n",
        "answer": "The function that flips the text input in the `blocks_flipper` demo is `flip_text`.",
        "source_doc": "gradio-app/gradio/blob/main/readme_files/zh-cn/README.md"
    },
    {
        "context": "<li><a href=\"https://github.com/huggingface/transformers/tree/v2.6.0/examples\">v2.6.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.5.1/examples\">v2.5.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.4.0/examples\">v2.4.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.3.0/examples\">v2.3.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.2.0/examples\">v2.2.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.1.0/examples\">v2.1.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.0.0/examples\">v2.0.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.2.0/examples\">v1.2.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.1.0/examples\">v1.1.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.0.0/examples\">v1.0.0</a></li>\n\t</ul>\n</details>",
        "question": "What is the URL for the examples in version 2.6.0 of transformers?\n",
        "answer": "https://github.com/huggingface/transformers/tree/v2.6.0/examples",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/run_scripts.md"
    },
    {
        "context": "Gradio Demo: image_classifier_2\n\n\n```\n!pip install -q gradio pillow torch torchvision\n```\n\n\n```\n# Downloading files from the demo repo\nimport os\nos.mkdir('files')\n!wget -q -O files/imagenet_labels.json https://github.com/gradio-app/gradio/raw/main/demo/image_classifier_2/files/imagenet_labels.json\n```\n\n\n```\nimport requests\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nimport gradio as gr\n\nmodel = torch.hub.load(\"pytorch/vision:v0.6.0\", \"resnet18\", pretrained=True).eval()\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef predict(inp):\n    inp = Image.fromarray(inp.astype(\"uint8\"), \"RGB\")\n    inp = transforms.ToTensor()(inp).unsqueeze(0)\n    with torch.no_grad():\n        prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\ninputs = gr.Image()\noutputs = gr.Label(num_top_classes=3)\n\ndemo = gr.Interface(fn=predict, inputs=inputs, outputs=outputs)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```",
        "question": "What is the name of the model used in the demo?\n",
        "answer": "The name of the model used in the demo is ResNet18.",
        "source_doc": "gradio-app/gradio/blob/main/demo/image_classifier_2/run.ipynb"
    },
    {
        "context": "## How to enable Hyperparameter search in example\n\nDefine the hyperparameter search space, different backends need different format.\n\nFor sigopt, see sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter), it's like following:\n```py\n>>> def sigopt_hp_space(trial):\n...     return [\n...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n...         {\n...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n...             \"name\": \"per_device_train_batch_size\",\n...             \"type\": \"categorical\",\n...         },\n...     ]\n```\n\nFor optuna, see optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py), it's like following:\n\n```py\n>>> def optuna_hp_space(trial):\n...     return {\n...         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n...         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n...     }\n```\n\nOptuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and define your own compute_objective to return multiple objective values. The Pareto Front (`List[BestRun]`) will be returned in hyperparameter_search, you should refer to the test case `TrainerHyperParameterMultiObjectOptunaIntegrationTest` in [test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py). It's like following\n\n```py\n>>> best_trials = trainer.hyperparameter_search(\n...     direction=[\"minimize\", \"maximize\"],\n...     backend=\"optuna\",\n...     hp_space=optuna_hp_space,\n...     n_trials=20,\n...     compute_objective=compute_objective,\n... )\n```\n\nFor raytune, see raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html), it's like following:",
        "question": "What is the format of hyperparameter search space for raytune?\n",
        "answer": "The format of hyperparameter search space for raytune is defined in the raytune documentation, which provides a variety of search spaces including discrete, choice, uniform, loguniform, and quantized. For example, a simple search space for learning rate and batch size can be defined as follows:\n```py\nfrom ray import tune\n\nspace = {\n    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n    \"batch_size\": tune.choice([16, 32, 64]),\n}\n```",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/hpo_train.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a\nlanguage model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms\ndiscriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\nthe state of the art in 9 out of the 12 tasks studied.*\n\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\nshowcasing the generative capabilities of several models. GPT is one of them.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\n\n## Usage tips\n\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\n  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\n  observed in the *run_generation.py* example script.\n\n\nNote:",
        "question": "What is the objective of the model GPT?\n",
        "answer": "The objective of the model GPT is causal language modeling (CLM).",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md"
    },
    {
        "context": "---\n\n## Adding a new element to the navigation bar\n\nAccepted files are Markdown (.md or .mdx).\n\nCreate a file with its extension and put it in the source directory. You can then link it to the toc-tree by putting\nthe filename without the extension in the [`_toctree.yml`](https://github.com/huggingface/transformers/blob/master/docs/source/_toctree.yml) file.\n\n## Renaming section headers and moving sections\n\nIt helps to keep the old links working when renaming section header and/or moving sections from one document to another. This is because the old links are likely to be used in Issues, Forums and Social media and it'd be make for a much more superior user experience if users reading those months later could still easily navigate to the originally intended information.\n\nTherefore we simply keep a little map of moved sections at the end of the document where the original section was. The key is to preserve the original anchor.\n\nSo if you renamed a section from: \"Section A\" to \"Section B\", then you can add at the end of the file:\n\n```\nSections that were moved:\n\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\nand of course if you moved it to another file, then:\n\n```\nSections that were moved:\n\n[ <a href=\"../new-file#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFor an example of a rich moved sections set please see the very end of [the Trainer doc](https://github.com/huggingface/transformers/blob/master/docs/source/main_classes/trainer.mdx).\n\n\n## Writing Documentation - Specification\n\nThe `huggingface/transformers` documentation follows the\n[Google documentation](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) style for docstrings,\nalthough we can write them directly in Markdown.\n\n### Adding a new tutorial\n\nAdding a new tutorial or section is done in two steps:",
        "question": "How are new tutorials added to the huggingface/transformers documentation?\n",
        "answer": "New tutorials are added in two steps: first, create a file with the tutorial in the source directory using Markdown (.md or .mdx) format; second, link the tutorial to the toc-tree by putting the filename without the extension in the [`_toctree.yml`](https://github.com/huggingface/transformers/blob/master/docs/source/_toctree.yml) file.",
        "source_doc": "huggingface/simulate/blob/main/docs/README.md"
    },
    {
        "context": "### Fixes\n\n- [#5285](https://github.com/gradio-app/gradio/pull/5285) [`cdfd4217`](https://github.com/gradio-app/gradio/commit/cdfd42174a9c777eaee9c1209bf8e90d8c7791f2) - Tweaks to `icon` parameter in `gr.Button()`. Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5312](https://github.com/gradio-app/gradio/pull/5312) [`f769cb67`](https://github.com/gradio-app/gradio/commit/f769cb67149d8e209091508f06d87014acaed965) - only start listening for events after the components are mounted. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5276](https://github.com/gradio-app/gradio/pull/5276) [`502f1015`](https://github.com/gradio-app/gradio/commit/502f1015bf23b365bc32446dd2e549b0c5d0dc72) - Ensure `Blocks` translation copy renders correctly. Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 1.2.0\n\n### Highlights\n\n#### Client.predict will now return the final output for streaming endpoints ([#5057](https://github.com/gradio-app/gradio/pull/5057) [`35856f8b`](https://github.com/gradio-app/gradio/commit/35856f8b54548cae7bd3b8d6a4de69e1748283b2))\n\n### This is a breaking change (for gradio_client only)!\n\nPreviously, `Client.predict` would only return the first output of an endpoint that streamed results. This was causing confusion for developers that wanted to call these streaming demos via the client.\n\nWe realize that developers using the client don't know the internals of whether a demo streams or not, so we're changing the behavior of predict to match developer expectations.\n\nUsing `Client.predict` will now return the final output of a streaming endpoint. This will make it even easier to use gradio apps via the client.\n\nThanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n### Features",
        "question": "Which user contributed to the change in the behavior of Client.predict?\n",
        "answer": "Freddy Aboulton",
        "source_doc": "gradio-app/gradio/blob/main/js/app/CHANGELOG.md"
    },
    {
        "context": "If you want to include only tests that include both patterns, `and` is to be used:\n\n```bash\npytest -k \"test and ada\" tests/test_optimization.py\n```\n\n### Run `accelerate` tests\n\nSometimes you need to run `accelerate` tests on your models. For that you can just add `-m accelerate_tests` to your command, if let's say you want to run these tests on `OPT` run:\n\n```bash\nRUN_SLOW=1 pytest -m accelerate_tests tests/models/opt/test_modeling_opt.py \n```\n\n\n### Run documentation tests \n\nIn order to test whether the documentation examples are correct, you should check that the `doctests` are passing. \nAs an example, let's use [`WhisperModel.forward`'s docstring](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py#L1017-L1035): \n\n```python \nr\"\"\"\nReturns:\n\nExample:\n    ```python\n    >>> import torch\n    >>> from transformers import WhisperModel, WhisperFeatureExtractor\n    >>> from datasets import load_dataset\n\n    >>> model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n    >>> feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n    >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n    >>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n    >>> input_features = inputs.input_features\n    >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n    >>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n    >>> list(last_hidden_state.shape)\n    [1, 2, 512]\n    ```\"\"\"\n\n```\n\nJust run the following line to automatically test every docstring example in the desired file: \n```bash \npytest --doctest-modules <path_to_file_or_dir>\n```\nIf the file has a markdown extention, you should add the `--doctest-glob=\"*.md\"` argument.\n\n### Run only modified tests",
        "question": "How to run only modified tests in a file?\n",
        "answer": "You can run only modified tests in a file by using the `--new-first` option with `pytest` command. For example, to run only modified tests in `tests/models/opt/test_modeling_opt.py`, you can use the following command: `pytest --new-first tests/models/opt/test_modeling_opt.py`",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/testing.md"
    },
    {
        "context": "When evaluating on a regression task (STS-B), you may add `--regression_threshold 0.1` to define the regression threshold.\n\n## Results\nOn the GLUE dev set:\n\n| Model        | \\#Param | Speed  | CoLA  | MNLI  | MRPC  | QNLI  | QQP   | RTE   | SST\\-2 | STS\\-B |\n|--------------|---------|--------|-------|-------|-------|-------|-------|-------|--------|--------|\n| ALBERT\\-base | 12M     |        | 58\\.9 | 84\\.6 | 89\\.5 | 91\\.7 | 89\\.6 | 78\\.6 | 92\\.8  | 89\\.5  |\n| \\+PABEE      | 12M     | 1\\.57x | 61\\.2 | 85\\.1 | 90\\.0 | 91\\.8 | 89\\.6 | 80\\.1 | 93\\.0  | 90\\.1  |\n\n| Model         | \\#Param | Speed\\-up | MNLI  | SST\\-2 | STS\\-B |\n|---------------|---------|-----------|-------|--------|--------|\n| BERT\\-base    | 108M    |           | 84\\.5 | 92\\.1  | 88\\.9  |\n| \\+PABEE       | 108M    | 1\\.62x    | 83\\.6 | 92\\.0  | 88\\.7  |\n| ALBERT\\-large | 18M     |           | 86\\.4 | 94\\.9  | 90\\.4  |\n| \\+PABEE       | 18M     | 2\\.42x    | 86\\.8 | 95\\.2  | 90\\.6  |\n\n\n## Citation\nIf you find this resource useful, please consider citing the following paper:\n```bibtex\n@misc{zhou2020bert,\n    title={BERT Loses Patience: Fast and Robust Inference with Early Exit},\n    author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},\n    year={2020},\n    eprint={2006.04152},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```",
        "question": "How to define the regression threshold in STS-B task?\n",
        "answer": "You may add `--regression_threshold 0.1` to define the regression threshold in STS-B task.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/bert-loses-patience/README.md"
    },
    {
        "context": "Gradio Demo: gpt2_xl\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ntitle = \"gpt2-xl\"\n\nexamples = [\n    [\"The tower is 324 metres (1,063 ft) tall,\"],\n    [\"The Moon's orbit around Earth has\"],\n    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n]\n\ndemo = gr.load(\n    \"huggingface/gpt2-xl\",\n    inputs=gr.Textbox(lines=5, max_lines=6, label=\"Input Text\"),\n    title=title,\n    examples=examples,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```",
        "question": "What does the smooth Borealis basin in the Northern Hemisphere cover according to the context?\n",
        "answer": "The smooth Borealis basin in the Northern Hemisphere covers 40%.",
        "source_doc": "gradio-app/gradio/blob/main/demo/gpt2_xl/run.ipynb"
    },
    {
        "context": "These steps can be seen in the following block of code:\n\n```python\nfrom tqdm.auto import tqdm\nimport torch\nimport numpy as np\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n    # Training\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation\n    model.eval()\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n            )\n\n            generated_tokens = accelerator.pad_across_processes(\n                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n            )\n            labels = batch[\"labels\"]\n\n            # If we did not pad to max length, we need to pad the labels too\n            labels = accelerator.pad_across_processes(\n                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n            )\n\n            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n            labels = accelerator.gather(labels).cpu().numpy()\n\n            # Replace -100 in the labels as we can't decode them\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n            if isinstance(generated_tokens, tuple):\n                generated_tokens = generated_tokens[0]\n            decoded_preds = tokenizer.batch_decode(\n                generated_tokens, skip_special_tokens=True\n            )\n            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n            decoded_preds, decoded_labels = postprocess_text(\n                decoded_preds, decoded_labels\n            )",
        "question": "How is the progress bar updated in the code?\n",
        "answer": "The progress bar is updated by 1 after each training step in the for loop.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/5.mdx"
    },
    {
        "context": "### Min-SNR weighting\n\nThe [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting either `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types. This weighting strategy is only supported by PyTorch and is unavailable in the Flax training script.\n\nAdd the `--snr_gamma` parameter and set it to the recommended value of 5.0:\n\n```bash\naccelerate launch train_text_to_image_sdxl.py \\\n  --snr_gamma=5.0\n```\n\n## Training script\n\nThe training script is also similar to the [Text-to-image](text2image#training-script) training guide, but it's been modified to support SDXL training. This guide will focus on the code that is unique to the SDXL training script.\n\nIt starts by creating functions to [tokenize the prompts](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L478) to calculate the prompt embeddings, and to compute the image embeddings with the [VAE](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L519). Next, you'll a function to [generate the timesteps weights](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L531) depending on the number of timesteps and the timestep bias strategy to apply.\n\nWithin the [`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109e2c998c9/examples/text_to_image/train_text_to_image_sdxl.py#L572) function, in addition to loading a tokenizer, the script loads a second tokenizer and text encoder because the SDXL architecture uses two of each:",
        "question": "What is the name of the weighting strategy used in the training script?\n",
        "answer": "The name of the weighting strategy used in the training script is Min-SNR.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable \"zero-shot\" generalization ability for various image tasks. However, how to effectively expand such new language-image pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of pretraining a new model from scratch. More concretely, to capture the long-range dependencies of frames along the temporal dimension, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-of-the-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited.*\n\nTips:\n\n- Usage of X-CLIP is identical to [CLIP](clip).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/xclip_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> X-CLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2208.02816\">original paper.</a> </small>",
        "question": "What is the top-1 accuracy of the approach on Kinectics-400 under fully-supervised settings?\n",
        "answer": "The top-1 accuracy of the approach on Kinectics-400 under fully-supervised settings is 87.1%.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/xclip.md"
    },
    {
        "context": "- Issue #768: Support passing none to resize and crop image by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 1144](https://github.com/gradio-app/gradio/pull/1144)\n- image gallery component + img css by [@aliabid94](https://github.com/aliabid94) in [PR 1140](https://github.com/gradio-app/gradio/pull/1140)\n- networking tweak by [@abidlabs](https://github.com/abidlabs) in [PR 1143](https://github.com/gradio-app/gradio/pull/1143)\n- Allow enabling queue per event listener by [@aliabid94](https://github.com/aliabid94) in [PR 1155](https://github.com/gradio-app/gradio/pull/1155)\n- config hotfix and v. 2.9b23 by [@abidlabs](https://github.com/abidlabs) in [PR 1158](https://github.com/gradio-app/gradio/pull/1158)\n- Custom JS calls by [@aliabid94](https://github.com/aliabid94) in [PR 1082](https://github.com/gradio-app/gradio/pull/1082)\n- Small fixes: queue default fix, ffmpeg installation message by [@abidlabs](https://github.com/abidlabs) in [PR 1159](https://github.com/gradio-app/gradio/pull/1159)\n- formatting by [@abidlabs](https://github.com/abidlabs) in [PR 1161](https://github.com/gradio-app/gradio/pull/1161)\n- enable flex grow for gr-box by [@radames](https://github.com/radames) in [PR 1165](https://github.com/gradio-app/gradio/pull/1165)\n- 1148 loading by [@pngwn](https://github.com/pngwn) in [PR 1164](https://github.com/gradio-app/gradio/pull/1164)\n- Put enable_queue kwarg back in launch() by [@aliabid94](https://github.com/aliabid94) in [PR 1167](https://github.com/gradio-app/gradio/pull/1167)\n- A few small fixes by [@abidlabs](https://github.com/abidlabs) in [PR 1171](https://github.com/gradio-app/gradio/pull/1171)\n- Hotfix for dropdown component by [@abidlabs](https://github.com/abidlabs) in [PR 1172](https://github.com/gradio-app/gradio/pull/1172)\n- use secondary buttons in interface by [@pngwn](https://github.com/pngwn) in [PR 1173](https://github.com/gradio-app/gradio/pull/1173)",
        "question": "Which user added the formatting in PR 1161?\n",
        "answer": "The formatting was added by [@abidlabs](https://github.com/abidlabs) in PR 1161.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "@gradio/app\n\n## 1.17.0\n\n### Features\n\n- [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown.  Thanks [@pngwn](https://github.com/pngwn)!\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144) - Improve source selection UX.  Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 1.16.2\n\n### Patch Changes\n\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4), [`c352811`](https://github.com/gradio-app/gradio/commit/c352811f76d4126613ece0a584f8c552fdd8d1f6)]:\n  - @gradio/client@0.9.2\n  - @gradio/audio@0.6.2\n  - @gradio/imageeditor@0.1.5\n  - @gradio/annotatedimage@0.3.12\n  - @gradio/button@0.2.12\n  - @gradio/chatbot@0.5.4\n  - @gradio/dataset@0.1.12\n  - @gradio/file@0.4.2\n  - @gradio/fileexplorer@0.3.12\n  - @gradio/gallery@0.4.13\n  - @gradio/image@0.5.2\n  - @gradio/model3d@0.4.10\n  - @gradio/upload@0.5.5\n  - @gradio/uploadbutton@0.3.3\n  - @gradio/video@0.2.2\n  - @gradio/dataframe@0.4.2\n  - @gradio/code@0.3.2\n\n## 1.16.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\n  - @gradio/model3d@0.4.9\n  - @gradio/upload@0.5.4\n  - @gradio/client@0.9.1\n  - @gradio/annotatedimage@0.3.11\n  - @gradio/audio@0.6.1\n  - @gradio/button@0.2.11\n  - @gradio/chatbot@0.5.3\n  - @gradio/code@0.3.1\n  - @gradio/dataframe@0.4.1\n  - @gradio/dataset@0.1.11\n  - @gradio/file@0.4.1\n  - @gradio/fileexplorer@0.3.11\n  - @gradio/gallery@0.4.12\n  - @gradio/image@0.5.1\n  - @gradio/imageeditor@0.1.4\n  - @gradio/uploadbutton@0.3.2\n  - @gradio/video@0.2.1\n\n## 1.16.0\n\n### Features",
        "question": "What is the version number of gradio/video in gradio/app 1.16.1?\n",
        "answer": "0.2.1",
        "source_doc": "gradio-app/gradio/blob/main/js/app/CHANGELOG.md"
    },
    {
        "context": "Libraries\n\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `huggingface_hub` Python library, it's easy to enable sharing your models on the Hub. The Hub supports many libraries, and we're working on expanding this support. We're happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.\n\nThe table below summarizes the supported libraries and their level of integration. Find all our supported libraries in [the model-libraries.ts file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts).",
        "question": "Which Python library makes it easy to share models on the Hugging Face Hub?\n",
        "answer": "The `huggingface_hub` Python library",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/models-libraries.md"
    },
    {
        "context": "For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.",
        "question": "What is the name of the directory where flagged data is stored in the sepia interface?\n",
        "answer": "flagged",
        "source_doc": "gradio-app/gradio/blob/main/guides/02_building-interfaces/00_the-interface-class.md"
    },
    {
        "context": "1. **Model standardization**: the [Transformer](https://arxiv.org/abs/1706.03762) architecture is now the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, Speech, and more. It’s now easier to build tools and workflows that perform well across many use cases.\n2. **Pre-trained models**: [hundreds of thousands](https://huggingface.co/models) of pre-trained models are just a click away. You can discover and test them directly on [Hugging Face](https://huggingface.co) and quickly shortlist the promising ones for your projects.\n3. **Open-source libraries**: the Hugging Face [libraries](https://huggingface.co/docs) let you download pre-trained models with a single line of code, and you can start experimenting with your data in minutes. From training to deployment to hardware optimization, customers can rely on a consistent set of community-driven tools that work the same everywhere, from their laptops to their production environment.\n\nIn addition, our cloud partnerships let customers use Hugging Face models and libraries at any scale without worrying about provisioning infrastructure and building technical environments. This makes it much easier to get high-quality models out the door at a rapid pace without having to reinvent the wheel.\n\nFollowing up on our collaboration with AWS on Amazon SageMaker and Microsoft on Azure Machine Learning, we're thrilled to work with none other than IBM on their new AI studio, [watsonx.ai](https://www.ibm.com/products/watsonx-ai). [watsonx.ai](http://watsonx.ai) is the next-generation enterprise studio for AI builders to train, validate, tune, and deploy both traditional ML and new generative AI capabilities, powered by foundation models.",
        "question": "How many pre-trained models are available on Hugging Face?\n",
        "answer": "Hundreds of thousands of pre-trained models are available on Hugging Face.",
        "source_doc": "huggingface/blog/blob/main/huggingface-and-ibm.md"
    },
    {
        "context": "```py\nmodel.add_weighted_adapter(\n    adapters=[\"adapter_1\", \"adapter_2\"],\n    weights=[0.7, 0.3],\n    adapter_name=\"new-weighted-adapter\"\n)\n```\n\n## Load adapters\n\nAdapters can be loaded onto a pretrained model with [`~PeftModel.load_adapter`], which is useful for trying out different adapters whose weights aren't merged. Set the active adapter weights with the [`~LoraModel.set_adapter`] function.\n\n```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\n\n# load different adapter\nmodel.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\n\n# set adapter as active\nmodel.set_adapter(\"dpo\")\n```\n\nTo return the base model, you could use [`~LoraModel.unload`] to unload all of the LoRA modules or [`~LoraModel.delete_adapter`] to delete the adapter entirely.\n\n```py\n# unload adapter\nmodel.unload()\n\n# delete adapter\nmodel.delete_adapter(\"dpo\")\n```",
        "question": "How to load multiple adapters onto a pretrained model?\n",
        "answer": "You can load multiple adapters onto a pretrained model using the `add_weighted_adapter` method, which takes in a list of adapter names and their corresponding weights. The new adapter with the combined weights can then be set active using the `set_adapter` method.",
        "source_doc": "huggingface/peft/blob/main/docs/source/developer_guides/lora.md"
    },
    {
        "context": "To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https://github.com/huggingface/transformers/tree/main/examples).\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Documentation](https://huggingface.co/docs/transformers/) | Full API documentation and tutorials |\n| [Task summary](https://huggingface.co/docs/transformers/task_summary) | Tasks supported by 🤗 Transformers |\n| [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n| [Training and fine-tuning](https://huggingface.co/docs/transformers/training) | Using the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the `Trainer` API |\n| [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/main/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |\n\n## Citation",
        "question": "Which section of the documentation provides example scripts for fine-tuning models on a wide range of tasks?\n",
        "answer": "Quick tour: Fine-tuning/usage scripts",
        "source_doc": "huggingface/transformers/blob/main/README.md"
    },
    {
        "context": "`coremltools` now includes a new submodule called `coremltools.optimize` with all the compression and optimization tools. For full details on this package, please take a look at [this WWDC session](https://developer.apple.com/wwdc23/10047). In the case of Stable Diffusion, we’ll be using _6-bit palettization_, a type of quantization that compresses model weights from a 16-bit floating-point representation to just 6 bits per parameter. The name “palettization” refers to a technique similar to the one used in computer graphics to work with a limited set of colors: the color table (or “palette”) contains a fixed number of colors, and the colors in the image are replaced with the indexes of the closest colors available in the palette. This immediately provides the benefit of drastically reducing storage size, and thus reducing download time and on-device disk use.\n\n<img style=\"border:none;\" alt=\"Illustration of 2-bit palettization. Image credit: Apple WWDC’23 Session 'Use Core ML Tools for machine learning model compression'\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-diffusers-coreml/palettization_illustration.png\" />\n<small>Illustration of 2-bit palettization. Image credit: Apple WWDC’23 Session <i><a href=\"https://developer.apple.com/wwdc23/10047\">Use Core ML Tools for machine learning model compression</a></i>.</small>",
        "question": "What is the name of the quantization technique used in Stable Diffusion?\n",
        "answer": "The name of the quantization technique used in Stable Diffusion is 6-bit palettization.",
        "source_doc": "huggingface/blog/blob/main/fast-diffusers-coreml.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# AutoPeftModels\n\nThe `AutoPeftModel` classes loads the appropriate PEFT model for the task type by automatically inferring it from the configuration file. They are designed to quickly and easily load a PEFT model in a single line of code without having to worry about which exact model class you need or manually loading a [`PeftConfig`].\n\n## AutoPeftModel\n\n[[autodoc]] auto.AutoPeftModel\n    - from_pretrained\n\n## AutoPeftModelForCausalLM\n\n[[autodoc]] auto.AutoPeftModelForCausalLM\n\n## AutoPeftModelForSeq2SeqLM\n\n[[autodoc]] auto.AutoPeftModelForSeq2SeqLM\n\n## AutoPeftModelForSequenceClassification\n\n[[autodoc]] auto.AutoPeftModelForSequenceClassification\n\n## AutoPeftModelForTokenClassification\n\n[[autodoc]] auto.AutoPeftModelForTokenClassification\n\n## AutoPeftModelForQuestionAnswering\n\n[[autodoc]] auto.AutoPeftModelForQuestionAnswering\n\n## AutoPeftModelForFeatureExtraction\n\n[[autodoc]] auto.AutoPeftModelForFeatureExtraction",
        "question": "What is the purpose of the AutoPeftModel classes in Hugging Face Transformers?\n",
        "answer": "The AutoPeftModel classes in Hugging Face Transformers are designed to quickly and easily load a PEFT (Parameter-Efficient Fine-Tuning) model in a single line of code without having to worry about which exact model class is needed or manually loading a PeftConfig. They automatically infer the appropriate PEFT model for the task type from the configuration file.",
        "source_doc": "huggingface/peft/blob/main/docs/source/package_reference/auto_class.md"
    },
    {
        "context": "```py\n>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer\n\n>>> model = AutoModelForCTC.from_pretrained(\n...     \"facebook/wav2vec2-base\",\n...     ctc_loss_reduction=\"mean\",\n...     pad_token_id=processor.tokenizer.pad_token_id,\n... )\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the WER and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_asr_mind_model\",\n...     per_device_train_batch_size=8,\n...     gradient_accumulation_steps=2,\n...     learning_rate=1e-5,\n...     warmup_steps=500,\n...     max_steps=2000,\n...     gradient_checkpointing=True,\n...     fp16=True,\n...     group_by_length=True,\n...     evaluation_strategy=\"steps\",\n...     per_device_eval_batch_size=8,\n...     save_steps=1000,\n...     eval_steps=1000,\n...     logging_steps=25,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"wer\",\n...     greater_is_better=False,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=encoded_minds[\"train\"],\n...     eval_dataset=encoded_minds[\"test\"],\n...     tokenizer=processor,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>",
        "question": "What is the required parameter for TrainingArguments?\n",
        "answer": "The required parameter for TrainingArguments is output_dir.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/asr.md"
    },
    {
        "context": "As we can see in the diagram, **it’s more probable to eat the cheese near us than the cheese close to the cat** (the closer we are to the cat, the more dangerous it is).\n\nConsequently, **the reward near the cat, even if it is bigger (more cheese), will be more discounted** since we’re not really sure we’ll be able to eat it.\n\nTo discount the rewards, we proceed like this:\n\n1. We define a discount rate called gamma. **It must be between 0 and 1.** Most of the time between **0.95 and 0.99**.\n- The larger the gamma, the smaller the discount. This means our agent **cares more about the long-term reward.**\n- On the other hand, the smaller the gamma, the bigger the discount. This means our **agent cares more about the short term reward (the nearest cheese).**\n\n2. Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, **so the future reward is less and less likely to happen.**\n\nOur discounted expected cumulative reward is:\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" alt=\"Rewards\" width=\"100%\">",
        "question": "What is the range of the gamma discount rate?\n",
        "answer": "The gamma discount rate must be between 0 and 1.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/rl-framework.mdx"
    },
    {
        "context": "For example, if you want to upload two files and delete a file in a Hub repository:\n\n1. Use the appropriate `CommitOperation` to add or delete a file and to delete a folder:\n\n```py\n>>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete\n>>> api = HfApi()\n>>> operations = [\n...     CommitOperationAdd(path_in_repo=\"LICENSE.md\", path_or_fileobj=\"~/repo/LICENSE.md\"),\n...     CommitOperationAdd(path_in_repo=\"weights.h5\", path_or_fileobj=\"~/repo/weights-final.h5\"),\n...     CommitOperationDelete(path_in_repo=\"old-weights.h5\"),\n...     CommitOperationDelete(path_in_repo=\"logs/\"),\n...     CommitOperationCopy(src_path_in_repo=\"image.png\", path_in_repo=\"duplicate_image.png\"),\n... ]\n```\n\n2. Pass your operations to [`create_commit`]:\n\n```py\n>>> api.create_commit(\n...     repo_id=\"lysandre/test-model\",\n...     operations=operations,\n...     commit_message=\"Upload my model weights and license\",\n... )\n```\n\nIn addition to [`upload_file`] and [`upload_folder`], the following functions also use [`create_commit`] under the hood:\n\n- [`delete_file`] deletes a single file from a repository on the Hub.\n- [`delete_folder`] deletes an entire folder from a repository on the Hub.\n- [`metadata_update`] updates a repository's metadata.\n\nFor more detailed information, take a look at the [`HfApi`] reference.\n\n### Preupload LFS files before commit",
        "question": "How to preupload LFS files before commit in Hugging Face Hub?\n",
        "answer": "The context does not provide information on how to preupload LFS files before commit in Hugging Face Hub.",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/upload.md"
    },
    {
        "context": "One of the key useful traits of ML is that it can learn from and find hidden patterns in large volumes of data. With a focus on digitization, the financial sector is producing digital data more than ever, which makes it challenging for humans to comprehend, process and make decisions. ML is enabling humans in making sense of the data, glean information from them, and make well-informed decisions. At Moody's Analytics, we are using ML and helping our clients to better manage risk and meet business and industry demands. \n\n \n#### **2. What are the biggest ML challenges within finance?**\n1. Reducing the False Positives without impacting the True Positives  - A number of applications using ML in the regtech space rely on alerts. With strict regulatory measures and big financial implications of a wrong decision, human investigations can be time consuming and demanding. ML certainly helps in these scenarios in assisting human analysts to arrive at the right decisions. But if a ML system results in a lot of False Positives, it makes an analysts' job harder. Coming up with the right balance is an important challenge for ML in finance.\n\n2. Gap between ML in basic research and education and ML in finance - Due to the regulated nature of the finance industry, we see limited exchange of ideas, data, and resources between the basic research and the finance sector, in the area of ML. There are few exceptions of course. This has led to scarcity of developing ML research that cater to the needs of the finance industry. I think more efforts must be made to decrease this gap. Otherwise, it will be increasingly challenging for the finance industry to leverage the latest ML advances.",
        "question": "What is the gap between ML in basic research and education and ML in finance?\n",
        "answer": "The gap between ML in basic research and education and ML in finance is that there is limited exchange of ideas, data, and resources between the two areas due to the regulated nature of the finance industry.",
        "source_doc": "huggingface/blog/blob/main/ml-director-insights-3.md"
    },
    {
        "context": "- Where to find the pretrained weights?\n- How to load the pretrained weights into the corresponding model?\n- How to run the tokenizer independently from the model?\n- Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,\n  you only have to reimplement those functions.\n- Be able to locate the important components of the model: Where is the model's class? Are there model sub-classes,\n  *e.g.* EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,\n  *e.g.* *self-attention*, *cross-attention*...?\n- How can you debug the model in the original environment of the repo? Do you have to add *print* statements, can you\n  work with an interactive debugger like *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?\n\nIt is very important that before you start the porting process, you can **efficiently** debug code in the original\nrepository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or\neven a pull request in the original repository. The maintainers of this repository are most likely very happy about\nsomeone looking into their code!\n\nAt this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original\nmodel. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to\ndive into the original repository and also when starting to write the 🤗 Transformers implementation of the model. Only\nat the very end, when the model has already been successfully ported to 🤗 Transformers, one should verify that the\nmodel also works as expected on GPU.\n\nIn general, there are two possible debugging environments for running the original model\n\n-  [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\n-  Local python scripts.",
        "question": "How many debugging environments are mentioned in the context?\n",
        "answer": "Two debugging environments are mentioned in the context.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/add_new_model.md"
    },
    {
        "context": "The range of this metric is [0, inf). A lower score is better.\n\n#### Values from Popular Papers\n\n\n### Examples\nCalculating perplexity on input_texts defined here:\n```python\nperplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\ninput_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\nresults = perplexity.compute(model_id='gpt2',\n                             add_start_token=False,\n                             data=input_texts)\nprint(list(results.keys()))\n>>>['perplexities', 'mean_perplexity']\nprint(round(results[\"mean_perplexity\"], 2))\n>>>646.75\nprint(round(results[\"perplexities\"][0], 2))\n>>>32.25\n```\nCalculating perplexity on input_texts loaded in from a dataset:\n```python\nperplexity = evaluate.load(\"perplexity\", module_type= \"measurement\")\ninput_texts = datasets.load_dataset(\"wikitext\",\n                                    \"wikitext-2-raw-v1\",\n                                    split=\"test\")[\"text\"][:50]\ninput_texts = [s for s in input_texts if s!='']\nresults = perplexity.compute(model_id='gpt2',\n                             data=input_texts)\nprint(list(results.keys()))\n>>>['perplexities', 'mean_perplexity']\nprint(round(results[\"mean_perplexity\"], 2))\n>>>576.76\nprint(round(results[\"perplexities\"][0], 2))\n>>>889.28\n```\n\n## Limitations and Bias\nNote that the output value is based heavily on what text the model was trained on. This means that perplexity scores are not comparable between models or datasets.\n\n\n## Citation\n\n```bibtex\n@article{jelinek1977perplexity,\ntitle={Perplexity—a measure of the difficulty of speech recognition tasks},\nauthor={Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K},\njournal={The Journal of the Acoustical Society of America},\nvolume={62},\nnumber={S1},\npages={S63--S63},\nyear={1977},\npublisher={Acoustical Society of America}\n}\n```\n\n## Further References\n- [Hugging Face Perplexity Blog Post](https://huggingface.co/docs/transformers/perplexity)",
        "question": "What is the range of the perplexity metric?\n",
        "answer": "The range of this metric is [0, inf).",
        "source_doc": "huggingface/evaluate/blob/main/measurements/perplexity/README.md"
    },
    {
        "context": "# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\n\n## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation",
        "question": "How many top categories are printed per image?\n",
        "answer": "5",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md"
    },
    {
        "context": "Contrib test suite\n\nThe contrib folder contains simple end-to-end scripts to test integration of `huggingface_hub` in downstream libraries. The main goal is to proactively notice breaking changes and deprecation warnings.\n\n## Add tests for a new library\n\nTo add another contrib lib, one must:\n1. Create a subfolder with the lib name. Example: `./contrib/transformers`\n2. Create a `requirements.txt` file specific to this lib. Example `./contrib/transformers/requirements.txt`\n3. Implements tests for this lib. Example: `./contrib/transformers/test_push_to_hub.py`\n4. Run `make style`. This will edit both `makefile` and `.github/workflows/contrib-tests.yml` to add the lib to list of libs to test. Make sure changes are accurate before committing.\n\n## Run contrib tests on CI\n\nContrib tests can be [manually triggered in GitHub](https://github.com/huggingface/huggingface_hub/actions) with the `Contrib tests` workflow.\n\nTests are not run in the default test suite (for each PR) as this would slow down development process. The goal is to notice breaking changes, not to avoid them. In particular, it is interesting to trigger it before a release to make sure it will not cause too much friction.\n\n## Run contrib tests locally\n\nTests must be ran individually for each dependent library. Here is an example to run\n`timm` tests. Tests are separated to avoid conflicts between version dependencies.\n\n### Run all contrib tests\n\nBefore running tests, a virtual env must be setup for each contrib library. To do so, run:\n\n```sh\n# Run setup in parallel to save time \nmake contrib_setup -j4\n```\n\nThen tests can be run\n\n```sh\n# Optional: -j4 to run in parallel. Output will be messy in that case.\nmake contrib_test -j4\n```\n\nOptionally, it is possible to setup and run all tests in a single command. However this\ntake more time as you don't need to setup the venv each time you run tests.\n\n```sh\nmake contrib -j4\n```",
        "question": "How to run contrib tests locally?\n",
        "answer": "To run contrib tests locally, first set up a virtual environment for each contrib library using the command `make contrib_setup -j4`. Then, run the tests using the command `make contrib_test -j4`. Alternatively, you can set up and run all tests in a single command using `make contrib -j4`.",
        "source_doc": "huggingface/huggingface_hub/blob/main/contrib/README.md"
    },
    {
        "context": "n a lot of our examples, you're going to see DataCollators popping up over and over. They're used in both PyTorch and TensorFlow workflows, and maybe even in JAX, but no-one really knows what's happening in JAX. We have a research team working on that, so maybe they'll tell us soon. But what are data collators? Data collators collate data. More specifically, they put together a list of samples into a single training minibatch. For some tasks, the data collator can be very straightforward. For example, when you're doing sequence classification, all you really need from your data collator is that it pads your samples to the same length and concatenates them into a single Tensor. But for other workflows, data collators can be more complex, as they handle some of the preprocessing needed for that particular task. For PyTorch users, you usually pass the DataCollator to your Trainer object. In TensorFlow, the easiest way to use a DataCollator is to pass it to the to_tf_dataset method of your dataset. You'll see these approaches used in the examples and notebooks throughout this course. In both cases, you end up with an iterable that's going to output collated batches, ready for training. Note that all of our collators take a return_tensors argument - you can set this to \"pt\" to get PyTorch Tensors, \"tf\" to get TensorFlow Tensors, or \"np\" to get Numpy arrays. For backward compatibility reasons, the default value is \"pt\", so PyTorch users don't even have to set this argument most of the time, and so are often totally unaware that this option exists. This is a valuable lesson about how the beneficiaries of privilege are often the most blind to its existence. So now let's see some specific DataCollators in action, though remember that if none of them do what you need, you can always write your own! First, we'll see the \"basic\" data collators. These are DefaultDataCollator and DataCollatorWithPadding",
        "question": "What is the main function of DataCollators in machine learning?\n",
        "answer": "DataCollators collate data by putting together a list of samples into a single training minibatch. They can also handle some of the preprocessing needed for a particular task.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter7/08_data-collators.md"
    },
    {
        "context": "For web development, a [JS client](https://huggingface.co/docs/huggingface.js/inference/README) has been released.\nIf you are interested in game development, you might have a look at our [C# project](https://github.com/huggingface/unity-api).\n\n</Tip>\n\n## Getting started\n\nLet's get started with a text-to-image task:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n>>> image = client.text_to_image(\"An astronaut riding a horse on the moon.\")\n>>> image.save(\"astronaut.png\")\n```\n\nWe initialized an [`InferenceClient`] with the default parameters. The only thing you need to know is the [task](#supported-tasks) you want\nto perform. By default, the client will connect to the Inference API and select a model to complete the task. In our\nexample, we generated an image from a text prompt. The returned value is a `PIL.Image` object that can be saved to a\nfile.\n\n<Tip warning={true}>\n\nThe API is designed to be simple. Not all parameters and options are available or described for the end user. Check out\n[this page](https://huggingface.co/docs/api-inference/detailed_parameters) if you are interested in learning more about\nall the parameters available for each task.\n\n</Tip>\n\n### Using a specific model\n\nWhat if you want to use a specific model? You can specify it either as a parameter or directly at an instance level:\n\n```python\n>>> from huggingface_hub import InferenceClient\n# Initialize client for a specific model\n>>> client = InferenceClient(model=\"prompthero/openjourney-v4\")\n>>> client.text_to_image(...)\n# Or use a generic client but pass your model as an argument\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"prompthero/openjourney-v4\")\n```\n\n<Tip>",
        "question": "How to convert text to speech using",
        "answer": "You can use the `text_to_image` method of the `InferenceClient` to generate an image from a text prompt.\n\n</Tip>\n\n### Text-to-Speech\n\nThe Text-to-Speech task converts text to speech.\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> audio = client.text_to_speech(\"Hello, world!\")\n>>> audio.save(\"hello.wav\")\n```\n\n<Tip>\n\nOutput:::\nFactoid question: How to convert text to speech using",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/inference.md"
    },
    {
        "context": "# Ensemble Adversarial Inception ResNet v2\n\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on the Inception family of architectures but incorporates [residual connections](https://paperswithcode.com/method/residual-connection) (replacing the filter concatenation stage of the Inception architecture).\n\nThis particular model was trained for study of adversarial examples (adversarial training).\n\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\n\n## How do I use this model on an image?\n\nTo load a pretrained model:\n\n```py\n>>> import timm\n>>> model = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True)\n>>> model.eval()\n```\n\nTo load and preprocess the image:\n\n```py \n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve_data_config\n>>> from timm.data.transforms_factory import create_transform\n\n>>> config = resolve_data_config({}, model=model)\n>>> transform = create_transform(**config)\n\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]",
        "question": "How do I load a pretrained Ensemble Adversarial Inception ResNet v2 model in Python?\n",
        "answer": "To load a pretrained Ensemble Adversarial Inception ResNet v2 model in Python, you can use the `timm` library and the `create_model` function with the argument 'ens_adv_inception_resnet_v2' and `pretrained=True`. Here is an example:\n```\nimport timm\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True)\nmodel.eval()\n```",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/ensemble-adversarial.mdx"
    },
    {
        "context": "That was fun, right? \n\nWith just a few lines of code, you were able to automatically gather tweets mentioning Notion using Tweepy, analyze them with a sentiment analysis model using the [Inference API](https://huggingface.co/inference-api), and finally create some visualizations to analyze the results. 💥 \n\nAre you interested in doing more? As a next step, you could use a second [text classifier](https://huggingface.co/tasks/text-classification) to classify each tweet by their theme or topic. This way, each tweet will be labeled with both sentiment and topic, and you can get more granular insights (e.g. are users praising how easy to use is Notion but are complaining about their pricing or customer support?).\n\n## How to do Twitter sentiment analysis without coding?\n\nTo get started with sentiment analysis, you don't need to be a developer or know how to code. 🤯 \n\nThere are some amazing no-code solutions that will enable you to easily do sentiment analysis in just a few minutes. \n\nIn this section, you will use [Zapier](https://zapier.com/), a no-code tool that enables users to connect 5,000+ apps with an easy to use user interface. You will create a [Zap](https://zapier.com/help/create/basics/create-zaps), that is triggered whenever someone mentions Notion on Twitter. Then the Zap will use the [Inference API](https://huggingface.co/inference-api) to analyze the tweet with a sentiment analysis model and finally it will save the results on Google Sheets:\n\n1. Step 1 (trigger): Getting the tweets.\n2. Step 2: Analyze tweets with sentiment analysis.\n3. Step 3: Save the results on Google Sheets.\n\nNo worries, it won't take much time; in under 10 minutes, you'll create and activate the zap, and will start seeing the sentiment analysis results pop up in Google Sheets.\n\nLet's get started! 🚀\n\n### Step 1: Getting the Tweets",
        "question": "What is the first step in doing Twitter sentiment analysis without coding?\n",
        "answer": "The first step in doing Twitter sentiment analysis without coding is getting the tweets. This is done by creating a Zap in Zapier, a no-code tool, that is triggered whenever someone mentions Notion on Twitter.",
        "source_doc": "huggingface/blog/blob/main/sentiment-analysis-twitter.md"
    },
    {
        "context": ">> \"My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so don't find simpus, and devoted abode, to at might in a r—\"\n```\n\n## Advanced Usage\n\nFor more ways to use the Gradio Python Client, check out our dedicated Guide on the Python client, available here: https://www.gradio.app/guides/getting-started-with-the-python-client",
        "question": "What is the URL for the dedicated guide on the Python client?\n",
        "answer": "https://www.gradio.app/guides/getting-started-with-the-python-client",
        "source_doc": "gradio-app/gradio/blob/main/client/python/README.md"
    },
    {
        "context": "Process text data\n\nThis guide shows specific methods for processing text datasets. Learn how to:\n\n- Tokenize a dataset with [`~Dataset.map`].\n- Align dataset labels with label ids for NLI datasets.\n\nFor a guide on how to process any type of dataset, take a look at the <a class=\"underline decoration-sky-400 decoration-2 font-semibold\" href=\"./process\">general process guide</a>.\n\n## Map\n\nThe [`~Dataset.map`] function supports processing batches of examples at once which speeds up tokenization.\n\nLoad a tokenizer from 🤗 [Transformers](https://huggingface.co/transformers/):\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n```\n\nSet the `batched` parameter to `True` in the [`~Dataset.map`] function to apply the tokenizer to batches of examples:\n\n```py\n>>> dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n>>> dataset[0]\n{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', \n 'label': 1, \n 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```",
        "question": "What is the function used to tokenize a dataset in the context?\n",
        "answer": "The function used to tokenize a dataset in the context is [`~Dataset.map`].",
        "source_doc": "huggingface/datasets/blob/main/docs/source/nlp_process.mdx"
    },
    {
        "context": "- The images - actually a sequence of frames - of shape (batch_size, 16, 3, 224, 224) are turned into a tensor of shape (batch_size, 50176, 243) using `PerceiverImagePreprocessor`. This is a “space to depth” transformation, after which fixed 2D Fourier position embeddings are concatenated.\n- The audio has shape (batch_size, 30720, 1) and is turned into a tensor of shape (batch_size, 1920, 401) using `PerceiverAudioPreprocessor` (which concatenates fixed Fourier position embeddings to the raw audio).\n- The class label of shape (batch_size, 700) is turned into a tensor of shape (batch_size, 1, 700) using `PerceiverOneHotPreprocessor`. In other words, this preprocessor just adds a dummy time (index) dimension. Note that one initializes the class label with a tensor of zeros during evaluation, so as to let the model act as a video classifier. \n\nNext, `PerceiverMultimodalPreprocessor` will pad the preprocessed modalities with modality-specific trainable embeddings to make concatenation along the time dimension possible. In this case, the modality with the highest channel dimension is the class label (it has 700 channels). The authors enforce a minimum padding size of 4, hence each modality will be padded to have 704 channels. They can then be concatenated, hence the final preprocessed input is a tensor of shape (batch_size, 50176 + 1920 + 1, 704) = (batch_size, 52097, 704). \n\nThe authors use 784 latents, with a dimensionality of 512 for each latent. Hence, the latents have shape (batch_size, 784, 512). After the cross-attention, one again has a tensor of the same shape (as the latents act as queries). Next, a single block of 8 self-attention layers (each of which has 8 attention heads) is applied to update the embeddings of the latents.",
        "question": "What is the shape of the final preprocessed input?\n",
        "answer": "The final preprocessed input is a tensor of shape (batch_size, 52097, 704).",
        "source_doc": "huggingface/blog/blob/main/perceiver.md"
    },
    {
        "context": "There's also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\n\n## Fine-tuning on CORD\n\nFine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:\n\n```bash\npython run_funsd_cord.py \\\n  --model_name_or_path microsoft/layoutlmv3-base \\\n  --dataset_name cord \\\n  --output_dir layoutlmv3-test \\\n  --do_train \\\n  --do_eval \\\n  --max_steps 1000 \\\n  --evaluation_strategy steps \\\n  --eval_steps 100 \\\n  --learning_rate 5e-5 \\\n  --load_best_model_at_end \\\n  --metric_for_best_model \"eval_f1\" \\\n  --push_to_hub \\\n  --push_to_hub°model_id layoutlmv3-finetuned-cord\n```\n\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.",
        "question": "Where can I find the resulting model after fine-tuning LayoutLMv3 for token classification on CORD?\n",
        "answer": "The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md"
    },
    {
        "context": "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large applications.\n\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\n- Corrected an issue that was causing markdown to re-render infinitely.\n- Ensured that the `gr.3DModel` does re-render prematurely.\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5268](https://github.com/gradio-app/gradio/pull/5268) [`f49028cf`](https://github.com/gradio-app/gradio/commit/f49028cfe3e21097001ddbda71c560b3d8b42e1c) - Move markdown & latex processing to the frontend for the gr.Markdown and gr.DataFrame components. Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\n- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!\n\n### Fixes",
        "question": "What is the performance improvement for large applications after the recent version of Gradio?\n",
        "answer": "The performance improvement for large applications after the recent version of Gradio is around twice as fast.",
        "source_doc": "gradio-app/gradio/blob/main/js/dataframe/CHANGELOG.md"
    },
    {
        "context": "<Tip>\n\nUse [`YolosImageProcessor`] for preparing images (and optional targets) for the model. Contrary to [DETR](detr), YOLOS doesn't require a `pixel_mask` to be created.\n\n</Tip>\n\n## YolosConfig\n\n[[autodoc]] YolosConfig\n\n## YolosImageProcessor\n\n[[autodoc]] YolosImageProcessor\n    - preprocess\n    - pad\n    - post_process_object_detection\n\n## YolosFeatureExtractor\n\n[[autodoc]] YolosFeatureExtractor\n    - __call__\n    - pad\n    - post_process_object_detection\n\n## YolosModel\n\n[[autodoc]] YolosModel\n    - forward\n\n## YolosForObjectDetection\n\n[[autodoc]] YolosForObjectDetection\n    - forward",
        "question": "What is the difference between YOLOS and DETR in terms of image preparation?\n",
        "answer": "YOLOS doesn't require a `pixel_mask` to be created for image preparation, unlike DETR.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/yolos.md"
    },
    {
        "context": "Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the Inception family that makes several improvements including using [Label Smoothing](https://paperswithcode.com/method/label-smoothing), Factorized 7 x 7 convolutions, and the use of an [auxiliary classifer](https://paperswithcode.com/method/auxiliary-classifier) to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an [Inception Module](https://paperswithcode.com/method/inception-v3-module).\n\n## How do I use this model on an image?\n\nTo load a pretrained model:\n\n```py\n>>> import timm\n>>> model = timm.create_model('inception_v3', pretrained=True)\n>>> model.eval()\n```\n\nTo load and preprocess the image:\n\n```py \n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve_data_config\n>>> from timm.data.transforms_factory import create_transform\n\n>>> config = resolve_data_config({}, model=model)\n>>> transform = create_transform(**config)\n\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]",
        "question": "How do I load a pretrained Inception v3 model in Python?\n",
        "answer": "To load a pretrained Inception v3 model in Python, you can use the `timm` library and the `create_model` function with the argument 'inception_v3' and `pretrained=True`. Here is an example:\n```\nimport timm\nmodel = timm.create_model('inception_v3', pretrained=True)\nmodel.eval()\n```\nThis will load the pretrained Inception v3 model and set it in evaluation mode.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/inception-v3.mdx"
    },
    {
        "context": "## 🚴 Usage\n\n```\nnpm init wasm-app\n```\n\n## 🔋 Batteries Included\n\n- `.gitignore`: ignores `node_modules`\n- `LICENSE-APACHE` and `LICENSE-MIT`: most Rust projects are licensed this way, so these are included for you\n- `README.md`: the file you are reading now!\n- `index.html`: a bare bones html document that includes the webpack bundle\n- `index.js`: example js file with a comment showing how to import and use a wasm pkg\n- `package.json` and `package-lock.json`:\n  - pulls in devDependencies for using webpack:\n      - [`webpack`](https://www.npmjs.com/package/webpack)\n      - [`webpack-cli`](https://www.npmjs.com/package/webpack-cli)\n      - [`webpack-dev-server`](https://www.npmjs.com/package/webpack-dev-server)\n  - defines a `start` script to run `webpack-dev-server`\n- `webpack.config.js`: configuration file for bundling your js with webpack\n\n## License\n\nLicensed under either of\n\n* Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nat your option.\n\n### Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally\nsubmitted for inclusion in the work by you, as defined in the Apache-2.0\nlicense, shall be dual licensed as above, without any additional terms or\nconditions.",
        "question": "What is the name of the license used in the project?\n",
        "answer": "The project is licensed under either the Apache License, Version 2.0 or the MIT license.",
        "source_doc": "huggingface/tokenizers/blob/main/tokenizers/examples/unstable_wasm/www/README.md"
    },
    {
        "context": ", clone it locally (assuming the `<username>` is `hf-test`)\n\n```bash\ngit clone hf-test/xls-r-300m-sv\n```\n\n, and, define the following hyperparameters for training\n\n```bash\necho '''python run_speech_recognition_ctc.py \\\n\t--dataset_name=\"mozilla-foundation/common_voice_7_0\" \\\n\t--model_name_or_path=\"facebook/wav2vec2-xls-r-300m\" \\\n\t--dataset_config_name=\"sv-SE\" \\\n\t--output_dir=\"./\" \\\n\t--overwrite_output_dir \\\n\t--num_train_epochs=\"50\" \\\n\t--per_device_train_batch_size=\"8\" \\\n\t--per_device_eval_batch_size=\"8\" \\\n\t--gradient_accumulation_steps=\"4\" \\\n\t--learning_rate=\"7.5e-5\" \\\n\t--warmup_steps=\"2000\" \\\n\t--length_column_name=\"input_length\" \\\n\t--evaluation_strategy=\"steps\" \\\n\t--text_column_name=\"sentence\" \\\n\t--chars_to_ignore , ? . ! \\- \\; \\: \\\" “ % ‘ ” � — ’ … – \\\n\t--save_steps=\"500\" \\\n\t--eval_steps=\"500\" \\\n\t--logging_steps=\"100\" \\\n\t--layerdrop=\"0.0\" \\\n\t--activation_dropout=\"0.1\" \\\n\t--save_total_limit=\"3\" \\\n\t--freeze_feature_encoder \\\n\t--feat_proj_dropout=\"0.0\" \\\n\t--mask_time_prob=\"0.75\" \\\n\t--mask_time_length=\"10\" \\\n\t--mask_feature_prob=\"0.25\" \\\n\t--mask_feature_length=\"64\" \\\n\t--gradient_checkpointing \\\n\t--use_auth_token \\\n\t--fp16 \\\n\t--group_by_length \\\n\t--do_train --do_eval \\\n\t--push_to_hub''' > run.sh\n```\n\nThe training takes *ca.* 7 hours and yields a reasonable test word \nerror rate of 27% as can be seen on the automatically generated [model card](https://huggingface.co/hf-test/xls-r-300m-sv).\n\nThe above-chosen hyperparameters probably work quite well on a range of different \ndatasets and languages but are by no means optimal. It is up to you to find a good set of \nhyperparameters.\n\n\n## How to finetune with OVH cloud\n\n[![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/XkMnYocAEO0) For a more detailed guide on setting up OVHcloud please watch this video: https://youtu.be/XkMnYocAEO0",
        "question": "How long does it take to train the model on OVH cloud?\n",
        "answer": "The training takes approximately 7 hours.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM-RoBERTa-XL\n\n## Overview\n\nThe XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. \n\nThe abstract from the paper is the following:\n\n*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*\n\nThis model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).",
        "question": "How many parameters does the XLM-RoBERTa-XL model have?\n",
        "answer": "The XLM-RoBERTa-XL model has 3.5 billion parameters.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/xlm-roberta-xl.md"
    },
    {
        "context": "#### 3. Change the generation strategy\n\nYou can use different [generation strategies](../generation_strategies) for text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, text_do_sample=True)` which will perform multinomial beam-search decoding on the text model. Note that speech generation only supports greedy - by default - or multinomial sampling, which can be used with e.g. `.generate(..., speech_do_sample=True, speech_temperature=0.6)`.\n\n#### 4. Generate speech and text at the same time\n\nUse `return_intermediate_token_ids=True` with [`SeamlessM4Tv2Model`] to return both speech and text !\n\n## Model architecture\n\nSeamlessM4T-v2 features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n\nEach modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.\n\n### Difference with SeamlessM4T-v1\n\nThe architecture of this new version differs from the first in a few aspects:\n\n#### Improvements on the second-pass model\n\nThe second seq2seq model, named text-to-unit model, is now non-auto regressive, meaning that it computes units in a **single forward pass**. This achievement is made possible by:\n- the use of **character-level embeddings**, meaning that each character of the predicted translated text has its own embeddings, which are then used to predict the unit tokens.\n- the use of an intermediate duration predictor, that predicts speech duration at the **character-level** on the predicted translated text.\n- the use of a new text-to-unit decoder mixing convolutions and self-attention to handle longer context.",
        "question": "What is the difference between SeamlessM4T-v1 and SeamlessM4T-v2 in terms of the second-pass model?\n",
        "answer": "The second seq2seq model, named text-to-unit model, in SeamlessM4T-v2 is now non-auto regressive, meaning that it computes units in a single forward pass. This is achieved through the use of character-level embeddings, an intermediate duration predictor that predicts speech duration at the character-level on the predicted translated text, and a new text-to-unit decoder mixing convolutions and self-attention to handle longer context.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md"
    },
    {
        "context": "For now, Transformers supports SDPA inference and training for the following architectures:\n* [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n* [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)\n* [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n* [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n\n<Tip>\n\nFlashAttention can only be used for models with the `fp16` or `bf16` torch type, so make sure to cast your model to the appropriate type first.\n\n</Tip>\n\nBy default, SDPA selects the most performant kernel available but you can check whether a backend is available in a given setting (hardware, problem size) with [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager:\n\n```diff\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\n# convert the model to BetterTransformer\nmodel.to_bettertransformer()\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    outputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nIf you see a bug with the traceback below, try using the nightly version of PyTorch which may have broader coverage for FlashAttention:\n\n```bash\nRuntimeError: No available kernel. Aborting execution.",
        "question": "Which models support SDPA inference and training in Transformers?\n",
        "answer": "Bart, GPTBigCode, Falcon, Llama, Idefics, and Whisper models support SDPA inference and training in Transformers.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md"
    },
    {
        "context": "- [#5312](https://github.com/gradio-app/gradio/pull/5312) [`f769cb67`](https://github.com/gradio-app/gradio/commit/f769cb67149d8e209091508f06d87014acaed965) - only start listening for events after the components are mounted.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5254](https://github.com/gradio-app/gradio/pull/5254) [`c39f06e1`](https://github.com/gradio-app/gradio/commit/c39f06e16b9feea97984e4822df35a99c807461c) - Fix `.update()` for `gr.Radio()` and `gr.CheckboxGroup()`.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5231](https://github.com/gradio-app/gradio/pull/5231) [`87f1c2b4`](https://github.com/gradio-app/gradio/commit/87f1c2b4ac7c685c43477215fa5b96b6cbeffa05) - Allow `gr.Interface.from_pipeline()` and `gr.load()` to work within `gr.Blocks()`.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5238](https://github.com/gradio-app/gradio/pull/5238) [`de23e9f7`](https://github.com/gradio-app/gradio/commit/de23e9f7d67e685e791faf48a21f34121f6d094a) - Improve audio streaming.  Thanks [@aliabid94](https://github.com/aliabid94)!/n  - Proper audio streaming with WAV files. We now do the proper processing to stream out wav files as a single stream of audio without any cracks in the seams./n  - Audio streaming with bytes. Stream any audio type by yielding out bytes, and it should work flawlessly.\n- [#5313](https://github.com/gradio-app/gradio/pull/5313) [`54bcb724`](https://github.com/gradio-app/gradio/commit/54bcb72417b2781ad9d7500ea0f89aa9d80f7d8f) - Restores missing part of bottom border on file component.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5235](https://github.com/gradio-app/gradio/pull/5235) [`1ecf88ac`](https://github.com/gradio-app/gradio/commit/1ecf88ac5f20bc5a1c91792d1a68559575e6afd7) - fix #5229.  Thanks [@breengles](https://github.com/breengles)!",
        "question": "Which Gradio components were fixed to have their bottom border restored?\n",
        "answer": "The file component",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (来自 Google AI) 伴随论文 [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) 由 Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu 发布。\n1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (来自 Google AI) 伴随论文 [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) 由 Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu 发布。\n1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (来自 Microsoft Research) 伴随论文 [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) 由 Brandon Smock, Rohith Pesala, Robin Abraham 发布。\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (来自 Google AI) 伴随论文 [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) 由 Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos 发布。\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (来自 Microsoft Research) 伴随论文 [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) 由 Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou 发布。\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).",
        "question": "Which model is from HuggingFace and is used for time series data?\n",
        "answer": "The Time Series Transformer is from HuggingFace and is used for time series data.",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hans.md"
    },
    {
        "context": "Now we can define the search algorithm and the scheduler for the hyper-parameter-search. \n\n```python\nscheduler =  ASHAScheduler(metric='objective', mode='max')\nsearch_algorithm = HyperOptSearch(metric='objective', mode='max', random_state_seed=SEED)\n# number of runs for parameter searching\nn_trials =  40\n```\n\nWe also need to tokenize the text data before passing it to the model, we can easily do this by using the loaded tokenizer. Ray Tune works in a black-box setting so I used tokenizer as a default argument for a work-around. Otherwise, an error about tokenizer definition would arise.\n\n```python\ndef tokenize(sample, tokenizer=tokenizer):\n    tokenized_sample = tokenizer(sample['text'], padding=True, truncation=True)\n    tokenized_sample['label'] = sample['label']\n    return tokenized_sample\n```\n\nAnother utility function that returns stratified and tokenized Torch dataset splits:\n\n```python\ndef prepare_datasets(dataset_df, test_size=.2, val_size=.2):\n    train_set, test_set = train_test_split(dataset_df, test_size=test_size,\n                                        stratify=dataset_df.label, random_state=SEED)\n\n    train_set, val_set = train_test_split(train_set, test_size=val_size,\n                                        stratify=train_set.label, random_state=SEED)\n\n    # shuffle the dataframes beforehand \n    train_set = train_set.sample(frac=1, random_state=SEED)\n    val_set = val_set.sample(frac=1, random_state=SEED)\n    test_set = test_set.sample(frac=1, random_state=SEED)\n\n    # convert dataframes to torch datasets\n    train_dataset = TextClassificationDataset(train_set)\n    val_dataset = TextClassificationDataset(val_set)\n    test_dataset = TextClassificationDataset(test_set)\n\n    # tokenize the datasets\n    tokenized_train_set = train_dataset.map(tokenize)\n    tokenized_val_set = val_dataset.map(tokenize)\n    tokenized_test_set = test_dataset.map(tokenize)",
        "question": "What is the name of the scheduler used in the hyper-parameter-search?\n",
        "answer": "ASHAScheduler",
        "source_doc": "huggingface/blog/blob/main/opinion-classification-with-kili.md"
    },
    {
        "context": ">>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```\n\n### Expected speedups\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `susnato/phi-1_dev` checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/phi_1_speedup_plot.jpg\">\n</div>\n\n\n## PhiConfig\n\n[[autodoc]] PhiConfig\n\n<frameworkcontent>\n<pt>\n\n## PhiModel\n\n[[autodoc]] PhiModel\n    - forward\n\n## PhiForCausalLM\n\n[[autodoc]] PhiForCausalLM\n    - forward\n    - generate\n\n## PhiForSequenceClassification\n\n[[autodoc]] PhiForSequenceClassification\n    - forward\n\n## PhiForTokenClassification\n\n[[autodoc]] PhiForTokenClassification\n    - forward\n\n</pt>\n</frameworkcontent>",
        "question": "What is the name of the class that inherits from `PhiModel` and is used for causal language modeling?\n",
        "answer": "`PhiForCausalLM`",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/phi.md"
    },
    {
        "context": "Tasks:\n    - Image Classification\n    Training Techniques:\n    - AdvProp\n    - AutoAugment\n    - Label Smoothing\n    - RMSProp\n    - Stochastic Depth\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    ID: tf_efficientnet_b1_ap\n    LR: 0.256\n    Epochs: 350\n    Crop Pct: '0.882'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '240'\n    Weight Decay: 1.0e-05\n    Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1344\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ap-44ef0a3d.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.28%\n      Top 5 Accuracy: 94.3%\n- Name: tf_efficientnet_b2_ap\n  In Collection: AdvProp\n  Metadata:\n    FLOPs: 1234321170\n    Parameters: 9110000\n    File Size: 36800745\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AdvProp\n    - AutoAugment\n    - Label Smoothing\n    - RMSProp\n    - Stochastic Depth\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    ID: tf_efficientnet_b2_ap\n    LR: 0.256\n    Epochs: 350\n    Crop Pct: '0.89'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '260'\n    Weight Decay: 1.0e-05\n    Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1354\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ap-2f8e7636.pth\n  Results:",
        "question": "What is the batch size used in tf_efficientnet_b2_ap?\n",
        "answer": "2048",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/advprop.mdx"
    },
    {
        "context": "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Blenderbot Small\n\nNote that [`BlenderbotSmallModel`] and\n[`BlenderbotSmallForConditionalGeneration`] are only used in combination with the checkpoint\n[facebook/blenderbot-90M](https://huggingface.co/facebook/blenderbot-90M). Larger Blenderbot checkpoints should\ninstead be used with [`BlenderbotModel`] and\n[`BlenderbotForConditionalGeneration`]\n\n## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:",
        "question": "Can the Blenderbot model generate responses that are not generic or repetitive?\n",
        "answer": "Yes, the Blenderbot model is able to generate responses that are not generic or repetitive.\n\nThe Blenderbot model is able to generate responses that are informative and interesting.\n\nOutput:::\nFactoid question",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/blenderbot-small.md"
    },
    {
        "context": "| **Hardware**        \t| **GPU Memory** \t| **CPU** \t| **Memory** \t| **Disk** \t| **Hourly Price** \t|\n|---------------------\t|----------------\t|----------\t|------------\t|----------\t| ----------------\t|\n| CPU Basic           \t| -             \t| 2 vCPU  \t| 16 GB     \t| 50 GB    \t| Free!            \t|\n| CPU Upgrade         \t| -             \t| 8 vCPU  \t| 32 GB      \t| 50 GB    \t| $0.03            \t|\n| Nvidia T4 - small   \t| 16GB          \t| 4 vCPU  \t| 15 GB      \t| 50 GB    \t| $0.60            \t|\n| Nvidia T4 - medium  \t| 16GB          \t| 8 vCPU  \t| 30 GB      \t| 100 GB   \t| $0.90            \t|\n| Nvidia A10G - small \t| 24GB          \t| 4 vCPU  \t| 15 GB      \t| 110 GB   \t| $1.05            \t|\n| Nvidia A10G - large \t| 24GB          \t| 12 vCPU \t| 46 GB      \t| 200 GB   \t| $3.15            \t|\n| 2x Nvidia A10G - large| 48GB          \t| 24 vCPU \t| 92 GB      \t| 1000 GB  \t| $5.70            \t|\n| 4x Nvidia A10G - large| 96GB          \t| 48 vCPU \t| 184 GB     \t| 2000 GB  \t| $10.80           \t|\n| Nvidia A100 - large \t| 40GB          \t| 12 vCPU \t| 142 GB     \t| 1000 GB  \t| $4.13            \t|\n \n| **Storage tier**     \t| **Size**             \t| **Persistent** \t| **Monthly price** \t|\n|---------------------\t|----------------------\t|------------------\t| ---------------------\t|\n| Ephemeral (default) \t| 50GB                \t| No               \t| Free!                \t|\n| Small               \t| Ephemeral + 20GB    \t| Yes              \t| $5                   \t|\n| Medium              \t| Ephemeral + 150GB   \t| Yes              \t| $25                  \t|\n| Large               \t| Ephemeral + 1TB     \t| yes              \t| $100                 \t|\n\nNote: Find more detailed and comprehensive pricing information on [our pricing page](https://huggingface.co/pricing).",
        "question": "What is the monthly price for the Medium storage tier?\n",
        "answer": "$25",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-overview.md"
    },
    {
        "context": "--\ntitle: \"Llama 2 on Amazon SageMaker a Benchmark\" \nthumbnail: /blog/assets/llama_sagemaker_benchmark/thumbnail.jpg\nauthors:\n- user: philschmid\n---\n\n# Llama 2 on Amazon SageMaker a Benchmark\n\n\n![Latency](assets/llama_sagemaker_benchmark/latency.png \"Latency\")\n\nDeploying large language models (LLMs) and other generative AI models can be challenging due to their computational requirements and latency needs. To provide useful recommendations to companies looking to deploy Llama 2 on Amazon SageMaker with the [Hugging Face LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm), we created a comprehensive benchmark analyzing over 60 different deployment configurations for Llama 2.\n\nIn this benchmark, we evaluated varying sizes of Llama 2 on a range of Amazon EC2 instance types with different load levels. Our goal was to measure latency (ms per token), and throughput (tokens per second) to find the optimal deployment strategies for three common use cases:\n\n- Most Cost-Effective Deployment: For users looking for good performance at low cost\n- Best Latency Deployment: Minimizing latency for real-time services\n- Best Throughput Deployment: Maximizing tokens processed per second\n\nTo keep this benchmark fair, transparent, and reproducible, we share all of the assets, code, and data we used and collected: \n\n- [GitHub Repository](https://github.com/philschmid/text-generation-inference-tests/tree/master/sagemaker_llm_container)\n- [Raw Data](https://github.com/philschmid/text-generation-inference-tests/tree/master/results/sagemaker)\n- [Spreadsheet with processed data](https://docs.google.com/spreadsheets/d/1PBjw6aG3gPaoxd53vp7ZtCdPngExi2vWPC0kPZXaKlw/edit?usp=sharing)\n\nWe hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used.",
        "question": "What is the goal of the benchmark?\n",
        "answer": "The goal of the benchmark is to measure latency and throughput of varying sizes of Llama 2 on a range of Amazon EC2 instance types with different load levels to find the optimal deployment strategies for three common use cases: Most Cost-Effective Deployment, Best Latency Deployment, and Best Throughput Deployment.",
        "source_doc": "huggingface/blog/blob/main/llama-sagemaker-benchmark.md"
    },
    {
        "context": "```python\nfrom transformers import Wav2Vec2Processor\n\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```\n\nNext, we can prepare the dataset.\n\n### Preprocess Data\n\nSo far, we have not looked at the actual values of the speech signal but\njust the transcription. In addition to `sentence`, our datasets include\ntwo more column names `path` and `audio`. `path` states the absolute\npath of the audio file. Let\\'s take a look.\n\n```python\ncommon_voice_train[0][\"path\"]\n```\n\nXLS-R expects the input in the format of a 1-dimensional array of 16\nkHz. This means that the audio file has to be loaded and resampled.\n\nThankfully, `datasets` does this automatically by calling the other\ncolumn `audio`. Let try it out.\n\n```python\ncommon_voice_train[0][\"audio\"]\n```\n\n```bash\n    {'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n            -8.8930130e-05, -3.8027763e-05, -2.9146671e-05], dtype=float32),\n     'path': '/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3',\n     'sampling_rate': 48000}\n```\n\nGreat, we can see that the audio file has automatically been loaded.\nThis is thanks to the new [`\"Audio\"`\nfeature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio)\nintroduced in `datasets == 1.18.3`, which loads and resamples audio\nfiles on-the-fly upon calling.\n\nIn the example above we can see that the audio data is loaded with a\nsampling rate of 48kHz whereas 16kHz are expected by the model. We can\nset the audio feature to the correct sampling rate by making use of\n[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column):",
        "question": "What is the name of the new feature introduced in datasets == 1.18.3?\n",
        "answer": "The new feature introduced in datasets == 1.18.3 is the `\"Audio\"` feature.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-xlsr-wav2vec2.md"
    },
    {
        "context": "| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb) | Show how to load and compare outputs from two Stable Diffusion models with different precision| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)|",
        "question": "What is the name of the notebook that shows how to load and compare outputs from two Stable Diffusion models with different precision?\n",
        "answer": "The name of the notebook is \"Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart\".",
        "source_doc": "huggingface/optimum/blob/main/notebooks/README.md"
    },
    {
        "context": "utomatic speech recognition English. Record from your microphone and the app will transcribe the audio.",
        "question": "How does the app transcribe audio?\n",
        "answer": "The app transcribes audio by using automatic speech recognition for English.",
        "source_doc": "gradio-app/gradio/blob/main/demo/automatic-speech-recognition/DESCRIPTION.md"
    },
    {
        "context": "- [#5642](https://github.com/gradio-app/gradio/pull/5642) [`21c7225bd`](https://github.com/gradio-app/gradio/commit/21c7225bda057117a9d3311854323520218720b5) - Improve plot rendering.  Thanks [@aliabid94](https://github.com/aliabid94)!\n- [#5677](https://github.com/gradio-app/gradio/pull/5677) [`9f9af327c`](https://github.com/gradio-app/gradio/commit/9f9af327c9115356433ec837f349d6286730fb97) - [Refactoring] Convert async functions that don't contain `await` statements to normal functions.  Thanks [@whitphx](https://github.com/whitphx)!\n- [#5660](https://github.com/gradio-app/gradio/pull/5660) [`d76555a12`](https://github.com/gradio-app/gradio/commit/d76555a122b545f0df7c9e7c1ca7bd2a6e262c86) - Fix secondary hue bug in gr.themes.builder().  Thanks [@hellofreckles](https://github.com/hellofreckles)!\n- [#5697](https://github.com/gradio-app/gradio/pull/5697) [`f4e4f82b5`](https://github.com/gradio-app/gradio/commit/f4e4f82b58a65efca9030a7e8e7c5ace60d8cc10) - Increase Slider clickable area.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n- [#5671](https://github.com/gradio-app/gradio/pull/5671) [`6a36c3b78`](https://github.com/gradio-app/gradio/commit/6a36c3b786700600d3826ce1e0629cc5308ddd47) - chore(deps): update dependency @types/prismjs to v1.26.1.  Thanks [@renovate](https://github.com/apps/renovate)!",
        "question": "Which dependency was updated to v1.26.1?\n",
        "answer": "@types/prismjs was updated to v1.26.1.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": ">>> # Print top categories per image\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5_prob.size(0)):\n...     print(categories[top5_catid[i]], top5_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\n\n## How do I finetune this model?\n\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n\n```py\n>>> model = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation",
        "question": "What is the name of the first class in the example?\n",
        "answer": "Samoyed",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/mobilenet-v3.mdx"
    },
    {
        "context": "Type of tasks [[tasks]]\n\nA task is an **instance** of a Reinforcement Learning problem. We can have two types of tasks: **episodic** and **continuing**.\n\n## Episodic task [[episodic-task]]\n\nIn this case, we have a starting point and an ending point **(a terminal state). This creates an episode**: a list of States, Actions, Rewards, and new States.\n\nFor instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends **when you’re killed or you reached the end of the level.**\n\n<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg\" alt=\"Mario\">\n<figcaption>Beginning of a new episode.\n</figcaption>\n</figure>\n\n\n## Continuing tasks [[continuing-tasks]]\n\nThese are tasks that continue forever (**no terminal state**). In this case, the agent must **learn how to choose the best actions and simultaneously interact with the environment.**\n\nFor instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. **The agent keeps running until we decide to stop it.**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/stock.jpg\" alt=\"Stock Market\" width=\"100%\">\n\nTo recap:\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/tasks.jpg\" alt=\"Tasks recap\" width=\"100%\">",
        "question": "What is the main difference between episodic and continuing tasks in reinforcement learning?\n",
        "answer": "Episodic tasks have a starting point and an ending point, creating an episode, while continuing tasks continue forever without a terminal state.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/tasks.mdx"
    },
    {
        "context": "String and binary objects are unchanged, since PyTorch only supports numbers.\n\nThe [`Image`] and [`Audio`] feature types are also supported.\n\n<Tip>\n\nTo use the [`Image`] feature type, you'll need to install the `vision` extra as\n`pip install datasets[vision]`.\n\n</Tip>\n\n```py\n>>> from datasets import Dataset, Features, Audio, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features) \n>>> ds = ds.with_format(\"tf\")  \n>>> ds[0]\n{'image': <tf.Tensor: shape=(512, 512, 4), dtype=uint8, numpy=\n array([[[255, 215, 106, 255],\n         [255, 215, 106, 255],\n         ...,\n         [255, 255, 255, 255],\n         [255, 255, 255, 255]]], dtype=uint8)>}\n>>> ds[:2]\n{'image': <tf.Tensor: shape=(2, 512, 512, 4), dtype=uint8, numpy=\n array([[[[255, 215, 106, 255],\n          [255, 215, 106, 255],\n          ...,\n          [255, 255, 255, 255],\n          [255, 255, 255, 255]]]], dtype=uint8)>}\n```\n\n<Tip>\n\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\n`pip install datasets[audio]`.\n\n</Tip>\n\n```py\n>>> from datasets import Dataset, Features, Audio, Image\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features) \n>>> ds = ds.with_format(\"tf\")  \n>>> ds[0][\"audio\"][\"array\"]\n<tf.Tensor: shape=(202311,), dtype=float32, numpy=\narray([ 6.1035156e-05,  1.5258789e-05,  1.6784668e-04, ...,\n       -1.5258789e-05, -1.5258789e-05,  1.5258789e-05], dtype=float32)>\n>>> ds[0][\"audio\"][\"sampling_rate\"]\n<tf.Tensor: shape=(), dtype=int32, numpy=44100>\n```\n\n## Data loading",
        "question": "What is the format of the data loaded using the `Dataset.from_dict` method?\n",
        "answer": "The data is loaded in the dictionary format using the `Dataset.from_dict` method.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx"
    },
    {
        "context": "Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don't know it instantly. It's only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass.\n\nSo how do we debug those errors? The answer is easy: we don't. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it.\n\nTo do this in our case, we just have to put the model back on the CPU and call it on our batch -- the batch returned by the `DataLoader` has not been moved to the GPU yet:\n\n```python\noutputs = trainer.model.cpu()(**batch)\n```\n\n```python out\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n   2386         )\n   2387     if dim == 2:\n-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n   2389     elif dim == 4:\n   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n\nIndexError: Target 2 is out of bounds.\n```\n\nSo, the picture is getting clearer. Instead of having a CUDA error, we now have an `IndexError` in the loss computation (so nothing to do with the backward pass, as we said earlier). More precisely, we can see that it's target 2 that creates the error, so this is a very good moment to check the number of labels of our model:\n\n```python\ntrainer.model.config.num_labels\n```",
        "question": "What is the error in the context?\n",
        "answer": "The error in the context is an `IndexError` in the loss computation.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter8/4.mdx"
    },
    {
        "context": "<small> SimMIM framework. Taken from the <a href=\"https://arxiv.org/abs/2111.09886\">original paper</a>. </small>\n\nThe goal for the model is to predict raw pixel values for the masked patches, using just a linear layer as prediction head. The model is trained using a simple L1 loss.\n\n### Using datasets from 🤗 datasets\n\nHere we show how to pre-train a `ViT` from scratch for masked image modeling on the [cifar10](https://huggingface.co/datasets/cifar10) dataset.\n\nAlternatively, one can decide to further pre-train an already pre-trained (or fine-tuned) checkpoint from the [hub](https://huggingface.co/). This can be done by setting the `model_name_or_path` argument to \"google/vit-base-patch16-224-in21k\" for example (and not specifying the `model_type` argument).\n\n```bash\n!python run_mim.py \\\n    --model_type vit \\\n    --output_dir ./outputs/ \\\n    --overwrite_output_dir \\\n    --remove_unused_columns False \\\n    --label_names bool_masked_pos \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 2e-5 \\\n    --weight_decay 0.05 \\\n    --num_train_epochs 100 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 8 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --evaluation_strategy epoch \\\n    --save_strategy epoch \\\n    --load_best_model_at_end True \\\n    --save_total_limit 3 \\\n    --seed 1337\n```\n\nHere, we train for 100 epochs with a learning rate of 2e-5. Note that the SimMIM authors used a more sophisticated learning rate schedule, see the [config files](https://github.com/microsoft/SimMIM/blob/main/configs/vit_base__800ep/simmim_pretrain__vit_base__img224__800ep.yaml) for more info. One can easily tweak the script to include this learning rate schedule (several learning rate schedulers are supported via the [training arguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)).",
        "question": "What is the goal of the model in the SimMIM framework?\n",
        "answer": "The goal of the model in the SimMIM framework is to predict raw pixel values for the masked patches, using just a linear layer as prediction head. The model is trained using a simple L1 loss.",
        "source_doc": "huggingface/transformers/blob/main/examples/pytorch/image-pretraining/README.md"
    },
    {
        "context": "```python\n>>> from transformers import AutoTokenizer\n>>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")  # doctest: +SKIP\n>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")  # doctest: +SKIP\n\n>>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")  # doctest: +SKIP\n>>> outputs = model(**inputs)  # doctest: +SKIP\n```\n\nPrinting the outputs would give that:\n\n```bash\nQuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.7652, -1.0452, -7.0409, -4.6864, -4.0277, -6.2021, -4.9473,  2.6287,\n          7.6111, -1.2488, -2.0551, -0.9350,  4.9758, -0.7707,  2.1493, -2.0703,\n         -4.3232, -4.9472]]), end_logits=tensor([[ 0.4382, -1.6502, -6.3654, -6.0661, -4.1482, -3.5779, -0.0774, -3.6168,\n         -1.8750, -2.8910,  6.2582,  0.5425, -3.7699,  3.8232, -1.5073,  6.2311,\n          3.3604, -0.0772]]), hidden_states=None, attentions=None)\n```\n\nAs you can see, converting a model to ONNX does not mean leaving the Hugging Face ecosystem. You end up with a similar API as regular 🤗 Transformers models!\n\n<Tip>\n\nIt is also possible to export the model to ONNX directly from the `ORTModelForQuestionAnswering` class by doing the following:\n\n```python\n>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\", export=True)\n```\n\nFor more information, check the `optimum.onnxruntime` documentation [page on this topic](/onnxruntime/overview).\n\n</Tip>\n\nThe process is identical for TensorFlow checkpoints on the Hub. For example, we can export a pure TensorFlow checkpoint from the [Keras\norganization](https://huggingface.co/keras-io) as follows:\n\n```bash\noptimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/\n```\n\n### Exporting a model to be used with Optimum's ORTModel",
        "question": "What is the name of the model exported to ONNX?\n",
        "answer": "The name of the model exported to ONNX is distilbert_base_cased_squad_onnx.",
        "source_doc": "huggingface/optimum/blob/main/docs/source/exporters/onnx/usage_guides/export_a_model.mdx"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UL2\n\n## Overview\n\nThe T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:",
        "question": "Where can I find the UL2 model?\n",
        "answer": "The UL2 model is available in the Hugging Face Transformers library.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/ul2.md"
    },
    {
        "context": "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={9}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nLet's test what you learned in this chapter!\n\n### 1. What can you use Gradio to do?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Create a demo for your machine learning model\",\n\t\t\texplain: \"With a few lines of python code you can generate a demo for your ML model using our library of pre-built components.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Share your machine learning model with others\",\n\t\t\texplain: \"Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone.\",\n            correct: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Debug your model\",\n\t\t\texplain: \"One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Train your model\",\n\t\t\texplain: \"Gradio is designed to be used for model inference, AFTER your model is trained.\",\n\t\t}\n\t]}\n/>\n\n### 2. Gradio ONLY works with PyTorch models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\n\t\t\tcorrect: true\n        }\n\t]}\n/>\n\n### 3. Where can you launch a Gradio demo from?",
        "question": "Where can I launch a Gradio demo from?\n",
        "answer": "You can launch a Gradio demo from your local machine or from a remote server.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx"
    },
    {
        "context": "Batch Size: 1280\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/dpn.py#L278\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dpn68b_ra-a31ca160.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.21%\n      Top 5 Accuracy: 94.42%\n- Name: dpn92\n  In Collection: DPN\n  Metadata:\n    FLOPs: 8357659624\n    Parameters: 37670000\n    File Size: 151248422\n    Architecture:\n    - Batch Normalization\n    - Convolution\n    - DPN Block\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 40x K80 GPUs\n    ID: dpn92\n    LR: 0.316\n    Layers: 92\n    Crop Pct: '0.875'\n    Batch Size: 1280\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/dpn.py#L286\n  Weights: https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn92_extra-b040e4a9b.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.99%\n      Top 5 Accuracy: 94.84%\n- Name: dpn98\n  In Collection: DPN\n  Metadata:\n    FLOPs: 15003675112\n    Parameters: 61570000\n    File Size: 247021307\n    Architecture:\n    - Batch Normalization\n    - Convolution\n    - DPN Block\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 40x K80 GPUs\n    ID: dpn98\n    LR: 0.4\n    Layers: 98\n    Crop Pct: '0.875'\n    Batch Size: 1280\n    Image Size: '224'",
        "question": "What is the top 1 accuracy of dpn92?\n",
        "answer": "The top 1 accuracy of dpn92 is 79.99%.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/dpn.mdx"
    },
    {
        "context": "**[Write With Transformer](https://transformer.huggingface.co)**，由 Hugging Face 團隊所打造，是一個文本生成的官方 demo。\n\n## 如果你在尋找由 Hugging Face 團隊所提供的客製化支援服務\n\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a><br>\n\n## 快速上手\n\n我們為快速使用模型提供了 `pipeline` API。 Pipeline 包含了預訓練模型和對應的文本預處理。下面是一個快速使用 pipeline 去判斷正負面情緒的例子：\n\n```python\n>>> from transformers import pipeline\n\n# 使用情緒分析 pipeline\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\n第二行程式碼下載並快取 pipeline 使用的預訓練模型，而第三行程式碼則在給定的文本上進行了評估。這裡的答案“正面” (positive) 具有 99.97% 的信賴度。\n\n許多的 NLP 任務都有隨選即用的預訓練 `pipeline`。例如，我們可以輕鬆地從給定文本中擷取問題答案：\n\n``` python\n>>> from transformers import pipeline\n\n# 使用問答 pipeline\n>>> question_answerer = pipeline('question-answering')\n>>> question_answerer({\n...     'question': 'What is the name of the repository ?',\n...     'context': 'Pipeline has been included in the huggingface/transformers repository'\n... })\n{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}\n\n```\n\n除了提供問題解答，預訓練模型還提供了對應的信賴度分數以及解答在 tokenized 後的文本中開始和結束的位置。你可以從[這個教學](https://huggingface.co/docs/transformers/task_summary)了解更多 `pipeline` API支援的任務。\n\n要在你的任務中下載和使用任何預訓練模型很簡單，只需三行程式碼。這裡是 PyTorch 版的範例：\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\n這裡是對應的 TensorFlow 程式碼：\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel",
        "question": "What is the name of the repository that includes Pipeline?\n",
        "answer": "huggingface/transformers",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "使用Gradio JavaScript客户端快速入门\n\nTags: CLIENT, API, SPACES\n\nGradio JavaScript客户端使得使用任何Gradio应用作为API非常简单。例如，考虑一下这个[从麦克风录音的Hugging Face Space，用于转录音频文件](https://huggingface.co/spaces/abidlabs/whisper)。\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\n使用`@gradio/client`库，我们可以轻松地以编程方式使用Gradio作为API来转录音频文件。\n\n以下是完成此操作的完整代码：\n\n```js\nimport { client } from \"@gradio/client\";\n\nconst response = await fetch(\n\t\"https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted_speakers/SalmanKhan/sample-1.wav\"\n);\nconst audio_file = await response.blob();\n\nconst app = await client(\"abidlabs/whisper\");\nconst transcription = await app.predict(\"/predict\", [audio_file]);\n\nconsole.log(transcription.data);\n// [ \"I said the same phrase 30 times.\" ]\n```\n\nGradio客户端适用于任何托管的Gradio应用，无论是图像生成器、文本摘要生成器、有状态的聊天机器人、税收计算器还是其他任何应用！Gradio客户端通常与托管在[Hugging Face Spaces](https://hf.space)上的应用一起使用，但您的应用可以托管在任何地方，比如您自己的服务器。\n\n**先决条件**：要使用Gradio客户端，您不需要深入了解`gradio`库的细节。但是，熟悉Gradio的输入和输出组件的概念会有所帮助。\n\n## 安装\n\n可以使用您选择的软件包管理器从npm注册表安装轻量级的`@gradio/client`包，并支持18及以上的Node版本：\n\n```bash\nnpm i @gradio/client\n```\n\n## 连接到正在运行的Gradio应用\n\n首先，通过实例化`client`对象并将其连接到在Hugging Face Spaces或任何其他位置运行的Gradio应用来建立连接。\n\n## 连接到Hugging Face Space\n\n```js\nimport { client } from \"@gradio/client\";\n\nconst app = client(\"abidlabs/en2fr\"); // 一个从英语翻译为法语的 Space\n```\n\n您还可以通过在options参数的`hf_token`属性中传入您的HF token来连接到私有Spaces。您可以在此处获取您的HF token：https://huggingface.co/settings/tokens\n\n```js\nimport { client } from \"@gradio/client\";\n\nconst app = client(\"abidlabs/my-private-space\", { hf_token=\"hf_...\" })\n```\n\n## 为私人使用复制一个Space\n\n虽然您可以将任何公共Space用作API，但是如果您发出的请求过多，Hugging Face可能会对您进行速率限制。为了无限制使用Space，只需复制Space以创建私有Space，然后使用它来进行任意数量的请求！\n\n`@gradio/client`还导出了另一个函数`duplicate`，以使此过程变得简单（您将需要传入您的[Hugging Face token](https://huggingface.co/settings/tokens)）。\n\n`duplicate`与`client`几乎相同，唯一的区别在于底层实现：\n\n```js\nimport { client } from \"@gradio/client\";",
        "question": "What is the function used to connect to a Hugging Face Space using the Gradio JavaScript client?\n",
        "answer": "The function used to connect to a Hugging Face Space using the Gradio JavaScript client is `client`.",
        "source_doc": "gradio-app/gradio/blob/main/guides/cn/06_client-libraries/02_getting-started-with-the-js-client.md"
    },
    {
        "context": "## Vision\n\nImage datasets are loaded just like text datasets. However, instead of a tokenizer, you'll need a [feature extractor](https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor) to preprocess the dataset. Applying data augmentation to an image is common in computer vision to make the model more robust against overfitting. You're free to use any data augmentation library you want, and then you can apply the augmentations with 🤗 Datasets. In this quickstart, you'll load the [Beans](https://huggingface.co/datasets/beans) dataset and get it ready for the model to train on and identify disease from the leaf images.\n\n**1**. Load the Beans dataset by providing the [`load_dataset`] function with the dataset name and a dataset split:\n\n```py\n>>> from datasets import load_dataset, Image\n\n>>> dataset = load_dataset(\"beans\", split=\"train\")\n```\n\n**2**. Now you can add some data augmentations with any library ([Albumentations](https://albumentations.ai/), [imgaug](https://imgaug.readthedocs.io/en/latest/), [Kornia](https://kornia.readthedocs.io/en/latest/)) you like. Here, you'll use [torchvision](https://pytorch.org/vision/stable/transforms.html) to randomly change the color properties of an image:\n\n```py\n>>> from torchvision.transforms import Compose, ColorJitter, ToTensor\n\n>>> jitter = Compose(\n...     [ColorJitter(brightness=0.5, hue=0.5), ToTensor()]\n... )\n```\n\n**3**. Create a function to apply your transform to the dataset and generate the model input: `pixel_values`.\n\n```python\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n...     return examples\n```\n\n**4**. Use the [`~Dataset.with_transform`] function to apply the data augmentations on-the-fly:\n\n```py\n>>> dataset = dataset.with_transform(transforms)\n```\n\n**5**. Set the dataset format according to the machine learning framework you're using.",
        "question": "What library is used to randomly change the color properties of an image?\n",
        "answer": "The library used to randomly change the color properties of an image is torchvision.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/quickstart.mdx"
    },
    {
        "context": "## Usage tips\n\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\n- NAT can be used as a *backbone*. When `output_hidden_states = True`,\nit will output both `hidden_states` and `reshaped_hidden_states`.\nThe `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than\n`(batch_size, height, width, num_channels)`.\n\nNotes:\n- NAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention.\nYou can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),\nor build on your system by running `pip install natten`.\nNote that the latter will likely take time to compile. NATTEN does not support Windows devices yet.\n- Patch size of 4 is only supported at the moment.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with NAT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`NatForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## NatConfig\n\n[[autodoc]] NatConfig\n\n## NatModel\n\n[[autodoc]] NatModel\n    - forward\n\n## NatForImageClassification\n\n[[autodoc]] NatForImageClassification\n    - forward",
        "question": "What is the shape of reshaped_hidden_states when using NAT?\n",
        "answer": "The shape of reshaped_hidden_states when using NAT is (batch, num\\_channels, height, width).",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/nat.md"
    },
    {
        "context": "* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [through code](https://huggingface.co/docs/huggingface_hub/how-to-model-cards) and [user interfaces](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool), including building more features directly into the repos and product. \n* As we further develop model tools such as [Evaluate on the Hub](https://huggingface.co/blog/eval-on-the-hub), we will integrate their usage within the model card development workflow. For example, as automatically evaluating model performance across disaggregated factors becomes easier, these results will be possible to import into the model card.\n* There is further study to be done to advance the pairing of research models and model cards, such as building out a research paper → to model documentation pipeline, making it  make it trivial to go from paper to model card creation. This would allow for greater cross-domain reach and further standardisation of model documentation.\n\nWe continue to learn more about how model cards are created and used, and the effect of cards on model usage. Based on these learnings, we will further update the model card template, instructions, and Hub integrations. \n\n\nAs we strive to incorporate more voices and stakeholders' use cases for model cards, [bookmark our model cards writing tool and give it a try](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool)!\n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/121_model-cards/like_the_space.gif\" width=\"680\"/>\n</p>\n\n\nWe are excited to know your thoughts on model cards, our model card writing GUI, and how AI documentation can empower your domain.🤗\n\n## Acknowledgements\n\nThis release would not have been possible without the extensive contributions of Omar Sanseviero, Lucain Pouget, Julien Chaumond, Nazneen Rajani, and Nate Raw.",
        "question": "Which feature is being built into the model card development workflow?\n",
        "answer": "Evaluate on the Hub is being built into the model card development workflow.",
        "source_doc": "huggingface/blog/blob/main/model-cards.md"
    },
    {
        "context": "## Further reading\n\nFor more information we recommend you check out the following resources:\n\n- [Google Research, 2022 & beyond: Robotics](https://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html)\n- [Pre-Trained Language Models for Interactive Decision-Making](https://arxiv.org/abs/2202.01771)\n- [Grounding Large Language Models with Online Reinforcement Learning](https://arxiv.org/abs/2302.02662v1)\n- [Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692)\n\n## Author\n\nThis section was written by <a href=\"https://twitter.com/ClementRomac\"> Clément Romac </a>",
        "question": "What is the title of the article written by Clément Romac?\n",
        "answer": "The title of the article written by Clément Romac is not mentioned in the context.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unitbonus3/language-models.mdx"
    },
    {
        "context": "After we have released the Release Candidate we need to create a PR at the [Deep Learning Container Repository](https://github.com/aws/deep-learning-containers).\n\n**Creating the update PR:**\n\n1. Update the two latest `buildspec.yaml` config for [PyTorch](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch) and [TensorFlow](https://github.com/aws/deep-learning-containers/tree/master/huggingface/tensorflow). The two latest `buildspec.yaml` are the `buildspec.yaml` without a version tag and the one with the highest framework version, e.g. `buildspec-1-7-1.yml` and not `buildspec-1-6.yml`.  \n\nTo update the `buildspec.yaml` we need to adjust either the `transformers_version` or the `datasets_version` or both. Example for upgrading to `transformers 4.5.0` and `datasets 1.6.0`.\n```yaml\naccount_id: &ACCOUNT_ID <set-$ACCOUNT_ID-in-environment>\nregion: &REGION <set-$REGION-in-environment>\nbase_framework: &BASE_FRAMEWORK pytorch\nframework: &FRAMEWORK !join [ \"huggingface_\", *BASE_FRAMEWORK]\nversion: &VERSION 1.6.0\nshort_version: &SHORT_VERSION 1.6\n\nrepository_info:\n  training_repository: &TRAINING_REPOSITORY\n    image_type: &TRAINING_IMAGE_TYPE training\n    root: !join [ \"huggingface/\", *BASE_FRAMEWORK, \"/\", *TRAINING_IMAGE_TYPE ]\n    repository_name: &REPOSITORY_NAME !join [\"pr\", \"-\", \"huggingface\", \"-\", *BASE_FRAMEWORK, \"-\", *TRAINING_IMAGE_TYPE]\n    repository: &REPOSITORY !join [ *ACCOUNT_ID, .dkr.ecr., *REGION, .amazonaws.com/,\n      *REPOSITORY_NAME ]",
        "question": "What is the name of the repository for the training image type of TensorFlow?\n",
        "answer": "The name of the repository for the training image type of TensorFlow is `huggingface/tensorflow/training`.",
        "source_doc": "huggingface/transformers/blob/main/tests/sagemaker/README.md"
    },
    {
        "context": "## The Perceiver\n\nThe [Perceiver](https://arxiv.org/abs/2103.03206) aims to solve this limitation by employing the self-attention mechanism on a set of latent variables, rather than on the inputs. The `inputs` (which could be text, image, audio, video) are only used for doing cross-attention with the latents. This has the advantage that the bulk of compute happens in a latent space, where compute is cheap (one typically uses 256 or 512 latents). The resulting architecture has no quadratic dependence on the input size: the Transformer encoder only depends linearly on the input size, while latent attention is independent of it. In a follow-up paper, called [Perceiver IO](https://arxiv.org/abs/2107.14795), the authors extend this idea to let the Perceiver also handle arbitrary outputs. The idea is similar: one only uses the outputs for doing cross-attention with the latents. Note that I'll use the terms \"Perceiver\" and \"Perceiver IO\" interchangeably to refer to the Perceiver IO model throughout this blog post.\n\nIn the following section, we look in a bit more detail at how Perceiver IO actually works by going over its implementation in [HuggingFace Transformers](https://github.com/huggingface/transformers), a popular library that initially implemented Transformer-based models for NLP, but is now starting to implement them for other domains as well. In the sections below, we explain in detail  - in terms of shapes of tensors - how the Perceiver actually pre and post processes modalities of any kind.\n\nAll Perceiver variants in HuggingFace Transformers are based on the `PerceiverModel` class. To initialize a `PerceiverModel`, one can provide 3 additional instances to the model:\n- a preprocessor\n- a decoder\n- a postprocessor.",
        "question": "What is the Perceiver in the context of the passage?\n",
        "answer": "The Perceiver is a model that uses self-attention on a set of latent variables, rather than on the inputs. The inputs are only used for doing cross-attention with the latents. This has the advantage that the bulk of compute happens in a latent space, where compute is cheap. The resulting architecture has no quadratic dependence on the input size.",
        "source_doc": "huggingface/blog/blob/main/perceiver.md"
    },
    {
        "context": "- Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - FixRes\n    - Label Smoothing\n    - Noisy Student\n    - RMSProp\n    - RandAugment\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resources: Cloud TPU v3 Pod\n    ID: tf_efficientnet_b1_ns\n    LR: 0.128\n    Epochs: 700\n    Dropout: 0.5\n    Crop Pct: '0.882'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '240'\n    Weight Decay: 1.0e-05\n    Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n    Stochastic Depth Survival: 0.8\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1437\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 81.39%\n      Top 5 Accuracy: 95.74%\n- Name: tf_efficientnet_b2_ns\n  In Collection: Noisy Student\n  Metadata:\n    FLOPs: 1234321170\n    Parameters: 9110000\n    File Size: 36801803\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - FixRes\n    - Label Smoothing\n    - Noisy Student\n    - RMSProp\n    - RandAugment\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resources: Cloud TPU v3 Pod\n    ID: tf_efficientnet_b2_ns\n    LR: 0.128\n    Epochs: 700\n    Dropout: 0.5\n    Crop Pct: '0.89'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '260'",
        "question": "What is the FLOPs of tf_efficientnet_b2_ns?\n",
        "answer": "1234321170",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/noisy-student.mdx"
    },
    {
        "context": "FrameworkSwitchCourse {fw} />\n\n# Behind the pipeline[[behind-the-pipeline]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner chapter={2}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb\"},\n]} />\n\n{:else}\n\n<CourseFloatingBanner chapter={2}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb\"},\n]} />\n\n{/if}\n\n<Tip>\nThis is the first section where the content is slightly different depending on whether you use PyTorch or TensorFlow. Toggle the switch on top of the title to select the platform you prefer!\n</Tip>\n\n{#if fw === 'pt'}\n<Youtube id=\"1pedAIvTWXk\"/>\n{:else}\n<Youtube id=\"wVN12smEvqg\"/>\n{/if}\n\nLet's start with a complete example, taking a look at what happened behind the scenes when we executed the following code in [Chapter 1](/course/chapter1):\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\n    [\n        \"I've been waiting for a HuggingFace course my whole life.\",\n        \"I hate this so much!\",\n    ]\n)\n```\n\nand obtained:\n\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n {'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n```\n\nAs we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:",
        "question": "Is it possible to create custom pipelines in the transformers library?\n",
        "answer": "Yes, it is possible to create custom pipelines in the transformers library.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/2.mdx"
    },
    {
        "context": "## Training\n\nOur training examples use two test conditioning images. They can be downloaded by running\n\n```sh\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be able to push the trained T2IAdapter parameters to Hugging Face Hub.\n\n```bash\nexport MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_t2i_adapter_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n --resolution=1024 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42 \\\n --push_to_hub\n```\n\nTo better track our training experiments, we're using the following flags in the command above:\n\n* `report_to=\"wandb` will ensure the training runs are tracked on Weights and Biases. To use it, be sure to install `wandb` with `pip install wandb`.\n* `validation_image`, `validation_prompt`, and `validation_steps` to allow the script to do a few validation inference runs. This allows us to qualitatively check if the training is progressing as expected.\n\nOur experiments were conducted on a single 40GB A100 GPU.\n\n### Inference\n\nOnce training is done, we can perform inference like so:\n\n```python\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch",
        "question": "What is the name of the dataset used for training?\n",
        "answer": "The name of the dataset used for training is fusing/fill50k.",
        "source_doc": "huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md"
    },
    {
        "context": "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n>>> print(encoded_input)\n{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n```\n</pt>\n<tf>\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n>>> print(encoded_input)\n{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>,\n 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=",
        "question": "What is the length of the first sentence in the batch?\n",
        "answer": "The length of the first sentence in the batch is 9.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/preprocessing.md"
    },
    {
        "context": "#### Remove it later\n```python hl_lines=\"3 6\"\nimport torch\nimport timm\nm = timm.create_model('densenet121', pretrained=True)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Original shape: {o.shape}')\nm.reset_classifier(0, '')\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Unpooled shape: {o.shape}')\n```\nOutput:\n```text\nOriginal shape: torch.Size([2, 1000])\nUnpooled shape: torch.Size([2, 1024, 7, 7])\n```\n\n### Pooled\n\nTo modify the network to return pooled features, one can use `forward_features()` and pool/flatten the result themselves, or modify the network like above but keep pooling intact. \n\n#### Create with no classifier\n```python hl_lines=\"3\"\nimport torch\nimport timm\nm = timm.create_model('resnet50', pretrained=True, num_classes=0)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Pooled shape: {o.shape}')\n```\nOutput:\n```text\nPooled shape: torch.Size([2, 2048])\n```\n\n#### Remove it later\n```python hl_lines=\"3 6\"\nimport torch\nimport timm\nm = timm.create_model('ese_vovnet19b_dw', pretrained=True)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Original shape: {o.shape}')\nm.reset_classifier(0)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Pooled shape: {o.shape}')\n```\nOutput:\n```text\nOriginal shape: torch.Size([2, 1000])\nPooled shape: torch.Size([2, 1024])\n```\n\n\n## Multi-scale Feature Maps (Feature Pyramid)\n\nObject detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library.\n\n`timm` allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels.",
        "question": "How to create a feature backbone that outputs feature maps for selected levels using timm?\n",
        "answer": "You can create a feature backbone that outputs feature maps for selected levels using timm by modifying the original classification network to support object detection, segmentation, keypoint, and a variety of dense pixel tasks. The timm library supports a consistent interface for creating any of the included models as feature backbones.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md"
    },
    {
        "context": "```python\nfrom time import perf_counter\nimport numpy as np\n\ndef measure_latency(pipe):\n    latencies = []\n    # warm up\n    for _ in range(10):\n        _ = pipe(question=question, context=context)\n    # Timed run\n    for _ in range(100):\n        start_time = perf_counter()\n        _ =  pipe(question=question, context=context)\n        latency = perf_counter() - start_time\n        latencies.append(latency)\n    # Compute run statistics\n    time_avg_ms = 1000 * np.mean(latencies)\n    time_std_ms = 1000 * np.std(latencies)\n    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n\nprint(f\"Vanilla model {measure_latency(optimum_qa)}\")\nprint(f\"Optimized & Quantized model {measure_latency(quantized_optimum_qa)}\")\n\n# Vanilla model Average latency (ms) - 117.61 +\\- 8.48\n# Optimized & Quantized model Average latency (ms) - 64.94 +\\- 3.65\n```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/66_optimum_inference/results.png\" alt=\"Latency & F1 results\"/>\n</figure>\n\nWe managed to accelerate our model latency from `117.61ms` to `64.94ms` or roughly 2x while keeping `99.61%` of the accuracy. Something we should keep in mind is that we used a mid-performant CPU instance with 2 physical cores. By switching to GPU or a more performant CPU instance, e.g. [ice-lake powered you can decrease the latency number down to a few milliseconds.](https://huggingface.co/blog/bert-cpu-scaling-part-2#more-efficient-ai-processing-on-latest-intel-ice-lake-cpus)\n\n## 4. Current Limitations\n\nWe just started supporting inference in [https://github.com/huggingface/optimum](https://github.com/huggingface/optimum) so we would like to share current limitations as well. All of those limitations are on the roadmap and will be resolved in the near future.",
        "question": "What is the current latency of the optimized and quantized model?\n",
        "answer": "The optimized and quantized model has a latency of 64.94 ms.",
        "source_doc": "huggingface/blog/blob/main/optimum-inference.md"
    },
    {
        "context": "--\ntitle: \"Probabilistic Time Series Forecasting with 🤗 Transformers\"\nthumbnail: /blog/assets/118_time-series-transformers/thumbnail.png\nauthors:\n- user: nielsr\n- user: kashif\n---\n\n# Probabilistic Time Series Forecasting with 🤗 Transformers\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n## Introduction\n\nTime series forecasting is an essential scientific and business problem and as such has also seen a lot of innovation recently with the use of [deep learning based](https://dl.acm.org/doi/abs/10.1145/3533382) models in addition to the [classical methods](https://otexts.com/fpp3/). An important difference between classical methods like ARIMA and novel deep learning methods is the following.\n\n##  Probabilistic Forecasting\n\nTypically, classical methods are fitted on each time series in a dataset individually. These are often referred to as  \"single\" or \"local\" methods. However, when dealing with a large amount of time series for some applications, it is beneficial to train a \"global\" model on all available time series, which enables the model to learn latent representations from many different sources.\n\nSome classical methods are point-valued (meaning, they just output a single value per time step) and models are trained by minimizing an L2 or L1 type of loss with respect to the ground truth data. However, since forecasts are often used in some real-world decision making pipeline, even with humans in the loop, it is much more beneficial to provide the uncertainties of predictions. This is also called \"probabilistic forecasting\", as opposed to \"point forecasting\". This entails modeling a probabilistic distribution, from which one can sample.",
        "question": "What is the difference between classical time series forecasting methods and deep learning methods?\n",
        "answer": "Classical time series forecasting methods are typically fitted on each time series in a dataset individually, while deep learning methods can be trained on all available time series, enabling them to learn latent representations from many different sources. Additionally, classical methods are often point-valued, while deep learning methods can model a probabilistic distribution, providing the uncertainties of predictions.",
        "source_doc": "huggingface/blog/blob/main/time-series-transformers.md"
    },
    {
        "context": "Pretrained models in 🧨 Diffusers are easily created from their model class with the parameters you want. For example, to create a [`UNet2DModel`]:\n\n```py\n>>> from diffusers import UNet2DModel\n\n>>> model = UNet2DModel(\n...     sample_size=config.image_size,  # the target image resolution\n...     in_channels=3,  # the number of input channels, 3 for RGB images\n...     out_channels=3,  # the number of output channels\n...     layers_per_block=2,  # how many ResNet layers to use per UNet block\n...     block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n...     down_block_types=(\n...         \"DownBlock2D\",  # a regular ResNet downsampling block\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n...         \"DownBlock2D\",\n...     ),\n...     up_block_types=(\n...         \"UpBlock2D\",  # a regular ResNet upsampling block\n...         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...     ),\n... )\n```\n\nIt is often a good idea to quickly check the sample image shape matches the model output shape:\n\n```py\n>>> sample_image = dataset[0][\"images\"].unsqueeze(0)\n>>> print(\"Input shape:\", sample_image.shape)\nInput shape: torch.Size([1, 3, 128, 128])\n\n>>> print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)\nOutput shape: torch.Size([1, 3, 128, 128])\n```\n\nGreat! Next, you'll need a scheduler to add some noise to the image.\n\n## Create a scheduler",
        "question": "What is the name of the scheduler class in the context?\n",
        "answer": "The name of the scheduler class in the context is not explicitly mentioned. However, the context suggests using a scheduler to add noise to an image, and the class for creating a pretrained model is `UNet2DModel`.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md"
    },
    {
        "context": "Instead of throwing more cores to the task as you would do in the core count scaling setup, now we will be using more model instances.\nEach instance will run independently on its own subset of the hardware resources in a truly parallel fashion on a subset of the CPU cores. \n\n### 7.1. How-to allocate multiple independent instances\n\nLet's start simple, if we want to spawn 2 instances, one on each socket with 24 cores assigned:\n```shell\nnumactl -C 0-23 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=24\nnumactl -C 24-47 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=24\n```\n\nStarting from here, each instance does not share any resource with the other, and everything is operating at maximum efficiency from a \nhardware perspective.  \nThe latency measurements are identical to what a single instance would achieve, but throughput is actually 2x higher\nas the two instances operate in a truly parallel way.\n\nWe can further increase the number of instances, lowering the number of cores assigned for each instance.  \nLet's run 4 independent instances, each of them effectively bound to 12 CPU cores.\n```shell\nnumactl -C 0-11 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12\nnumactl -C 12-23 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12\nnumactl -C 24-35 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12\nnumactl -C 36-47 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12\n```",
        "question": "How many independent instances are running with 12 CPU cores assigned to each?\n",
        "answer": "4 independent instances are running with 12 CPU cores assigned to each.",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-1.md"
    },
    {
        "context": "Metric Card for CUAD\n\n## Metric description\n\nThis metric wraps the official scoring script for version 1 of the [Contract Understanding Atticus Dataset (CUAD)](https://huggingface.co/datasets/cuad), which is a corpus of more than 13,000 labels in 510 commercial legal contracts that have been manually labeled to identify 41 categories of important clauses that lawyers look for when reviewing contracts in connection with corporate transactions.\n\nThe CUAD metric computes several scores: [Exact Match](https://huggingface.co/metrics/exact_match), [F1 score](https://huggingface.co/metrics/f1), Area Under the Precision-Recall Curve, [Precision](https://huggingface.co/metrics/precision) at 80% [recall](https://huggingface.co/metrics/recall) and Precision at 90% recall.\n\n## How to use \n\nThe CUAD metric takes two inputs :\n\n\n`predictions`, a list of question-answer dictionaries with the following key-values:\n- `id`: the id of the question-answer pair as given in the references.\n- `prediction_text`: a list of possible texts for the answer, as a list of strings depending on a threshold on the confidence probability of each prediction.\n\n\n`references`: a list of question-answer dictionaries with the following key-values:\n - `id`: the id of the question-answer pair (the same as above).\n - `answers`: a dictionary *in the CUAD dataset format* with the following keys:\n   - `text`: a list of possible texts for the answer, as a list of strings.\n   - `answer_start`: a list of start positions for the answer, as a list of ints.\n\n Note that `answer_start` values are not taken into account to compute the metric.",
        "question": "What is the CUAD metric?\n",
        "answer": "The CUAD metric is a metric that computes several scores for the Contract Understanding Atticus Dataset (CUAD), which is a corpus of more than 13,000 labels in 510 commercial legal contracts that have been manually labeled to identify 41 categories of important clauses that lawyers look for when reviewing contracts in connection with corporate transactions. The CUAD metric computes Exact Match, F1 score, Area Under the Precision-Recall Curve, Precision at 80% recall and Precision at 90% recall.",
        "source_doc": "huggingface/datasets/blob/main/metrics/cuad/README.md"
    },
    {
        "context": "如果第一个作业已开始处理，则它将不会被取消。如果第二个作业尚未开始，则它将成功取消并从队列中删除。\n\n## 生成器端点 （Generator Endpoints）\n\n某些Gradio API端点不返回单个值，而是返回一系列值。你可以随时从这样的生成器端点获取返回的一系列值，方法是运行`job.outputs()`：\n\n```py\nfrom gradio_client import Client\n\nclient = Client(src=\"gradio/count_generator\")\njob = client.submit(3, api_name=\"/count\")\nwhile not job.done():\n    time.sleep(0.1)\njob.outputs()\n\n>> ['0', '1', '2']\n```\n\n请注意，在生成器端点上运行`job.result()`只会获得端点返回的*第一个*值。\n\n`Job`对象还是可迭代的，这意味着您可以使用它按照从端点返回的结果逐个显示生成器函数的结果。以下是使用`Job`作为生成器的等效示例：\n\n```py\nfrom gradio_client import Client\n\nclient = Client(src=\"gradio/count_generator\")\njob = client.submit(3, api_name=\"/count\")\n\nfor o in job:\n    print(o)\n\n>> 0\n>> 1\n>> 2\n```\n\n你还可以取消具有迭代输出的作业，在这种情况下，作业将在当前迭代完成运行后完成。\n\n```py\nfrom gradio_client import Client\nimport time\n\nclient = Client(\"abidlabs/test-yield\")\njob = client.submit(\"abcdef\")\ntime.sleep(3)\njob.cancel()  # 作业在运行 2 个迭代后取消\n```",
        "question": "How does the gradio_client handle generator endpoints?\n",
        "answer": "The gradio_client handles generator endpoints by allowing users to retrieve the returned series of values using `job.outputs()`. The `Job` object is also iterable, which means users can iterate over the returned results one by one using a for loop. Users can also cancel a job with iterative outputs, which will complete the current iteration before cancelling.",
        "source_doc": "gradio-app/gradio/blob/main/guides/cn/06_client-libraries/01_getting-started-with-the-python-client.md"
    },
    {
        "context": "<details>\n  <summary>Examples for older versions of 🤗 Transformers</summary>\n\t<ul>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.21.0/examples\">v4.21.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.20.1/examples\">v4.20.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.19.4/examples\">v4.19.4</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.18.0/examples\">v4.18.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.17.0/examples\">v4.17.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.16.2/examples\">v4.16.2</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.15.0/examples\">v4.15.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.14.1/examples\">v4.14.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.13.0/examples\">v4.13.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.12.5/examples\">v4.12.5</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.11.3/examples\">v4.11.3</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.10.3/examples\">v4.10.3</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.9.2/examples\">v4.9.2</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.8.2/examples\">v4.8.2</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.7.0/examples\">v4.7.0</a></li>\n\t    <li><a href=\"https://github.com/huggingface/transformers/tree/v4.6.1/examples\">v4.6.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.5.1/examples\">v4.5.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.4.2/examples\">v4.4.2</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.3.3/examples\">v4.3.3</a></li>",
        "question": "What is the link to the examples for version 4.12.5 of 🤗 Transformers?\n",
        "answer": "https://github.com/huggingface/transformers/tree/v4.12.5/examples",
        "source_doc": "huggingface/transformers/blob/main/examples/README.md"
    },
    {
        "context": "</Tip>\n\n```py\n# Pause and resume endpoint\n>>> endpoint.pause()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='paused', url=None)\n>>> endpoint.resume()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n>>> endpoint.wait().client.text_generation(...)\n...\n\n# Scale to zero\n>>> endpoint.scale_to_zero()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='scaledToZero', url='https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud')\n# Endpoint is not 'running' but still has a URL and will restart on first call.\n```\n\n### Update model or hardware requirements\n\nIn some cases, you might also want to update your Inference Endpoint without creating a new one. You can either update the hosted model or the hardware requirements to run the model. You can do this using [`~InferenceEndpoint.update`]:\n\n```py\n# Change target model\n>>> endpoint.update(repository=\"gpt2-large\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update number of replicas\n>>> endpoint.update(min_replica=2, max_replica=6)\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update to larger instance\n>>> endpoint.update(accelerator=\"cpu\", instance_size=\"large\", instance_type=\"c6i\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n```\n\n### Delete the endpoint\n\nFinally if you won't use the Inference Endpoint anymore, you can simply call [`~InferenceEndpoint.delete()`].\n\n<Tip warning={true}>\n\nThis is a non-revertible action that will completely remove the endpoint, including its configuration, logs and usage metrics. You cannot restore a deleted Inference Endpoint.\n\n</Tip>\n\n\n## An end-to-end example",
        "question": "How do I update the model in an Inference Endpoint?\n",
        "answer": "You can update the model in an Inference Endpoint by calling the `update` method and specifying the new repository as an argument. For example, `endpoint.update(repository=\"gpt2-large\")` updates the endpoint to use the `gpt2-large` model.",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/inference_endpoints.md"
    },
    {
        "context": "Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/keras-image-classifier\nTags: VISION, MOBILENET, TENSORFLOW\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 — Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own.\n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## Step 2 — Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests",
        "question": "What is the name of the text file that contains the class names?\n",
        "answer": "The name of the text file that contains the class names is https://git.io/JJkYN.",
        "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-in-tensorflow.md"
    },
    {
        "context": "You can also use the [`~Dataset.set_transform`] function to decode formats not supported by [`Features`]. For example, the [`Audio`] feature uses [`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/) - a fast and simple library to install - but it does not provide support for less common audio formats. Here is where you can use [`~Dataset.set_transform`] to apply a custom decoding transform on the fly. You're free to use any library you like to decode the audio files.\n\nThe example below uses the [`pydub`](http://pydub.com/) package to open an audio format not supported by `soundfile`:\n\n```py\n>>> import numpy as np\n>>> from pydub import AudioSegment\n\n>>> audio_dataset_amr = Dataset.from_dict({\"audio\": [\"audio_samples/audio.amr\"]})\n\n>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):\n...     def pydub_decode_file(audio_path):\n...         sound = AudioSegment.from_file(audio_path)\n...         if sound.frame_rate != sampling_rate:\n...             sound = sound.set_frame_rate(sampling_rate)\n...         channel_sounds = sound.split_to_mono()\n...         samples = [s.get_array_of_samples() for s in channel_sounds]\n...         fp_arr = np.array(samples).T.astype(np.float32)\n...         fp_arr /= np.iinfo(samples[0].typecode).max\n...         return fp_arr\n...\n...     batch[\"audio\"] = [pydub_decode_file(audio_path) for audio_path in batch[\"audio\"]]\n...     return batch\n\n>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)\n```\n\n## Save\n\nOnce you are done processing your dataset, you can save and reuse it later with [`~Dataset.save_to_disk`].\n\nSave your dataset by providing the path to the directory you wish to save it to:\n\n```py\n>>> encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\")\n```\n\nUse the [`load_from_disk`] function to reload the dataset:\n\n```py\n>>> from datasets import load_from_disk\n>>> reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\")\n```\n\n<Tip>",
        "question": "What library is used to decode the audio format not supported by `soundfile`?\n",
        "answer": "The `pydub` library is used to decode the audio format not supported by `soundfile`.\n</Tip>",
        "source_doc": "huggingface/datasets/blob/main/docs/source/process.mdx"
    },
    {
        "context": "Let's do a small recap on what we learned in the first Unit:\n\n- Reinforcement Learning is a **computational approach to learning from actions**. We build an agent that learns from the environment by **interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.\n\n- The goal of any RL agent is to **maximize its expected cumulative reward** (also called expected return) because RL is based on the _reward hypothesis_, which is that all goals can be described as the maximization of an expected cumulative reward.\n\n- The RL process is a **loop that outputs a sequence of state, action, reward, and next state**.\n\n- To calculate the expected cumulative reward (expected return), **we discount the rewards**: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long-term future reward.\n\n- To solve an RL problem, you want to **find an optimal policy**; the policy is the \"brain\" of your AI that will tell us what action to take given a state. The optimal one is the one that gives you the actions that max the expected return.\n\nThere are **two** ways to find your optimal policy:\n\n- By **training your policy directly**: policy-based methods.\n- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n\n- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name \"deep.\"**\n\n# Let's train our first Deep Reinforcement Learning agent and upload it to the Hub 🚀\n\n## Get a certificate 🎓\n\nTo validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained model to the Hub and **get a result of >= 200**.",
        "question": "What is the goal of any RL agent?\n",
        "answer": "The goal of any RL agent is to maximize its expected cumulative reward.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx"
    },
    {
        "context": "# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\n\n## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('tf_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation",
        "question": "How many top categories are printed per image?\n",
        "answer": "5",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/tf-inception-v3.md"
    },
    {
        "context": "You need to define seven parameters:\n- `--model`: your trained model.\n- `--model_architecture`: name of the architecture of your model (DQN, PPO, A2C, SAC...).\n- `--env_id`: name of the environment.\n- `--eval_env`: environment used to evaluate the agent.\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. It’s `<your huggingface username>/<the repo name>`.\n- `--commit-message`.\n- `--filename`: the file you want to push to the Hub.\n\n2. `push_to_hub()`: simply push a file to the Hub\n\n```\npush_to_hub(\n    repo_id=\"ThomasSimonini/ppo-LunarLander-v2\",\n    filename=\"ppo-LunarLander-v2.zip\",\n    commit_message=\"Added LunarLander-v2 model trained with PPO\",\n)\n```\nYou need to define three parameters:\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. It’s `<your huggingface username>/<the repo name>`.\n- `--filename`: the file you want to push to the Hub.\n- `--commit-message`.\n\n\n## Additional resources\n\n* Hugging Face Stable-Baselines3 [documentation](https://github.com/huggingface/huggingface_sb3#hugging-face--x-stable-baselines3-v20)\n* Stable-Baselines3 [documentation](https://stable-baselines3.readthedocs.io/en/master/)",
        "question": "What is the name of the parameter that specifies the name of the Hugging Face repo you want to create or update?\n",
        "answer": "`--repo-id`",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/stable-baselines3.md"
    },
    {
        "context": "```py\n>>> ds['train'].set_transform(transforms)\n```\n\nYou can verify the transform works by visualizing the 10th example:\n\n```py\n>>> example = ds['train'][10]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         example['image'],\n...         box_convert(example['bbox'], 'xywh', 'xyxy'),\n...         colors='red',\n...         labels=[categories.int2str(x) for x in example['category']]\n...     )\n... )\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/visualize_detection_example_transformed_2.png\">\n</div>\n\n<Tip>\n\nNow that you know how to process a dataset for object detection, learn\n[how to train an object detection model](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb)\nand use it for inference.\n\n</Tip>",
        "question": "How to visualize the 10th example in the transformed dataset?\n",
        "answer": "You can visualize the 10th example in the transformed dataset by running the code:\n\n```py\nexample = ds['train'][10]\nto_pil_image(\n    draw_bounding_boxes(\n        example['image'],\n        box_convert(example['bbox'], 'xywh', 'xyxy'),\n        colors='red',\n        labels=[categories.int2str(x) for x in example['category']]\n    )\n)\n```",
        "source_doc": "huggingface/datasets/blob/main/docs/source/object_detection.mdx"
    },
    {
        "context": "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗\n\n*Acknowlegements: Thanks to Omar Sanseviero, Nate Raw, Niels Rogge, Alara Dirik, Amy Roberts, Maria Khalusova, and Lysandre Debut for their rigorous and timely reviews on the blog draft. Thanks to Chunte Lee for creating the blog thumbnail.*",
        "question": "Who created the blog thumbnail?\n",
        "answer": "Chunte Lee",
        "source_doc": "huggingface/blog/blob/main/cv_state.md"
    },
    {
        "context": "The dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \nat the end of the robust speech challenge.\n\nIf there is no real audio data for your language the final score will be \ncomputed solely based on the Common Voice 7 test dataset. If there is also\nno Common Voice 7 test dataset for your language, we will see together how to \nscore your model - if this is the case, please don't be discouraged. We are \nespecially excited about speech recognition systems of such low-resource \nlanguages and will make sure that we'll decide on a good approach to evaluating \nyour model.\n\n## Prizes\n\nTODO(Patrick, Omar, ...)\n\n## Communication and Problems\n\nIf you encounter any problems or have any questions, you should use one of the following platforms\ndepending on your type of problem. Hugging Face is an \"open-source-first\" organization meaning \nthat we'll try to solve all problems in the most public and most transparent way possible so that everybody\nin the community profits.\n\nThe following table summarizes what platform to use for which problem.",
        "question": "What is the name of the dataset that will be published at the end of the robust speech challenge?\n",
        "answer": "WER_REAL_AUDIO_TEST",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md"
    },
    {
        "context": "### Values from popular papers\nThe [original SuperGLUE paper](https://arxiv.org/pdf/1905.00537.pdf) reported average scores ranging from 47 to 71.5%, depending on the model used (with all evaluation values scaled by 100 to make computing the average possible). \n\nFor more recent model performance, see the [dataset leaderboard](https://super.gluebenchmark.com/leaderboard).\n\n## Examples \n\nMaximal values for the COPA subset (which outputs `accuracy`):\n\n```python\nfrom evaluate import load\nsuper_glue_metric = load('super_glue', 'copa')  # any of [\"copa\", \"rte\", \"wic\", \"wsc\", \"wsc.fixed\", \"boolq\", \"axg\"]\npredictions = [0, 1]\nreferences = [0, 1]\nresults = super_glue_metric.compute(predictions=predictions, references=references)\nprint(results)\n{'accuracy': 1.0}\n```\n\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\n\n```python\nfrom evaluate import load\nsuper_glue_metric = load('super_glue', 'multirc')\npredictions = [{'idx': {'answer': 0, 'paragraph': 0, 'question': 0}, 'prediction': 0}, {'idx': {'answer': 1, 'paragraph': 2, 'question': 3}, 'prediction': 1}]\nreferences = [1,0]\nresults = super_glue_metric.compute(predictions=predictions, references=references)\nprint(results)\n{'exact_match': 0.0, 'f1_m': 0.0, 'f1_a': 0.0}\n```\n\nPartial match for the COLA subset (which outputs `matthews_correlation`) \n\n```python\nfrom evaluate import load\nsuper_glue_metric = load('super_glue', 'axb')\nreferences = [0, 1]\npredictions = [1,1]\nresults = super_glue_metric.compute(predictions=predictions, references=references)\nprint(results)\n{'matthews_correlation': 0.0}\n```\n\n## Limitations and bias\nThis metric works only with datasets that have the same format as the [SuperGLUE dataset](https://huggingface.co/datasets/super_glue).",
        "question": "What is the range of average scores reported in the original SuperGLUE paper?\n",
        "answer": "The average scores reported in the original SuperGLUE paper range from 47 to 71.5%.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/super_glue/README.md"
    },
    {
        "context": "1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.",
        "question": "Which model was released by Allegro.pl and AGH University of Science and Technology?\n",
        "answer": "HerBERT",
        "source_doc": "huggingface/transformers/blob/main/README_pt-br.md"
    },
    {
        "context": "In this example we will use the vision model from [CLIP](https://huggingface.co/models?filter=clip)\nas the image encoder and [`roberta-base`](https://huggingface.co/roberta-base) as the text encoder.\nNote that one can also use the [ViT](https://huggingface.co/models?filter=vit) model as image encoder and any other BERT or ROBERTa model as text encoder.\nTo train the model on languages other than English one should choose a text encoder trained on the desired\nlanguage and a image-text dataset in that language. One such dataset is [WIT](https://github.com/google-research-datasets/wit).\t\n\nLet's start by creating a model repository to save the trained model and logs.\nHere we call the model `\"clip-roberta-base\"`, but you can change the model name as you like.\n\nYou can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n\n```\nhuggingface-cli repo create clip-roberta-base\n```\nNext we clone the model repository to add the tokenizer and model files.\n```\ngit clone https://huggingface.co/<your-username>/clip-roberta-base\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to \ntrack them. You can run the following command inside your model repo to do so.\n\n```\ncd clip-roberta-base\ngit lfs track \"*tfevents*\"\n```\n\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo.\n\nNext, let's add a symbolic link to the `run_hybrid_clip.py`.\n\n```bash\nexport MODEL_DIR=\"./clip-roberta-base\nln -s ~/transformers/examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py run_hybrid_clip.py\n```\n\n## How to use the `FlaxHybridCLIP` model:\n\nThe `FlaxHybridCLIP` class let's you load any text and vision encoder model to create a dual encoder. \nHere is an example of how to load the model using pre-trained text and vision models.\n\n```python\nfrom modeling_hybrid_clip import FlaxHybridCLIP",
        "question": "What is the name of the vision model used in the example?\n",
        "answer": "The vision model used in the example is from CLIP.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/hybrid_clip/README.md"
    },
    {
        "context": ". Perfect recall sounds great, but imagine if our generated summary had been “I really really really really loved reading the Hunger Games”. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we can also compute precision, which in the ROUGE context measures how much of the generated summary was relevant. In this example, the precision is 6/7. In practice, both precision and recall are usually computed and then the F1-score is reported. We can change the granularity of the comparison by comparing bigrams instead of unigrams. With bigrams we chunk the sentence into pairs of consecutive words and then count how many pairs in the generated summary are present in the reference one. This gives us ROUGE-2 precision and recall, which we can see is lower than the ROUGE-1 scores we saw earlier. Note that if the summaries are long, the ROUGE-2 score will be small as there are typically fewer bigrams to match. This is also true for abstractive summarization, so both ROUGE-1 and ROUGE-2 scores are usually reported. The last ROUGE variant we'll discuss is ROUGE-L. ROUGE-L doesn't compare n-grams, but instead treats each summary as a sequence of words and then looks for the longest common subsequence or LCS. A subsequence is a sequence that appears in the same relative order, but not necessarily contiguous. So in this example, \"I loved reading the Hunger Games\" is the longest common subsequence. The main advantage of ROUGE-L over ROUGE-1 or ROUGE-2 is that is doesn't depend on consecutive n-gram matches, so it tends to capture sentence structure more accurately",
        "question": "What is the main advantage of ROUGE-L over ROUGE-1 or ROUGE-2?\n",
        "answer": "The main advantage of ROUGE-L over ROUGE-1 or ROUGE-2 is that it doesn't depend on consecutive n-gram matches, so it tends to capture sentence structure more accurately.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter7/05b_rouge.md"
    },
    {
        "context": "# Instantiate tokenizer\ntokenizer = ByteLevelBPETokenizer()\n\ndef batch_iterator(batch_size=1000):\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i: i + batch_size][\"text\"]\n\n# Customized training\ntokenizer.train_from_iterator(batch_iterator(), vocab_size=50265, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n])\n\n# Save files to disk\ntokenizer.save(\"./tokenizer.json\")\n```\n\nThis creates and saves our tokenizer directly in the cloned repository.\nFinally, we can start training. For now, we'll simply use the official [`run_mlm_flax`](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py)\nscript, but we might make some changes later. So let's copy the script into our model repository.\n\n```bash\n$ cp ~/transformers/examples/flax/language-modeling/run_mlm_flax.py ./\n```\n\nThis way we are certain to have all the code used to train the model tracked in our repository.\nLet's start training by running:\n\n```bash\n./run_mlm_flax.py \\\n    --output_dir=\"./\" \\\n    --model_type=\"roberta\" \\\n    --config_name=\"./\" \\\n    --tokenizer_name=\"./\" \\\n    --dataset_name=\"oscar\" \\\n    --dataset_config_name=\"unshuffled_deduplicated_als\" \\\n    --max_seq_length=\"128\" \\\n    --per_device_train_batch_size=\"4\" \\\n    --per_device_eval_batch_size=\"4\" \\\n    --learning_rate=\"3e-4\" \\\n    --warmup_steps=\"1000\" \\\n    --overwrite_output_dir \\\n    --num_train_epochs=\"8\" \\\n    --push_to_hub\n```\n\nSince the dataset is tiny this command should actually run in less than 5 minutes. Note that we attach \nthe flag ``--push_to_hub`` so that both model weights and tensorboard traces are automatically uploaded to the hub.\nYou can see the tensorboard directly on the model page, under the [Training metrics tab](https://huggingface.co/flax-community/roberta-base-als/tensorboard).",
        "question": "What is the name of the tokenizer saved in the context?\n",
        "answer": "tokenizer.json",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/README.md"
    },
    {
        "context": "<!-- Hackiest hack ever for the draft -->\n<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/sentence-transformers/paraphrase-MiniLM-L6-v2\"><code>sentence-transformers/paraphrase-MiniLM-L6-v2</code></a>",
        "question": "What is the name of the model that is a paraphrase model?\n",
        "answer": "The name of the model is sentence-transformers/paraphrase-MiniLM-L6-v2.",
        "source_doc": "huggingface/blog/blob/main/sentence-transformers-in-the-hub.md"
    },
    {
        "context": "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Token classification\n\n## PyTorch version\n\nFine-tuning the library models for token classification task such as Named Entity Recognition (NER), Parts-of-speech\ntagging (POS) or phrase extraction (CHUNKS). The main scrip `run_ner.py` leverages the 🤗 Datasets library and the Trainer API. You can easily\ncustomize it to your needs if you need extra processing on your datasets.\n\nIt will either run on a datasets hosted on our [hub](https://huggingface.co/datasets) or with your own text files for\ntraining and validation, you might just need to add some tweaks in the data preprocessing.\n\nThe following example fine-tunes BERT on CoNLL-2003:\n\n```bash\npython run_ner.py \\\n  --model_name_or_path bert-base-uncased \\\n  --dataset_name conll2003 \\\n  --output_dir /tmp/test-ner \\\n  --do_train \\\n  --do_eval\n```\n\nor just can just run the bash script `run.sh`.\n\nTo run on your own training and validation files, use the following command:\n\n```bash\npython run_ner.py \\\n  --model_name_or_path bert-base-uncased \\\n  --train_file path_to_train_file \\\n  --validation_file path_to_validation_file \\\n  --output_dir /tmp/test-ner \\\n  --do_train \\\n  --do_eval\n```",
        "question": "What is the name of the main script for fine-tuning the library models for token classification task?\n",
        "answer": "The name of the main script for fine-tuning the library models for token classification task is `run_ner.py`.",
        "source_doc": "huggingface/transformers/blob/main/examples/pytorch/token-classification/README.md"
    },
    {
        "context": "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->",
        "question": "What is the world's largest coral reef system?\n",
        "answer": "The Great Barrier Reef is the world's largest coral reef system.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md"
    },
    {
        "context": "## Starcraft II\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/alphastar.jpg\" alt=\"Alphastar\"/>\n\nStarcraft II is a famous *real-time strategy game*. DeepMind has used this game for their Deep Reinforcement Learning research with [Alphastar](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)\n\nTo start using this environment, check these resources:\n- [Starcraft gym](http://starcraftgym.com/)\n- [A. I. Learns to Play Starcraft 2 (Reinforcement Learning) tutorial](https://www.youtube.com/watch?v=q59wap1ELQ4)\n\n## Author\n\nThis section was written by <a href=\"https://twitter.com/ThomasSimonini\"> Thomas Simonini</a>",
        "question": "What is the name of the real-time strategy game used by DeepMind for their Deep Reinforcement Learning research?\n",
        "answer": "Starcraft II",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unitbonus3/envs-to-try.mdx"
    },
    {
        "context": "Gradio Demo: image-simple\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo repo\nimport os\n!wget -q https://github.com/gradio-app/gradio/raw/main/demo/image-simple/cheetah.jpg\n```\n\n\n```\nimport gradio as gr\n\n\ndef image(im):\n    return im\n\n\nwith gr.Blocks() as demo:\n    im = gr.Image()\n    im2 = gr.Image()\n    btn = gr.Button()\n    btn.click(lambda x: x, outputs=im2, inputs=im)\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```",
        "question": "What is the name of the image file downloaded in the demo?\n",
        "answer": "cheetah.jpg",
        "source_doc": "gradio-app/gradio/blob/main/demo/image-simple/run.ipynb"
    },
    {
        "context": "Instagram ResNeXt WSL\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-block) that aggregates a set of transformations with the same topology. Compared to a [ResNet](https://paperswithcode.com/method/resnet), it exposes a new dimension,  *cardinality* (the size of the set of transformations) \\\\( C \\\\), as an essential factor in addition to the dimensions of depth and width.\n\nThis model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance.\n\nPlease note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.\n\n## How do I use this model on an image?\n\nTo load a pretrained model:\n\n```py\n>>> import timm\n>>> model = timm.create_model('ig_resnext101_32x16d', pretrained=True)\n>>> model.eval()\n```\n\nTo load and preprocess the image:\n\n```py\n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve_data_config\n>>> from timm.data.transforms_factory import create_transform\n\n>>> config = resolve_data_config({}, model=model)\n>>> transform = create_transform(**config)\n\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]",
        "question": "How do I load a pretrained Instagram ResNeXt model in Python?\n",
        "answer": "To load a pretrained Instagram ResNeXt model in Python, you can use the `timm` library and the `create_model` function with the model name `'ig_resnext101_32x16d'` and the `pretrained` argument set to `True`. Here's an example:\n```\nimport timm\nmodel = timm.create_model('ig_resnext101_32x16d', pretrained=True)\nmodel.eval()\n```",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/ig-resnext.mdx"
    },
    {
        "context": "The `body` looks as follows:\n\n```html\n<body>\n    <div id=\"container\" class=\"mol-container\"></div>\n    <script>\n        let pdb = mol // mol contains PDB file content, check the hf.space/simonduerr/3dmol.js for full python code\n        $(document).ready(function () {\n            let element = $(\"#container\");\n            let config = { backgroundColor: \"white\" };\n            let viewer = $3Dmol.createViewer(element, config);\n            viewer.addModel(pdb, \"pdb\");\n            viewer.getModel(0).setStyle({}, { cartoon: { colorscheme:\"whiteCarbon\" } });\n            viewer.zoomTo();\n            viewer.render();\n            viewer.zoom(0.8, 2000);\n            })\n    </script>\n</body>\n```\nWe use a template literal (denoted by backticks) to store our pdb file in the html document directly and then output it using 3dmol.js.\n\nAnd that's it, now you can couple your favorite protein ML model to a fun and easy to use gradio app and directly visualize predicted or redesigned structures. If you are predicting properities of a structure (e.g how likely each amino acid is to bind a ligand), 3Dmol.js also allows to use a custom `colorfunc` based on a property of each atom. \n\nYou can check the [source code](https://huggingface.co/spaces/simonduerr/3dmol.js/blob/main/app.py) of the 3Dmol.js space for the full code.\n\nFor a production example, you can check the [ProteinMPNN](https://hf.space/simonduerr/ProteinMPNN) space where a user can upload a backbone, the inverse folding model ProteinMPNN predicts new optimal sequences and then one can run AlphaFold2 on all predicted sequences to verify whether they adopt the initial input backbone. Successful redesigns that qualitiatively adopt the same structure as predicted by AlphaFold2 with high pLDDT score should be tested in the lab. \n\n<gradio-app theme_mode=\"light\" space=\"simonduerr/ProteinMPNN\"></gradio-app>\n\n# Issues",
        "question": "What is the name of the inverse folding model used in the ProteinMPNN space?\n",
        "answer": "ProteinMPNN",
        "source_doc": "huggingface/blog/blob/main/spaces_3dmoljs.md"
    },
    {
        "context": "3. If there is a software failure, always provide the full traceback, for example:\n\n   ```python\n   $ python -c 'import transformers'\n   Traceback (most recent call last):\n     File \"<string>\", line 1, in <module>\n     File \"/transformers/src/transformers/__init__.py\", line 34, in <module>\n       from . import dependency_versions_check\n     File \"/transformers/src/transformers/dependency_versions_check.py\", line 34, in <module>\n       from .utils import is_tokenizers_available\n     File \"/transformers/src/transformers/utils/import_utils.py\", line 40, in <module>\n       from tqdm.auto import tqdm\n   ModuleNotFoundError: No module named 'tqdm.auto'\n   ```\n\n   As compared to providing just the last line of the error message, e.g.:\n   ```python\n   ModuleNotFoundError: No module named 'tqdm.auto'\n   ```\n   which is not sufficient.\n\n   If your application is running on more than one GPU (e.g. under `DistributedDataParallel`) and typically getting every log and traceback printed multiple times, please make sure that you paste only one copy of it. At times the traceback from parallel processes may get interleaved - so either disentangle these or change the loggers to log only for `local_rank==0` so that only one process logs things.\n\n4. When quoting a traceback, command line instructions and any type of code always enclose it in triple backticks inside the editor window, that is:\n\n   ````\n   ```\n   git clone https://github.com/huggingface/transformers\n   cd transformers\n   pip install .\n   ```\n   ````\n\n   If it's a command line with a long argument list, please consider breaking it down using backslashes and new lines. Here is an example of a good command line quote:",
        "question": "How should a traceback be provided in case of a software failure?\n",
        "answer": "A traceback should be provided in full, including the error message and the lines of code that led to the error. It should be enclosed in triple backticks inside the editor window. If the traceback is from parallel processes, only one copy should be pasted and the loggers should be changed to log only for local\\_rank==0.",
        "source_doc": "huggingface/transformers/blob/main/ISSUES.md"
    },
    {
        "context": "At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. `4.5689` which is usually stored in either [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), or [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) format. This allows us to easily compute the memory requirement to load the LLM into memory:\n\n> *Loading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in float32 precision*\n\nNowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:\n\n> *Loading the weights of a model having X billion parameters requires roughly 2 * X GB of VRAM in bfloat16/float16 precision*\n\nFor shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let's assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\n\nTo give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** requires 2 \\* 175 GB = **350 GB** VRAM\n-   [**Bloom**](https://huggingface.co/bigscience/bloom) requires 2 \\* 176 GB = **352 GB** VRAM\n-   [**Llama-2-70b**](https://huggingface.co/meta-llama/Llama-2-70b-hf) requires 2 \\* 70 GB = **140 GB** VRAM\n-   [**Falcon-40b**](https://huggingface.co/tiiuae/falcon-40b) requires 2 \\* 40 GB = **80 GB** VRAM\n-   [**MPT-30b**](https://huggingface.co/mosaicml/mpt-30b) requires 2 \\* 30 GB = **60 GB** VRAM\n-   [**bigcode/starcoder**](https://huggingface.co/bigcode/starcoder) requires 2 \\* 15.5 = **31 GB** VRAM",
        "question": "How much VRAM does GPT3 require in bfloat16?\n",
        "answer": "350 GB",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Kandinsky 2.2\n\nKandinsky 2.2 is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).\n\nThe description from it's GitHub page is:\n\n*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model's capability to generate more aesthetic pictures and better understand text, thus enhancing the model's overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*\n\nThe original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\n\n<Tip>\n\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\n<Tip>",
        "question": "What is the original codebase of Kandinsky 2.2?\n",
        "answer": "The original codebase of Kandinsky 2.2 can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\n</Tip>",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md"
    },
    {
        "context": "## MT5Config\n\n[[autodoc]] MT5Config\n\n## MT5Tokenizer\n\n[[autodoc]] MT5Tokenizer\n\nSee [`T5Tokenizer`] for all details.\n\n\n## MT5TokenizerFast\n\n[[autodoc]] MT5TokenizerFast\n\nSee [`T5TokenizerFast`] for all details.\n\n<frameworkcontent>\n<pt>\n\n## MT5Model\n\n[[autodoc]] MT5Model\n\n## MT5ForConditionalGeneration\n\n[[autodoc]] MT5ForConditionalGeneration\n\n## MT5EncoderModel\n\n[[autodoc]] MT5EncoderModel\n\n## MT5ForSequenceClassification\n\n[[autodoc]] MT5ForSequenceClassification\n\n## MT5ForQuestionAnswering\n\n[[autodoc]] MT5ForQuestionAnswering\n\n</pt>\n<tf>\n\n## TFMT5Model\n\n[[autodoc]] TFMT5Model\n\n## TFMT5ForConditionalGeneration\n\n[[autodoc]] TFMT5ForConditionalGeneration\n\n## TFMT5EncoderModel\n\n[[autodoc]] TFMT5EncoderModel\n\n</tf>\n<jax>\n\n## FlaxMT5Model\n\n[[autodoc]] FlaxMT5Model\n\n## FlaxMT5ForConditionalGeneration\n\n[[autodoc]] FlaxMT5ForConditionalGeneration\n\n## FlaxMT5EncoderModel\n\n[[autodoc]] FlaxMT5EncoderModel\n\n</jax>\n</frameworkcontent>",
        "question": "What is the name of the class for the MT5 model in TensorFlow?\n",
        "answer": "TFMT5Model",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/mt5.md"
    },
    {
        "context": "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n```\n\nRe-loading the first audio sample in the Common Voice dataset will resample \nit to the desired sampling rate:\n\n```python\nprint(common_voice[\"train\"][0])\n```\n**Print Output:**\n```python\n{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3', \n           'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n       -3.4206650e-07,  3.2979898e-07,  1.0042874e-06], dtype=float32),\n           'sampling_rate': 16000},\n 'sentence': 'खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई'}\n```\nGreat! We can see that the sampling rate has been downsampled to 16kHz. The \narray values are also different, as we've now only got approximately one amplitude value \nfor every three we had before.\n\nNow we can write a function to prepare our data ready for the model:\n1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, 🤗 Datasets performs any necessary resampling operations on the fly.\n2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n3. We encode the transcriptions to label ids through the use of the tokenizer.\n\n```python\ndef prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"audio\"]\n\n    # compute log-Mel input features from input audio array \n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n\n    # encode target text to label ids \n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n```\n\nWe can apply the data preparation function to all of our training examples using dataset's `.map` method:",
        "question": "What does the `prepare_dataset` function return?\n",
        "answer": "The `prepare_dataset` function returns a batch with the following keys: `input_features`, `labels`, `audio`.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-whisper.md"
    },
    {
        "context": "| Model       | NER F1 (mean ± std) | NCC Accuracy (mean ± std)           |\n|:-------------:|:-------------:|:-------------:|\n|[sahajBERT](https://huggingface.co/neuropark/sahajBERT) |  95.45 ± 0.53|  91.97 ± 0.47|\n|[XLM-R-large](https://huggingface.co/xlm-roberta-large) |  96.48 ± 0.22| 90.05 ± 0.38|\n|[IndicBert](https://huggingface.co/ai4bharat/indic-bert) |  92.52 ± 0.45| 74.46 ± 1.91|\n|[bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta)       |82.32 ± 0.67|80.94 ± 0.45|\n\nThese models are available on the Hub as well. You can test them directly by playing with the Hosted Inference API widget on their Model Cards or by loading them directly in your Python code.\n\n#### sahajBERT-NER\nModel card: [https://hf.co/neuropark/sahajBERT-NER](https://hf.co/neuropark/sahajBERT-NER)\n```python\nfrom transformers import (\n    AlbertForTokenClassification,\n    TokenClassificationPipeline,\n    PreTrainedTokenizerFast,\n)\n\n# Initialize tokenizer\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"neuropark/sahajBERT-NER\")\n\n# Initialize model\nmodel = AlbertForTokenClassification.from_pretrained(\"neuropark/sahajBERT-NER\")\n\n# Initialize pipeline\npipeline = TokenClassificationPipeline(tokenizer=tokenizer, model=model)\n\nraw_text = \"এই ইউনিয়নে ৩ টি মৌজা ও ১০ টি গ্রাম আছে ।\" # Change me\noutput = pipeline(raw_text)\n```\n\n#### sahajBERT-NCC\nModel card: [https://hf.co/neuropark/sahajBERT-NER](https://hf.co/neuropark/sahajBERT-NCC)\n```python\nfrom transformers import (\n    AlbertForSequenceClassification,\n    TextClassificationPipeline,\n    PreTrainedTokenizerFast,\n)\n\n# Initialize tokenizer\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"neuropark/sahajBERT-NCC\")\n\n# Initialize model\nmodel = AlbertForSequenceClassification.from_pretrained(\"neuropark/sahajBERT-NCC\")\n\n# Initialize pipeline\npipeline = TextClassificationPipeline(tokenizer=tokenizer, model=model)",
        "question": "What is the NCC accuracy of bnRoBERTa?\n",
        "answer": "The NCC accuracy of bnRoBERTa is 80.94 ± 0.45.",
        "source_doc": "huggingface/blog/blob/main/collaborative-training.md"
    },
    {
        "context": "This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\n\n## Usage tips\n\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\n  will work.\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\n  cross-modality layer, so they contain information from both modalities. To access a modality that only attends to\n  itself, select the vision/language hidden states from the first input in the tuple.\n- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\n  as the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\n  contains self-attention for each respective modality and cross-attention, only the cross attention is returned and\n  both self attention outputs are disregarded.\n\n## Resources\n\n- [Question answering task guide](../tasks/question_answering)\n\n## LxmertConfig\n\n[[autodoc]] LxmertConfig\n\n## LxmertTokenizer\n\n[[autodoc]] LxmertTokenizer\n\n## LxmertTokenizerFast\n\n[[autodoc]] LxmertTokenizerFast\n\n## Lxmert specific outputs\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\n\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\n\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## LxmertModel\n\n[[autodoc]] LxmertModel\n    - forward\n\n## LxmertForPreTraining\n\n[[autodoc]] LxmertForPreTraining\n    - forward\n\n## LxmertForQuestionAnswering\n\n[[autodoc]] LxmertForQuestionAnswering\n    - forward\n\n</pt>\n<tf>\n\n## TFLxmertModel\n\n[[autodoc]] TFLxmertModel\n    - call\n\n## TFLxmertForPreTraining\n\n[[autodoc]] TFLxmertForPreTraining\n    - call",
        "question": "What is the name of the model's tokenizer?\n",
        "answer": "LxmertTokenizer or LxmertTokenizerFast",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md"
    },
    {
        "context": "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (The FAIR team of Meta AI から) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.. から公開された研究論文 [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX)\n1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (Microsoft Research & University of Wisconsin-Madison から) Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. から公開された研究論文 [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (AllenAI から) Iz Beltagy, Matthew E. Peters, Arman Cohan から公開された研究論文: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)",
        "question": "Which model was developed by the FAIR team of Meta AI?\n",
        "answer": "Llama2",
        "source_doc": "huggingface/transformers/blob/main/README_ja.md"
    },
    {
        "context": "upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide. It's only one API to learn and master instead of ten different ones. More specifically, you have to import and instantiate an accelerator object, that will handle all the necessary code for your  specific setup. Then you have to send it the model, optimizer and dataloaders you are using in the prepare method, which is the main method to remember. Accelerate handles device placement, so you don't need to put your batch on the specific device you are using. Finally, you have to replace the loss.backward line by accelerate.backward(loss), and that's all you need! Accelerate also handles distributed evaluation. You can still use a classic evaluation loop such as the one we saw in the \"Raw training loop\" video, in which case all processes will each perform the full evaluation",
        "question": "How many lines of code do I need to add to my training loop to use Hugging Face Accelerate?\n",
        "answer": "You need to add four lines of code to your training loop to use Hugging Face Accelerate.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter3/04b_accelerate.md"
    },
    {
        "context": "Stable Diffusion\n\n## Overview\n\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\n\nThe summary of the model is the following:\n\n*Stable Diffusion is a text-to-image model that will empower billions of people to create stunning art within seconds. It is a breakthrough in speed and quality meaning that it can run on consumer GPUs. You can see some of the amazing output that has been created by this model without pre or post-processing on this page. The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all.*\n\n## Tips:\n\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion).\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\n- Stable Diffusion can work with a variety of different samplers as is shown below.\n\n## Available Pipelines:",
        "question": "What is the architecture of Stable Diffusion?\n",
        "answer": "The architecture of Stable Diffusion is the same as Latent Diffusion but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.",
        "source_doc": "huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md"
    },
    {
        "context": "FrameworkSwitchCourse {fw} />\n\n# Summarization[[summarization]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner chapter={7}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb\"},\n]} />\n\n{:else}\n\n<CourseFloatingBanner chapter={7}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb\"},\n]} />\n\n{/if}\n\n\nIn this section we'll take a look at how Transformer models can be used to condense long documents into summaries, a task known as _text summarization_. This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail.\n\n<Youtube id=\"yHnr5Dk2zCI\"/>\n\nAlthough there already exist various fine-tuned models for summarization on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), almost all of these are only suitable for English documents. So, to add a twist in this section, we'll train a bilingual model for English and Spanish. By the end of this section, you'll have a [model](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) that can summarize customer reviews like the one shown here:",
        "question": "What is the NLP task that Transformer models can be used for in this section?\n",
        "answer": "Text summarization",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/5.mdx"
    },
    {
        "context": "```bash\nsudo apt-get update\napt install python-opengl\napt install ffmpeg\napt install xvfb\npip3 install pyvirtualdisplay\n```\n\nTo make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**\n\n```python\nimport os\n\nos.kill(os.getpid(), 9)\n```\n\n```python\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n## Import the packages 📦\n\nOne additional library we import is huggingface_hub **to be able to upload and download trained models from the hub**.\n\n\nThe Hugging Face Hub 🤗 works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.\n\nYou can see here all the Deep reinforcement Learning models available here👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n\n\n\n```python\nimport gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import (\n    notebook_login,\n)  # To log to our Hugging Face account to be able to upload models to the Hub.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\n```\n\n## Understand Gymnasium and how it works 🤖\n\n🏋 The library containing our environment is called Gymnasium.\n**You'll use Gymnasium a lot in Deep Reinforcement Learning.**\n\nGymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).\n\nThe Gymnasium library provides two things:",
        "question": "What is the new version of Gym library?\n",
        "answer": "Gymnasium",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx"
    },
    {
        "context": "```py\n>>> processor = ViltProcessor.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n\n>>> # prepare inputs\n>>> inputs = processor(image, question, return_tensors=\"pt\")\n\n>>> model = ViltForQuestionAnswering.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: down\n```\n\n## Zero-shot VQA\n\nThe previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach \nVQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training \nparadigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). \nThis enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. \n\nLet's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a \nGPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: \n\n```py\n>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n>>> import torch\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n```\n\nThe model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset: \n\n```py \n>>> example = dataset[0]\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n```",
        "question": "What is the predicted answer for the example?\n",
        "answer": "The predicted answer for the example is 'The balloon is red.'",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md"
    },
    {
        "context": "|              [DreamBooth](#dreambooth)              |         ❌         |                   ✅                    |                                                                                                 |\n|       [Textual Inversion](#textual-inversion)       |         ❌         |                   ✅                    |                                                                                                 |\n|              [ControlNet](#controlnet)              |         ✅         |                   ❌                    |             A ControlNet can be <br>trained/fine-tuned on<br>a custom conditioning.             |\n|        [Prompt Weighting](#prompt-weighting)        |         ✅         |                   ❌                    |                                                                                                 |\n|        [Custom Diffusion](#custom-diffusion)        |         ❌         |                   ✅                    |                                                                                                 |\n|           [Model Editing](#model-editing)           |         ✅         |                   ❌                    |                                                                                                 |\n|                [DiffEdit](#diffedit)                |         ✅         |                   ❌                    |                                                                                                 |\n|             [T2I-Adapter](#t2i-adapter)             |         ✅         |                   ❌                    |                                                                                                 |\n|                [Fabric](#fabric)                    |         ✅         |                   ❌                    |                                                                                                 |\n## InstructPix2Pix",
        "question": "What is InstructPix2Pix?\n",
        "answer": "InstructPix2Pix is a method that can be trained/fine-tuned on a custom conditioning.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlling_generation.md"
    },
    {
        "context": "### The clipped Part of the Clipped Surrogate Objective function\n\n<img src=\"assets/93_deep_rl_ppo/clipped.jpg\" alt=\"PPO\"/>\n\nConsequently, we need to constrain this objective function by penalizing changes that lead to a ratio away from 1 (in the paper, the ratio can only vary from 0.8 to 1.2).\n\n**By clipping the ratio, we ensure that we do not have a too large policy update because the current policy can't be too different from the older one.**\n\nTo do that, we have two solutions:\n\n- *TRPO (Trust Region Policy Optimization)* uses KL divergence constraints outside the objective function to constrain the policy update. But this method **is complicated to implement and takes more computation time.**\n- *PPO* clip probability ratio directly in the objective function with its **Clipped surrogate objective function.**\n\n<img src=\"assets/93_deep_rl_ppo/clipped.jpg\" alt=\"PPO\"/>\n\nThis clipped part is a version where rt(theta) is clipped between  \\\\( [1 - \\epsilon, 1 + \\epsilon] \\\\).\n\nWith the Clipped Surrogate Objective function, we have two probability ratios, one non-clipped and one clipped in a range (between  \\\\( [1 - \\epsilon, 1 + \\epsilon] \\\\), epsilon is a hyperparameter that helps us to define this clip range (in the paper  \\\\( \\epsilon = 0.2 \\\\).).\n\nThen, we take the minimum of the clipped and non-clipped objective, **so the final objective is a lower bound (pessimistic bound) of the unclipped objective.**\n\nTaking the minimum of the clipped and non-clipped objective means **we'll select either the clipped or the non-clipped objective based on the ratio and advantage situation**.\n\n## Visualize the Clipped Surrogate Objective\nDon't worry. **It's normal if this seems complex to handle right now**. But we're going to see what this Clipped Surrogate Objective Function looks like, and this will help you to visualize better what's going on.",
        "question": "What is the range of the clipped probability ratio in PPO?\n",
        "answer": "The clipped probability ratio in PPO is clipped between [1 - ϵ, 1 + ϵ], where ϵ is a hyperparameter that helps define the clip range. In the paper, ϵ = 0.2.",
        "source_doc": "huggingface/blog/blob/main/deep-rl-ppo.md"
    },
    {
        "context": "With PyTorch 1.6+ it'll automatically use `native AMP` when `--fp16` is set.\n\nTo see all the possible command line options, run:\n\n```bash\npython finetune_trainer.py --help\n```\n\nFor multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\n```bash\ntorchrun --nproc_per_node=2  finetune_trainer.py ...\n```\n\n**At the moment, `Seq2SeqTrainer` does not support *with teacher* distillation.**\n\nAll `Seq2SeqTrainer`-based fine-tuning scripts are included in the `builtin_trainer` directory.\n\n#### TPU Training\n`Seq2SeqTrainer` supports TPU training with few caveats\n1. As `generate` method does not work on TPU at the moment, `predict_with_generate` cannot be used. You should use `--prediction_loss_only` to only calculate loss, and do not set `--do_predict` and `--predict_with_generate`.\n2. All sequences should be padded to be of equal length to avoid extremely slow training. (`finetune_trainer.py` does this automatically when running on TPU.)\n\nWe provide a very simple launcher script named `xla_spawn.py` that lets you run our example scripts on multiple TPU cores without any boilerplate. Just pass a `--num_cores` flag to this script, then your regular training script with its arguments (this is similar to the `torch.distributed.launch` helper for `torch.distributed`).\n\n`builtin_trainer/finetune_tpu.sh` script provides minimal arguments needed for TPU training.\n\nThe following command fine-tunes `sshleifer/student_marian_en_ro_6_3` on TPU V3-8 and should complete one epoch in ~5-6 mins.\n\n```bash\n./builtin_trainer/train_distil_marian_enro_tpu.sh\n```\n\n## Evaluation Commands\n\nTo create summaries for each article in dataset, we use `run_eval.py`, here are a few commands that run eval for different tasks and models.\nIf 'translation' is in your task name, the computed metric will be BLEU. Otherwise, ROUGE will be used.",
        "question": "What does the `Seq2SeqTrainer` not support?\n",
        "answer": "The `Seq2SeqTrainer` does not support 'with teacher' distillation.",
        "source_doc": "huggingface/transformers/blob/main/examples/legacy/seq2seq/README.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at [this https URL.](https://huggingface.co/bigcode)*\n\nThe model is an optimized [GPT2 model](https://huggingface.co/docs/transformers/model_doc/gpt2) with support for Multi-Query Attention.\n\n## Implementation details",
        "question": "What is the license of the BigCode project models?\n",
        "answer": "The BigCode project models are released under an OpenRAIL license.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_bigcode.md"
    },
    {
        "context": "## Wrap the base model as a PeftModel for LoRA training\n\nTo leverage the LoRa method, you need to wrap the base model as a `PeftModel`.  This involves two steps:\n\n1. Defining LoRa configuration with `LoraConfig`\n2. Wrapping the original `model` with `get_peft_model()` using the config defined in the step above.\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"lora_only\",\n    modules_to_save=[\"decode_head\"],\n)\nlora_model = get_peft_model(model, config)\nprint_trainable_parameters(lora_model)\n```\n\nLet's review the `LoraConfig`. To enable LoRA technique, we must define the target modules within `LoraConfig` so that \n`PeftModel` can update the necessary matrices. Specifically, we want to target the `query` and `value` matrices in the \nattention blocks of the base model. These matrices are identified by their respective names, \"query\" and \"value\". \nTherefore, we should specify these names in the `target_modules` argument of `LoraConfig`.\n\nAfter we wrap our base model `model` with `PeftModel` along with the config, we get \na new model where only the LoRA parameters are trainable (so-called \"update matrices\") while the pre-trained parameters \nare kept frozen. These include the parameters of the randomly initialized classifier parameters too. This is NOT we want \nwhen fine-tuning the base model on our custom dataset. To ensure that the classifier parameters are also trained, we \nspecify `modules_to_save`. This also ensures that these modules are serialized alongside the LoRA trainable parameters \nwhen using utilities like `save_pretrained()` and `push_to_hub()`.",
        "question": "How to enable LoRA technique in the base model?\n",
        "answer": "To enable LoRA technique in the base model, you must define the target modules within `LoraConfig` so that `PeftModel` can update the necessary matrices. Specifically, you should specify the names of the matrices you want to target in the `target_modules` argument of `LoraConfig`. In this case, the `query` and `value` matrices in the attention blocks of the base model should be targeted.",
        "source_doc": "huggingface/peft/blob/main/docs/source/task_guides/semantic_segmentation_lora.md"
    },
    {
        "context": "**`disable_embedding`** : _boolean_  \nWhether the Space iframe can be embedded in other websites.\nDefaults to false, i.e. Spaces *can* be embedded.\n\n**`startup_duration_timeout`**: _string_  \nSet a custom startup duration timeout for your Space. This is the maximum time your Space is allowed to start before it times out and is flagged as unhealthy.\nDefaults to 30 minutes, but any valid duration (like `1h`, `30m`) is acceptable.\n\n**`custom_headers`** : _Dict[string, string]_  \nSet custom HTTP headers that will be added to all HTTP responses when serving your Space.  \nFor now, only the [cross-origin-embedder-policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Embedder-Policy) (COEP), [cross-origin-opener-policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Opener-Policy) (COOP), and [cross-origin-resource-policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Resource-Policy) (CORP) headers are allowed. These headers can be used to set up a cross-origin isolated environment and enable powerful features like `SharedArrayBuffer`, for example:\n\n```yaml\ncustom_headers:\n  cross-origin-embedder-policy: require-corp\n  cross-origin-opener-policy: same-origin\n  cross-origin-resource-policy: cross-origin\n```\n\n*Note:* all headers and values must be lowercase.\n\n**`preload_from_hub`**: _List[string]_\nSpecify a list of Hugging Face Hub models or other large files to be preloaded during the build time of your Space. This optimizes the startup time by having the files ready when your application starts. This is particularly useful for Spaces that rely on large models or datasets that would otherwise need to be downloaded at runtime.",
        "question": "What is the purpose of `preload_from_hub`?\n",
        "answer": "The purpose of `preload_from_hub` is to specify a list of Hugging Face Hub models or other large files to be preloaded during the build time of a Space, optimizing the startup time by having the files ready when the application starts.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-config-reference.md"
    },
    {
        "context": "### Contributors Shoutout:\n\nNo changes to highlight.\n\n- Fixed typo in parameter `visible` in classes in `templates.py` by [@abidlabs](https://github.com/abidlabs) in [PR 2805](https://github.com/gradio-app/gradio/pull/2805)\n- Switched external service for getting IP address from `https://api.ipify.org` to `https://checkip.amazonaws.com/` by [@abidlabs](https://github.com/abidlabs) in [PR 2810](https://github.com/gradio-app/gradio/pull/2810)\n\n## 3.13.0\n\n### New Features:\n\n###### Scatter plot component\n\nIt is now possible to create a scatter plot natively in Gradio!\n\nThe `gr.ScatterPlot` component accepts a pandas dataframe and some optional configuration parameters\nand will automatically create a plot for you!\n\nThis is the first of many native plotting components in Gradio!\n\nFor an example of how to use `gr.ScatterPlot` see below:\n\n```python\nimport gradio as gr\nfrom vega_datasets import data\n\ncars = data.cars()\n\nwith gr.Blocks() as demo:\n    gr.ScatterPlot(show_label=False,\n                   value=cars,\n                   x=\"Horsepower\",\n                   y=\"Miles_per_Gallon\",\n                   color=\"Origin\",\n                   tooltip=\"Name\",\n                   title=\"Car Data\",\n                   y_title=\"Miles per Gallon\",\n                   color_legend_title=\"Origin of Car\").style(container=False)\n\ndemo.launch()\n```\n\n<img width=\"404\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41651716/206737726-4c4da5f0-dee8-4f0a-b1e1-e2b75c4638e9.png\">\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2764](https://github.com/gradio-app/gradio/pull/2764)\n\n###### Support for altair plots\n\nThe `Plot` component can now accept altair plots as values!\nSimply return an altair plot from your event listener and gradio will display it in the front-end.\nSee the example below:\n\n```python\nimport gradio as gr\nimport altair as alt\nfrom vega_datasets import data",
        "question": "Who contributed to the addition of altair plots in Gradio?\n",
        "answer": "The addition of altair plots in Gradio was contributed by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2764](https://github.com/gradio-app/gradio/pull/2764).",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "> *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add the flag `--inference_with_ort`.*\n---",
        "question": "How can I use ONNX Runtime backend for inference?\n",
        "answer": "By adding the flag `--inference_with_ort` when using the inference tool.",
        "source_doc": "huggingface/optimum/blob/main/examples/onnxruntime/training/question-answering/README.md"
    },
    {
        "context": "`@gradio/gallery`\n\n```html\n<script>\n\timport { BaseGallery } from \"@gradio/gallery\";\n</script>\n```\n\nBaseGallery\n```javascript\n\texport let show_label = true;\n\texport let label: string;\n\texport let root = \"\";\n\texport let root_url: null | string = null;\n\texport let value: { image: FileData; caption: string | null }[] | null = null;\n\texport let columns: number | number[] | undefined = [2];\n\texport let rows: number | number[] | undefined = undefined;\n\texport let height: number | \"auto\" = \"auto\";\n\texport let preview: boolean;\n\texport let allow_preview = true;\n\texport let object_fit: \"contain\" | \"cover\" | \"fill\" | \"none\" | \"scale-down\" =\n\t\t\"cover\";\n\texport let show_share_button = false;\n\texport let show_download_button = false;\n\texport let i18n: I18nFormatter;\n\texport let selected_index: number | null = null;\n```",
        "question": "What is the default value of the `show_label` property in the `BaseGallery` component?\n",
        "answer": "The default value of the `show_label` property in the `BaseGallery` component is `true`.",
        "source_doc": "gradio-app/gradio/blob/main/js/gallery/README.md"
    },
    {
        "context": "Next, enter your model's name. This will also be the name of the repository. Finally, you can specify whether you want your model to be public or private. Private models are hidden from public view.\n\nAfter creating your model repository, you should see a page like this:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/empty_model.png\" alt=\"An empty model page after creating a new repository.\" width=\"80%\"/>\n</div>\n\nThis is where your model will be hosted. To start populating it, you can add a README file directly from the web interface.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/dummy_model.png\" alt=\"The README file showing the Markdown capabilities.\" width=\"80%\"/>\n</div>\n\nThe README file is in Markdown — feel free to go wild with it! The third part of this chapter is dedicated to building a model card. These are of prime importance in bringing value to your model, as they're where you tell others what it can do.\n\nIf you look at the \"Files and versions\" tab, you'll see that there aren't many files there yet — just the *README.md* you just created and the *.gitattributes* file that keeps track of large files.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/files.png\" alt=\"The 'Files and versions' tab only shows the .gitattributes and README.md files.\" width=\"80%\"/>\n</div>\n\nWe'll take a look at how to add some new files next.\n\n## Uploading the model files[[uploading-the-model-files]]\n\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for [Git Large File Storage](https://git-lfs.github.com/)) for larger files.",
        "question": "What is the system used to manage files on the Hugging Face Hub?\n",
        "answer": "The system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for Git Large File Storage) for larger files.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter4/3.mdx"
    },
    {
        "context": "- Dropout\n    - Global Average Pooling\n    - Hard Swish\n    - Inverted Residual Block\n    - ReLU\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - RMSProp\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 16x GPUs\n    ID: tf_mobilenetv3_small_100\n    LR: 0.045\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '224'\n    Weight Decay: 4.0e-05\n    Interpolation: bilinear\n    RMSProp Decay: 0.9\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/mobilenetv3.py#L430\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_100-37f49e2b.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 67.92%\n      Top 5 Accuracy: 87.68%\n- Name: tf_mobilenetv3_small_minimal_100\n  In Collection: TF MobileNet V3\n  Metadata:\n    FLOPs: 60827936\n    Parameters: 2040000\n    File Size: 8258083\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Depthwise Separable Convolution\n    - Dropout\n    - Global Average Pooling\n    - Hard Swish\n    - Inverted Residual Block\n    - ReLU\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - RMSProp\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 16x GPUs\n    ID: tf_mobilenetv3_small_minimal_100\n    LR: 0.045\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '224'\n    Weight Decay: 4.0e-05\n    Interpolation: bilinear\n    RMSProp Decay: 0.9\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/mobilenetv3.py#L439",
        "question": "What is the FLOPs of tf_mobilenetv3_small_minimal_100?\n",
        "answer": "The FLOPs of tf_mobilenetv3_small_minimal_100 is 60827936.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/tf-mobilenet-v3.mdx"
    },
    {
        "context": "🚀 Creating Discord Bots from Gradio Apps 🚀\n\nTags: NLP, TEXT, CHAT\n\nWe're excited to announce that Gradio can now automatically create a discord bot from a deployed app! 🤖\n\nDiscord is a popular communication platform that allows users to chat and interact with each other in real-time. By turning your Gradio app into a Discord bot, you can bring cutting edge AI to your discord server and give your community a whole new way to interact.\n\n## 💻 How does it work? 💻\n\nWith `gradio_client` version `0.3.0`, any gradio `ChatInterface` app on the internet can automatically be deployed as a discord bot via the `deploy_discord` method of the `Client` class.\n\nTechnically, any gradio app that exposes an api route that takes in a single string and outputs a single string can be deployed to discord. In this guide, we will focus on `gr.ChatInterface` as those apps naturally lend themselves to discord's chat functionality.\n\n## 🛠️ Requirements 🛠️\n\nMake sure you have the latest `gradio_client` and `gradio` versions installed.\n\n```bash\npip install gradio_client>=0.3.0 gradio>=3.38.0\n```\n\nAlso, make sure you have a [Hugging Face account](https://huggingface.co/) and a [write access token](https://huggingface.co/docs/hub/security-tokens).\n\n⚠️ Tip ⚠️: Make sure you login to the Hugging Face Hub by running `huggingface-cli login`. This will let you skip passing your token in all subsequent commands in this guide.\n\n## 🏃‍♀️ Quickstart 🏃‍♀️\n\n### Step 1: Implementing our chatbot\n\nLet's build a very simple Chatbot using `ChatInterface` that simply repeats the user message. Write the following code into an `app.py`\n\n```python\nimport gradio as gr\n\ndef slow_echo(message, history):\n    return message\n\ndemo = gr.ChatInterface(slow_echo).queue().launch()\n```\n\n### Step 2: Deploying our App",
        "question": "How to deploy a gradio app as a discord bot?\n",
        "answer": "To deploy a gradio app as a discord bot, you need to use the `deploy_discord` method of the `Client` class from the `gradio_client` version `0.3.0` or higher. The app should expose an api route that takes in a single string and outputs a single string. The `ChatInterface` app is a natural fit for discord's chat functionality. You also need to have the latest `gradio_client` and `gradio` versions installed, and a Hugging Face account with a write access token.",
        "source_doc": "gradio-app/gradio/blob/main/guides/04_chatbots/03_creating-a-discord-bot-from-a-gradio-app.md"
    },
    {
        "context": "New in v2:\n\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K built from the training data.\n  Instead of a GPT2-based tokenizer, the tokenizer is now\n  [sentencepiece-based](https://github.com/google/sentencepiece) tokenizer.\n- **nGiE(nGram Induced Input Encoding)** The DeBERTa-v2 model uses an additional convolution layer aside with the first\n  transformer layer to better learn the local dependency of input tokens.\n- **Sharing position projection matrix with content projection matrix in attention layer** Based on previous\n  experiments, this can save parameters without affecting the performance.\n- **Apply bucket to encode relative positions** The DeBERTa-v2 model uses log bucket to encode relative positions\n  similar to T5.\n- **900M model & 1.5B model** Two additional model sizes are available: 900M and 1.5B, which significantly improves the\n  performance of downstream tasks.\n\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/DeBERTa).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## DebertaV2Config\n\n[[autodoc]] DebertaV2Config\n\n## DebertaV2Tokenizer\n\n[[autodoc]] DebertaV2Tokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## DebertaV2TokenizerFast\n\n[[autodoc]] DebertaV2TokenizerFast\n    - build_inputs_with_special_tokens\n    - create_token_type_ids_from_sequences\n\n<frameworkcontent>\n<pt>\n\n## DebertaV2Model\n\n[[autodoc]] DebertaV2Model\n    - forward",
        "question": "What is the name of the original code that can be found in v2?\n",
        "answer": "The name of the original code that can be found in v2 is [here](https://github.com/microsoft/DeBERTa).\n\n</pt>\n</frameworkcontent>",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/deberta-v2.md"
    },
    {
        "context": ">>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n>>> print_gpu_utilization()\nGPU memory occupied: 2631 MB.\n```\n\nWe can see that the model weights alone take up 1.3 GB of GPU memory. The exact number depends on the specific \nGPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an \noptimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result \nas with `nvidia-smi` CLI:\n\n\n```bash\nnvidia-smi\n```\n\n```bash\nTue Jan 11 08:58:05 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+",
        "question": "How much GPU memory is being used by the model?\n",
        "answer": "2631 MB",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md"
    },
    {
        "context": "# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `ssl_resnext101_32x16d`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\n\n## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('ssl_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation",
        "question": "How many top categories are printed per image?\n",
        "answer": "5",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/ssl-resnext.md"
    },
    {
        "context": "Interpolation: bilinear\n    RMSProp Decay: 0.9\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/mobilenetv3.py#L439\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_minimal_100-922a7843.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 62.91%\n      Top 5 Accuracy: 84.24%\n-->",
        "question": "What is the top 1 accuracy of the mobilenetv3_small_minimal model on ImageNet?\n",
        "answer": "62.91%",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/tf-mobilenet-v3.md"
    },
    {
        "context": "```py\ngr.Image(source=\"canvas\", tools=\"sketch\")\n```\n\nNow, you should write:\n\n```py\ngr.ImageEditor(sources=(), brush=gr.Brush(colors=[\"#000000\"]))\n```\n\nNote: you can supply a list of supported stroke colors in `gr.Brush`, as well as control whether users can choose their own colors by setting the `color_mode` parameter of `gr.Brush` to be either `\"fixed\"` or `\"defaults\"`.\n\n* If you want to create a sketchpad where users can draw in any color, simply omit the `brush` parameter. In other words, where previously, you would do:\n\n```py\ngr.Image(source=\"canvas\", tools=\"color-sketch\")\n```\n\nNow, you should write:\n\n```py\ngr.ImageEditor(sources=())\n```\n\n\n* If you want to allow users to choose a background image and then draw on the image, previously, you would do:\n\n```py\ngr.Image(source=\"upload\", tools=\"color-sketch\")\n```\n\nNow, this is the default behavior of the `ImageEditor` component, so you should just write:\n\n```py\ngr.ImageEditor()\n```\n\nUnlike the `Image` component, which passes the input image as a single value into the prediction function, the `ImageEditor` passes a dictionary consisting of three key-value pairs:\n\n* the key `\"background\"`, whose value is the background image\n* the key `\"layers\"`, which consists of a list of values, with the strokes in each layer corresponding to one list element.\n* the key `\"composite\"`, whose value is to the complete image consisting of background image and all of the strokes.\n\nThe type of each value can be set by the `type` parameter (`\"filepath\"`, `\"pil\"`, or `\"numpy\"`, with the default being `\"numpy\"`), just like in the `Image` component.\n\nPlease see the documentation of the `gr.ImageEditor` component for more details: https://www.gradio.app/docs/imageeditor\n\n### Features",
        "question": "What is the default behavior of the `ImageEditor` component?\n",
        "answer": "The default behavior of the `ImageEditor` component is to allow users to choose a background image and then draw on the image.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "## Initializing the Tokenizer and Model\n\nFirst we need a tokenizer. Let's train one specifically on code so it splits code tokens well. We can take an existing tokenizer (e.g. GPT-2) and directly train it on our own dataset with the `train_new_from_iterator()` method. We then push it to the Hub. Note that we omit imports, arguments parsing and logging from the code examples to keep the code blocks compact. But you'll find the full code including preprocessing and downstream task evaluation [here](https://github.com/huggingface/transformers/tree/master/examples/research_projects/codeparrot).\n\n```Python\n# Iterator for Training\ndef batch_iterator(batch_size=10):\n    for _ in tqdm(range(0, args.n_examples, batch_size)):\n        yield [next(iter_dataset)[\"content\"] for _ in range(batch_size)]\n\n# Base tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nbase_vocab = list(bytes_to_unicode().values())\n\n# Load dataset\ndataset = load_dataset(\"lvwerra/codeparrot-clean\", split=\"train\", streaming=True)\niter_dataset = iter(dataset)\n\n# Training and saving\nnew_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),\n                                                  vocab_size=args.vocab_size,\n                                                  initial_alphabet=base_vocab)\nnew_tokenizer.save_pretrained(args.tokenizer_name, push_to_hub=args.push_to_hub)\n```\n\nLearn more about tokenizers and how to build them in the [Hugging Face course](https://huggingface.co/course/chapter6/1?fw=pt). \n\nSee that inconspicuous `streaming=True` argument? This small change has a big impact: instead of downloading the full (50GB) dataset this will stream individual samples as needed saving a lot of disk space! Checkout the [Hugging Face course](https://huggingface.co/course/chapter5/4?fw=pt\n) for more information on streaming.",
        "question": "What argument is used to stream individual samples of a dataset instead of downloading the full dataset?\n",
        "answer": "The `streaming=True` argument is used to stream individual samples of a dataset instead of downloading the full dataset.",
        "source_doc": "huggingface/blog/blob/main/codeparrot.md"
    },
    {
        "context": "1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.\n1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.\n1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.\n1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\n1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.",
        "question": "Which model was released by ADEPT?\n",
        "answer": "Persimmon",
        "source_doc": "huggingface/transformers/blob/main/README_pt-br.md"
    },
    {
        "context": "First, let's compare the default `google/reformer-enwik8` model without chunked feed forward layers to the one with chunked feed forward layers.\n\n\n```\nconfig_no_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\")  # no chunk\nconfig_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forward=1)  # feed forward chunk\nbenchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[1024, 2048, 4096], batch_sizes=[8], models=[\"Reformer-No-Chunk\", \"Reformer-Chunk\"], no_speed=True, no_env_print=True)\nbenchmark = PyTorchBenchmark(configs=[config_no_chunk, config_chunk], args=benchmark_args)\nresult = benchmark.run()\n```\n\n    1 / 2\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)\n    2 / 2\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.24 GiB free; 9.56 GiB reserved in total by PyTorch)\n    \n    ====================      INFERENCE - MEMORY - RESULT       ====================\n    --------------------------------------------------------------------------------\n              Model Name             Batch Size     Seq Length    Memory in MB \n    --------------------------------------------------------------------------------\n          Reformer-No-Chunk              8              1024            4281     \n          Reformer-No-Chunk              8              2048            7607     \n          Reformer-No-Chunk              8              4096            N/A      \n            Reformer-Chunk               8              1024            4309     \n            Reformer-Chunk               8              2048            7669     \n            Reformer-Chunk               8              4096            N/A      \n    --------------------------------------------------------------------------------",
        "question": "What is the memory usage of the Reformer-Chunk model with a sequence length of 1024 and a batch size of 8?\n",
        "answer": "The memory usage of the Reformer-Chunk model with a sequence length of 1024 and a batch size of 8 is 4309 MB.",
        "source_doc": "huggingface/blog/blob/main/reformer.md"
    },
    {
        "context": "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (Google Research から) Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya から公開された研究論文: [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)\n1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (META Platforms から) Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár から公開された研究論文: [Designing Network Design Space](https://arxiv.org/abs/2003.13678)\n1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (Google Research から) Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder から公開された研究論文: [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821)\n1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (Microsoft Research から) Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun から公開された研究論文: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (Facebook から), Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov から公開された研究論文: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (Facebook から) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli から公開された研究論文: [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038)\n1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (WeChatAI から) HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou から公開された研究論文: [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf)",
        "question": "What is the name of the research paper that introduced the Reformer model?\n",
        "answer": "The Efficient Transformer",
        "source_doc": "huggingface/transformers/blob/main/README_ja.md"
    },
    {
        "context": ".\\n(e) (1) Every contract subject to this chapter shall contain a statement by which the contractor certifies that the contractor is in compliance with this section.\\n(2) The department or other contracting agency shall enforce this section pursuant to its existing enforcement powers.\\n(3) (A) If a contractor falsely certifies that it is in compliance with this section, the contract with that contractor shall be subject to Article 9 (commencing with Section 10420), unless, within a time period specified by the department or other contracting agency, the contractor provides to the department or agency proof that it has complied, or is in the process of complying, with this section.\\n(B) The application of the remedies or penalties contained in Article 9 (commencing with Section 10420) to a contract subject to this chapter shall not preclude the application of any existing remedies otherwise available to the department or other contracting agency under its existing enforcement powers.\\n(f) Nothing in this section is intended to regulate the contracting practices of any local jurisdiction.\\n(g) This section shall be construed so as not to conflict with applicable federal laws, rules, or regulations. In the event that a court or agency of competent jurisdiction holds that federal law, rule, or regulation invalidates any clause, sentence, paragraph, or section of this code or the application thereof to any person or circumstances, it is the intent of the state that the court or agency sever that clause, sentence, paragraph, or section so that the remainder of this section shall remain in effect.\\nSEC. 2.\\nSection 10295.35 of the Public Contract Code shall not be construed to create any new enforcement authority or responsibility in the Department of General Services or any other contracting agency.\\nSEC. 3",
        "question": "What is the penalty for a contractor falsely certifying compliance with the section?\n",
        "answer": "If a contractor falsely certifies that it is in compliance with this section, the contract with that contractor shall be subject to Article 9 (commencing with Section 10420).",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md"
    },
    {
        "context": "## అనులేఖనం\n\n🤗 ట్రాన్స్‌ఫార్మర్స్ లైబ్రరీ కోసం మీరు ఉదహరించగల [పేపర్](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) ఇప్పుడు మా వద్ద ఉంది:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```",
        "question": "Who are the authors of the Transformers: State-of-the-Art Natural Language Processing paper?\n",
        "answer": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, and Quentin Lhoest",
        "source_doc": "huggingface/transformers/blob/main/README_te.md"
    },
    {
        "context": "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\n\n##### Various performance improvements\n\nThese improvements will be particularly beneficial to large applications.\n\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\n- Corrected an issue that was causing markdown to re-render infinitely.\n- Ensured that the `gr.3DModel` does re-render prematurely.\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.0.2\n\n### Patch Changes\n\n- Updated dependencies [[`61129052`](https://github.com/gradio-app/gradio/commit/61129052ed1391a75c825c891d57fa0ad6c09fc8), [`667875b2`](https://github.com/gradio-app/gradio/commit/667875b2441753e74d25bd9d3c8adedd8ede11cd), [`67265a58`](https://github.com/gradio-app/gradio/commit/67265a58027ef1f9e4c0eb849a532f72eaebde48), [`8b4eb8ca`](https://github.com/gradio-app/gradio/commit/8b4eb8cac9ea07bde31b44e2006ca2b7b5f4de36), [`37caa2e0`](https://github.com/gradio-app/gradio/commit/37caa2e0fe95d6cab8beb174580fb557904f137f)]:\n  - @gradio/client@0.2.0\n  - @gradio/upload@0.0.3\n  - @gradio/button@0.1.0",
        "question": "What is the main improvement in the markdown support in `gr.Markdown` and `gr.Dataframe`?\n",
        "answer": "The main improvement in the markdown support in `gr.Markdown` and `gr.Dataframe` is the addition of syntax highlighting and Github Flavoured Markdown, as well as more consistent markdown behavior and styling.",
        "source_doc": "gradio-app/gradio/blob/main/js/uploadbutton/CHANGELOG.md"
    },
    {
        "context": "1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 60,000 pretrained models across all modalities.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n## Why shouldn't I use transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)).\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/main/examples) are just that: examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n## Installation\n\n### With pip\n\nThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, and TensorFlow 2.6+.",
        "question": "What is the library not intended to be used as?\n",
        "answer": "The library is not intended to be used as a modular toolbox of building blocks for neural nets.",
        "source_doc": "huggingface/transformers/blob/main/README.md"
    },
    {
        "context": "Now time to see how to benefit from this integration and how to successfully use it in `transformers`!\n\n## How to use it in `transformers`\n\n### Hardware requirements\n\n8-bit tensor cores are not supported on the CPU. bitsandbytes can be run on 8-bit tensor core-supported hardware, which are Turing and Ampere GPUs (RTX 20s, RTX 30s, A40-A100, T4+). For example, Google Colab GPUs are usually NVIDIA T4 GPUs, and their latest generation of GPUs does support 8-bit tensor cores. Our demos are based on Google Colab so check them out below!\n\n### Installation\n\nJust install the latest version of the libraries using the commands below (make sure that you are using python>=3.8) and run the commands below to try out\n\n```bash\npip install accelerate\npip install bitsandbytes\npip install git+https://github.com/huggingface/transformers.git\n```\n\n### Example demos - running T5 11b on a Google Colab\n\nCheck out the Google Colab demos for running 8bit models on a BLOOM-3B model!\n\nHere is the demo for running T5-11B. The T5-11B model checkpoint is in FP32 which uses 42GB of memory and does not fit on Google Colab. With our 8-bit modules it only uses 11GB and fits easily:\n\n[![Open In Colab: T5-11b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)\n\n\nOr this demo for BLOOM-3B:\n\n[![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/HuggingFace_int8_demo.ipynb)\n\n## Scope of improvements\n\nThis approach, in our opinion, greatly improves access to very large models. With no performance degradation, it enables users with less compute to access models that were previously inaccessible.\nWe've found several areas for improvement that can be worked on in the future to make this method even better for large models!\n\n### Faster inference speed for smaller models",
        "question": "How much memory does the T5-11B model checkpoint use in FP32?\n",
        "answer": "The T5-11B model checkpoint uses 42GB of memory in FP32.",
        "source_doc": "huggingface/blog/blob/main/hf-bitsandbytes-integration.md"
    },
    {
        "context": "<iframe src=\"https://course-demos-bert-finetuned-squad.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"block dark:hidden container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nThis is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it and double-check the predictions [here](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F).\n\n<Tip>\n\n💡 Encoder-only models like BERT tend to be great at extracting answers to factoid questions like \"Who invented the Transformer architecture?\" but fare poorly when given open-ended questions like \"Why is the sky blue?\" In these more challenging cases, encoder-decoder models like T5 and BART are typically used to synthesize the information in a way that's quite similar to [text summarization](/course/chapter7/5). If you're interested in this type of *generative* question answering, we recommend checking out our [demo](https://yjernite.github.io/lfqa.html) based on the [ELI5 dataset](https://huggingface.co/datasets/eli5).\n\n</Tip>\n\n## Preparing the data[[preparing-the-data]]",
        "question": "Which deep learning libraries back Transformers?\n",
        "answer": "The three most popular deep learning libraries that back Transformers are Jax, PyTorch, and TensorFlow.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/7.mdx"
    },
    {
        "context": "[#1096]: https://github.com/huggingface/tokenizers/pull/1096\n[#1072]: https://github.com/huggingface/tokenizers/pull/1072\n[#956]: https://github.com/huggingface/tokenizers/pull/956\n[#1008]: https://github.com/huggingface/tokenizers/pull/1008\n[#1009]: https://github.com/huggingface/tokenizers/pull/1009\n[#1047]: https://github.com/huggingface/tokenizers/pull/1047\n[#1055]: https://github.com/huggingface/tokenizers/pull/1055\n[#1051]: https://github.com/huggingface/tokenizers/pull/1051\n[#1052]: https://github.com/huggingface/tokenizers/pull/1052\n[#938]: https://github.com/huggingface/tokenizers/pull/938\n[#939]: https://github.com/huggingface/tokenizers/pull/939\n[#952]: https://github.com/huggingface/tokenizers/pull/952\n[#954]: https://github.com/huggingface/tokenizers/pull/954\n[#962]: https://github.com/huggingface/tokenizers/pull/962\n[#961]: https://github.com/huggingface/tokenizers/pull/961\n[#960]: https://github.com/huggingface/tokenizers/pull/960\n[#919]: https://github.com/huggingface/tokenizers/pull/919\n[#916]: https://github.com/huggingface/tokenizers/pull/916\n[#895]: https://github.com/huggingface/tokenizers/pull/895\n[#884]: https://github.com/huggingface/tokenizers/pull/884\n[#882]: https://github.com/huggingface/tokenizers/pull/882\n[#868]: https://github.com/huggingface/tokenizers/pull/868\n[#860]: https://github.com/huggingface/tokenizers/pull/860\n[#850]: https://github.com/huggingface/tokenizers/pull/850\n[#844]: https://github.com/huggingface/tokenizers/pull/844\n[#845]: https://github.com/huggingface/tokenizers/pull/845\n[#851]: https://github.com/huggingface/tokenizers/pull/851\n[#585]: https://github.com/huggingface/tokenizers/pull/585\n[#793]: https://github.com/huggingface/tokenizers/pull/793\n[#780]: https://github.com/huggingface/tokenizers/pull/780\n[#770]: https://github.com/huggingface/tokenizers/pull/770\n[#762]: https://github.com/huggingface/tokenizers/pull/762\n[#718]: https://github.com/huggingface/tokenizers/pull/718",
        "question": "What is the number of pull requests that have been merged into the tokenizers",
        "answer": "The number of pull requests that have been merged into the tokenizers repository in the month of October 2021 is not provided in the context.\n\nFactoid question: What is the number of pull requests that have been merged into the tokenizers",
        "source_doc": "huggingface/tokenizers/blob/main/bindings/python/CHANGELOG.md"
    },
    {
        "context": "- *Observation o*: is a **partial description of the state.** In a partially observed environment.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img class=\"center\" src=\"assets/63_deep_rl_intro/mario.jpg\" alt=\"Mario\"/>\n  <figcaption>In Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.</figcaption>\n</figure>\n\nIn Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.\n\nIn Super Mario Bros, we are in a partially observed environment. We receive an observation **since we only see a part of the level.**\n\n> In reality, we use the term state in this course but we will make the distinction in implementations.\n>\n\nTo recap:\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/63_deep_rl_intro/obs_space_recap.jpg\" alt=\"Obs space recap\"/>\n</figure>\n\n### Action Space\n\nThe Action space is the set of **all possible actions in an environment.**\n\nThe actions can come from a *discrete* or *continuous space*:\n\n- *Discrete space*: the number of possible actions is **finite**.\n\n<figure class=\"image table image-center text-center m-0 w-full\">\n  <img class=\"center\" src=\"assets/63_deep_rl_intro/mario.jpg\" alt=\"Mario\"/>\n  <figcaption>Again, in Super Mario Bros, we have only 4 directions and jump possible</figcaption>\n</figure>\n\nIn Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.\n\n- *Continuous space*: the number of possible actions is **infinite**.\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/63_deep_rl_intro/self_driving_car.jpg\" alt=\"Self Driving Car\"/>\n  <figcaption>A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…\n</figcaption>\n</figure>\n\nTo recap:\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/63_deep_rl_intro/action_space.jpg\" alt=\"Recap action space\"/>\n</figcaption>\n</figure>",
        "question": "What is the set of all possible actions in an environment called?\n",
        "answer": "The action space is the set of all possible actions in an environment.",
        "source_doc": "huggingface/blog/blob/main/deep-rl-intro.md"
    },
    {
        "context": "Minimal values (no match):\n\n```python\n>>> from datasets import load_metric\n>>> math = load_metric(\"competition_math\")\n>>> references = [\"\\\\frac{1}{2}\"]\n>>> predictions = [\"3/4\"]\n>>> results = math.compute(references=references, predictions=predictions)\n>>> print(results)\n{'accuracy': 0.0}\n```\n\nPartial match:\n\n```python\n>>> from datasets import load_metric\n>>> math = load_metric(\"competition_math\")\n>>> references = [\"\\\\frac{1}{2}\",\"\\\\frac{3}{4}\"]\n>>> predictions = [\"1/5\", \"3/4\"]\n>>> results = math.compute(references=references, predictions=predictions)\n>>> print(results)\n{'accuracy': 0.5}\n```\n\n## Limitations and bias\n\nThis metric is limited to datasets with the same format as the [Mathematics Aptitude Test of Heuristics (MATH) dataset](https://huggingface.co/datasets/competition_math), and is meant to evaluate the performance of large language models at solving mathematical problems.\n\nN.B. The MATH dataset also assigns levels of difficulty to different problems, so disagregating model performance by difficulty level (similarly to what was done in the [original paper](https://arxiv.org/abs/2103.03874) can give a better indication of how a given model does on a given difficulty of math problem, compared to overall accuracy. \n\n## Citation\n\n```bibtex\n@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}\n```\n    \n## Further References \n- [MATH dataset](https://huggingface.co/datasets/competition_math)\n- [MATH leaderboard](https://paperswithcode.com/sota/math-word-problem-solving-on-math)\n- [MATH paper](https://arxiv.org/abs/2103.03874)",
        "question": "What is the output of the code when the references are [\"\\\\frac{1}{2}\",\"\\\\frac{3}{4}\"] and the predictions are [\"1/5\", \"1/5\"]?\n",
        "answer": "{'accuracy': 0.0}\n",
        "source_doc": "huggingface/datasets/blob/main/metrics/competition_math/README.md"
    },
    {
        "context": "Partial match:\n\n```python\n>>> seqeval = evaluate.load('seqeval')\n>>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n>>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n>>> results = seqeval.compute(predictions=predictions, references=references)\n>>> print(results)\n{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.8}\n```\n\n## Limitations and bias\n\nseqeval supports following IOB formats (short for inside, outside, beginning) : `IOB1`, `IOB2`, `IOE1`, `IOE2`, `IOBES`, `IOBES` (only in strict mode) and `BILOU` (only in strict mode). \n\nFor more information about IOB formats, refer to the [Wikipedia page](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) and the description of the [CoNLL-2000 shared task](https://aclanthology.org/W02-2024).\n\n\n## Citation\n\n```bibtex\n@inproceedings{ramshaw-marcus-1995-text,\n    title = \"Text Chunking using Transformation-Based Learning\",\n    author = \"Ramshaw, Lance  and\n      Marcus, Mitch\",\n    booktitle = \"Third Workshop on Very Large Corpora\",\n    year = \"1995\",\n    url = \"https://www.aclweb.org/anthology/W95-0107\",\n}\n```\n\n```bibtex\n@misc{seqeval,\n  title={{seqeval}: A Python framework for sequence labeling evaluation},\n  url={https://github.com/chakki-works/seqeval},\n  note={Software available from https://github.com/chakki-works/seqeval},\n  author={Hiroki Nakayama},\n  year={2018},\n}\n```\n    \n## Further References \n- [README for seqeval at GitHub](https://github.com/chakki-works/seqeval)\n- [CoNLL-2000 shared task](https://www.clips.uantwerpen.be/conll2002/ner/bin/conlleval.txt)",
        "question": "What is the precision of the 'PER' tag in the given results?\n",
        "answer": "The precision of the 'PER' tag in the given results is 1.0.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/seqeval/README.md"
    },
    {
        "context": "However, looking at the out-of-scope use cases in the model card:\n\n> ### Out-of-Scope Use Cases\n>\n> Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in  a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class  taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of  CLIP's performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case > currently potentially harmful.  > [source](https://huggingface.co/openai/clip-vit-base-patch32)\n\nsuggests that 'deployment' is not a good idea. Whilst the results I got are interesting, I haven't played around with the model enough yet (and haven't done anything more systematic to evaluate its performance and biases) to be confident about 'deploying' it. Another additional consideration is the target dataset itself. The images are drawn from books covering a variety of subjects and time periods. There are plenty of books which represent colonial attitudes and as a result some of the images included may represent certain groups of people in a negative way. This could potentially be a bad combo with a tool which allows any arbitrary text input to be encoded as a prompt.\n\nThere may be ways around this issue but this will require a bit more thought.\n\n## Conclusion\n\nAlthough we don't have a nice demo to show for it, we've seen how we can use `datasets` to:\n\n- load images into the new `Image` feature type\n- 'save' our work using `push_to_hub` and use this to move data between machines/sessions\n- create a `faiss` index for images that we can use to retrieve images from a text (or image) input.",
        "question": "Why is deployment not recommended for the CLIP model?\n",
        "answer": "Deployment is not recommended for the CLIP model because its safety assessment demonstrated a high need for task-specific testing, especially given the variability of its performance with different class taxonomies. Untested and unconstrained deployment of the model in any use case could currently be potentially harmful.",
        "source_doc": "huggingface/blog/blob/main/image-search-datasets.md"
    },
    {
        "context": "# we can drop the `labels` column now\ndf_ns = df_ns.drop(columns=['labels'])\n\n# we'll remove the multi-labeled samples\ndf_ns = df_ns[df_ns['label'] != 'MULTI_LABEL'].copy()\n\n# also remove the samples with label specified in remove argument if it's given\nif args['remove']:\n    df_ns = df_ns.drop(index=df_ns[df_ns['label'] == args['remove']].index)\n\nprint(‘DATA FETCHING DONE')\nprint('DATASET HAS %d SAMPLES' % (len(df_ns)))\nprint('SAVING THE PROCESSED DATASET TO: %s' % os.path.abspath(output_path))\n\ndf_ns.to_csv(output_path, index=False)\n\nprint('DONE!')\n```\n\nNice! We now have the labeled data as a csv file. Let's create a dataset repository in HuggingFace and upload the data there!\n\nIt's really simple, just click your profile picture and select `New Dataset` option. \n\n\n![](assets/59_opinion-classification-with-kili/19.png)\n\nThen enter the repository name, pick a license if you want and it's done!\n\n![](assets/59_opinion-classification-with-kili/20.png)\n\nNow we can upload the dataset from `Add file` in the `Files and versions` tab.  \n\n![](assets/59_opinion-classification-with-kili/22.png)\n\nDataset viewer is automatically available after you upload the data, we can easily check the samples!\n\n![](assets/59_opinion-classification-with-kili/24.png)\n\nIt is also possible to [upload the dataset to Hugging Face's dataset hub](https://huggingface.co/docs/datasets/upload_dataset#upload-from-python) by using `datasets` package. \n\n## Modeling\n\nLet's use active learning. We iteratively label and fine-tune the model. In each iteration, we label 50 samples in the dataset. The number of samples is shown below:\n\n![](assets/59_opinion-classification-with-kili/6.png)\n\nLet’s try out AutoTrain first:\n\nFirst, open the [AutoTrain](https://ui.autonlp.huggingface.co/)\n\n1. Create a project\n\n![](assets/59_opinion-classification-with-kili/7.png)\n\n2. We can select the dataset repository we created before or upload the dataset again. Then we need to choose the split type, I’ll leave it as Auto.",
        "question": "How many samples are used in the batch in AutoTrain?\n",
        "answer": "100 samples are used in the batch in AutoTr",
        "source_doc": "huggingface/blog/blob/main/opinion-classification-with-kili.md"
    },
    {
        "context": "### Tips and Tricks\n\nGeneral Tips:\n- since you need to run from `examples/legacy/seq2seq`, and likely need to modify code, the easiest workflow is fork transformers, clone your fork, and run `pip install -e .` before you get started.\n- try `--freeze_encoder` or `--freeze_embeds` for faster training/larger batch size.  (3hr per epoch with bs=8, see the \"xsum_shared_task\" command below)\n- `fp16_opt_level=O1` (the default works best).\n- In addition to the pytorch-lightning .ckpt checkpoint, a transformers checkpoint will be saved.\nLoad it with `BartForConditionalGeneration.from_pretrained(f'{output_dir}/best_tfmr)`.\n- At the moment, `--do_predict` does not work in a multi-gpu setting. You need to use `evaluate_checkpoint` or the `run_eval.py` code.\n- This warning can be safely ignored:\n    > \"Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: ['final_logits_bias']\"\n- Both finetuning and eval are 30% faster with `--fp16`. For that you need to [install apex](https://github.com/NVIDIA/apex#quick-start).\n- Read scripts before you run them!",
        "question": "How to safely ignore a warning in finetuning and eval?\n",
        "answer": "The warning \"Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: ['final_logits_bias']\" can be safely ignored.",
        "source_doc": "huggingface/transformers/blob/main/examples/legacy/seq2seq/README.md"
    },
    {
        "context": "`@gradio/imageeditor`",
        "question": "What is the name of the gradio library for image editing?\n",
        "answer": "The name of the gradio library for image editing is `@gradio/imageeditor`.",
        "source_doc": "gradio-app/gradio/blob/main/js/imageeditor/README.md"
    },
    {
        "context": "@gradio/upload\n\n## 0.5.6\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144) - Improve source selection UX.  Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 0.5.5\n\n### Patch Changes\n\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\n  - @gradio/client@0.9.2\n  - @gradio/upload@0.5.5\n\n## 0.5.4\n\n### Fixes\n\n- [#6525](https://github.com/gradio-app/gradio/pull/6525) [`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a) - Fixes Drag and Drop for Upload. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n\n## 0.5.3\n\n### Fixes\n\n- [#6709](https://github.com/gradio-app/gradio/pull/6709) [`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88) - Remove progress animation on streaming. Thanks [@aliabid94](https://github.com/aliabid94)!\n\n## 0.5.2\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\n  - @gradio/icons@0.3.1\n  - @gradio/atoms@0.3.1\n  - @gradio/upload@0.5.2\n\n## 0.5.1\n\n### Patch Changes\n\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\n  - @gradio/client@0.8.2\n  - @gradio/upload@0.5.1\n\n## 0.5.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\n\nA brand new component, completely separate from `Image` that provides simple editing capabilities.",
        "question": "What is the new component in gradio 0.5.0 for multi-line text input?\n",
        "answer": "The new component in gradio 0.5.0 for multi-line text input is `Textarea`.\n\n#### New `Number` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\n\nA brand new component, completely separate from `Slider",
        "source_doc": "gradio-app/gradio/blob/main/js/upload/CHANGELOG.md"
    },
    {
        "context": "However, it turns out that the traditional model optimization methods, such as post-training 8-bit quantization, do not work for this model. There are two main reasons for that. First, pixel-level prediction models, such as semantic segmentation, super-resolution, etc., are one of the most complicated in terms of model optimization because of the complexity of the task, so tweaking model parameters and the structure breaks the results in numerous ways. The second reason is that the model has a lower level of redundancy because it accommodates a lot of information while being trained on [hundreds of millions of samples](https://laion.ai/blog/laion-5b/). That is why researchers have to employ more sophisticated quantization methods to preserve the accuracy after optimization. For example, Qualcomm used the layer-wise Knowledge Distillation method ([AdaRound](https://arxiv.org/abs/2004.10568)) to [quantize](https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android) Stable Diffusion models. It means that model tuning after quantization is required, anyway. If so, why not just use [Quantization-Aware Training](https://arxiv.org/abs/1712.05877) (QAT) which can tune the model and quantization parameters simultaneously in the same way the source model is trained? Thus, we tried this approach in our work using [NNCF](https://github.com/openvinotoolkit/nncf), [OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html), and [Diffusers](https://github.com/huggingface/diffusers) and coupled it with [Token Merging](https://arxiv.org/abs/2210.09461).\n\n## Optimization workflow\n\nWe usually start the optimization of a model after it's trained. Here, we start from a [model](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-en) fine-tuned on the [Pokemons dataset](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) containing images of Pokemons and their text descriptions.",
        "question": "Which method was coupled with Quantization-Aware Training (QAT) in the work?\n",
        "answer": "Token Merging was coupled with Quantization-Aware Training (QAT) in the work.",
        "source_doc": "huggingface/blog/blob/main/train-optimize-sd-intel.md"
    },
    {
        "context": "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<!---\nA useful guide for English-Traditional Chinese translation of Hugging Face documentation\n- Add space around English words and numbers when they appear between Chinese characters. E.g., 共 100 多種語言; 使用 transformers 函式庫。\n- Use square quotes, e.g.,「引用」\n- Some of terms in the file can be found at National Academy for Educational Research (https://terms.naer.edu.tw/), an official website providing bilingual translations between English and Traditional Chinese.\n\nDictionary\n\nAPI: API (不翻譯）\nadd: 加入\ncheckpoint: 檢查點\ncode: 程式碼\ncommunity: 社群\nconfidence: 信賴度\ndataset: 資料集\ndocumentation: 文件\nexample: 基本翻譯為「範例」，或依語意翻為「例子」\nfinetune: 微調\nHugging Face: Hugging Face（不翻譯）\nimplementation: 實作\ninference: 推論\nlibrary: 函式庫\nmodule: 模組\nNLP/Natural Language Processing: 以 NLP 出現時不翻譯，以 Natural Language Processing 出現時翻譯為自然語言處理\nonline demos: 線上Demo\n\bpipeline: pipeline（不翻譯）\npretrained/pretrain: 預訓練\nPython data structures (e.g., list, set, dict): 翻譯為串列，集合，字典，並用括號標註原英文\nrepository: repository（不翻譯）\nsummary: 概覽\ntoken-: token-（不翻譯）\nTrainer: Trainer（不翻譯）\ntransformer: transformer（不翻譯）\ntutorial: 教學\nuser: 使用者\n-->",
        "question": "What is the license of Hugging Face?\n",
        "answer": "The Hugging Face license is the Apache License, Version 2.0.",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "The `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens 👀 `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available",
        "question": "What is the name of the model used in the context?\n",
        "answer": "The name of the model used in the context is Generator.",
        "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/create-your-own-friends-with-a-gan.md"
    },
    {
        "context": "We have turned these off for now, but in order to see how to set up remote executors tools yourself,\nwe recommend reading the [custom tool guide](./custom_tools).\n\n### What's happening here? What are tools, and what are agents?\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/diagram.png\">\n\n#### Agents\n\nThe \"agent\" here is a large language model, and we're prompting it so that it has access to a specific set of tools.\n\nLLMs are pretty good at generating small samples of code, so this API takes advantage of that by prompting the \nLLM gives a small sample of code performing a task with a set of tools. This prompt is then completed by the \ntask you give your agent and the description of the tools you give it. This way it gets access to the doc of the \ntools you are using, especially their expected inputs and outputs, and can generate the relevant code.\n\n#### Tools\n\nTools are very simple: they're a single function, with a name, and a description. We then use these tools' descriptions \nto prompt the agent. Through the prompt, we show the agent how it would leverage tools to perform what was \nrequested in the query.\n\nThis is using brand-new tools and not pipelines, because the agent writes better code with very atomic tools. \nPipelines are more refactored and often combine several tasks in one. Tools are meant to be focused on\none very simple task only.\n\n#### Code-execution?!\n\nThis code is then executed with our small Python interpreter on the set of inputs passed along with your tools. \nWe hear you screaming \"Arbitrary code execution!\" in the back, but let us explain why that is not the case.\n\nThe only functions that can be called are the tools you provided and the print function, so you're already \nlimited in what can be executed. You should be safe if it's limited to Hugging Face tools.",
        "question": "What is the function of the agent in this context?\n",
        "answer": "The agent in this context is a large language model that is prompted to generate code for a specific task with a set of tools. It uses the tools' descriptions to prompt the agent and show it how to leverage tools to perform the requested task.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/transformers_agents.md"
    },
    {
        "context": "#### Audio[[pytorch-audio]]",
        "question": "What is the name of the library for audio processing in pytorch?\n",
        "answer": "pytorch-audio",
        "source_doc": "huggingface/transformers/blob/main/notebooks/README.md"
    },
    {
        "context": ". We just have to make sure we change the B- labels to their I- counterparts for tokens that are inside (but not at the beginning) of a word. The special tokens get a label of -100, which is how we tell the Transformer loss functions to ignore them when computing the loss. The code is then pretty straightforward, we write a function that shifts the labels for tokens that are inside a word (that you can customize) and use it when generating the labels for each token. Once that function to create our labels is written, we can preprocess the whole dataset using the map function. With the option batched=True, we unleash the speed of out fast tokenizers. The last problem comes when we need to create a batch. Unless you changed the preprocessing function to apply some fixed padding, we will get sentences of various lengths, which we need to pad to the same length. The padding needs to be applied to the inputs as well as the labels, since we should have one label per token. Again, -100 indicates the labels that should be ignored for the loss computation. This is all done for us by the DataCollatorForTokenClassification, which you can use in PyTorch or TensorFlow. With all of this, you are either ready to send your data and this data collator to the Trainer, or to use the to_tf_dataset method and use the fit method of your model.",
        "question": "What is the label for tokens that should be ignored for the loss computation?\n",
        "answer": "The label for tokens that should be ignored for the loss computation is -100.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter7/02_token-classification-processing.md"
    },
    {
        "context": "We'll first load the model from the Hub using `SegformerForSemanticSegmentation.from_pretrained()`.\n\n```python\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(f\"{hf_username}/{hub_model_id}\")\n```\n\nNext, we'll load an image from our test dataset.\n\n\n```python\nimage = test_ds[0]['pixel_values']\ngt_seg = test_ds[0]['label']\nimage\n```\n\nTo segment this test image, we first need to prepare the image using the image processor. Then we forward it through the model.\n\nWe also need to remember to upscale the output logits to the original image size. In order to get the actual category predictions, we just have to apply an `argmax` on the logits.\n\n\n```python\nfrom torch import nn\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n\n# First, rescale logits to original image size\nupsampled_logits = nn.functional.interpolate(\n    logits,\n    size=image.size[::-1], # (height, width)\n    mode='bilinear',\n    align_corners=False\n)\n\n# Second, apply argmax on the class dimension\npred_seg = upsampled_logits.argmax(dim=1)[0]\n```\n\nNow it's time to display the result. We'll display the result next to the ground-truth mask.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(1,1,1,1)\" alt=\"SegFormer prediction vs the ground truth\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/56_fine_tune_segformer/output.png\"></medium-zoom>\n</figure>\n\nWhat do you think? Would you send our pizza delivery robot on the road with this segmentation information?\n\nThe result might not be perfect yet, but we can always expand our dataset to make the model more robust. We can now also go train a larger SegFormer model, and see how it stacks up.",
        "question": "What is the function of the SegformerForSemanticSegmentation model in the context?\n",
        "answer": "The SegformerForSemanticSegmentation model is used to segment an image by taking in an image as input and outputting logits, which are then upsampled and passed through an argmax function to get the actual category predictions.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-segformer.md"
    },
    {
        "context": "There are three ways to instantiate a DETR model (depending on what you prefer):\n\nOption 1: Instantiate DETR with pre-trained weights for entire model\n```py\n>>> from transformers import DetrForObjectDetection\n\n>>> model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n```\n\nOption 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n```py\n>>> from transformers import DetrConfig, DetrForObjectDetection\n\n>>> config = DetrConfig()\n>>> model = DetrForObjectDetection(config)\n```\nOption 3: Instantiate DETR with randomly initialized weights for backbone + Transformer\n```py\n>>> config = DetrConfig(use_pretrained_backbone=False)\n>>> model = DetrForObjectDetection(config)\n```\n\nAs a summary, consider the following table:",
        "question": "How to instantiate DETR with randomly initialized weights for backbone + Transformer?\n",
        "answer": "You can instantiate DETR with randomly initialized weights for backbone + Transformer by first creating a `DetrConfig` object with the `use_pretrained_backbone` parameter set to `False`, and then using the `DetrForObjectDetection` class to initialize the model, passing the `DetrConfig` object as an argument.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md"
    },
    {
        "context": "Gradio Demo: blocks_speech_text_sentiment\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\nfrom transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```",
        "question": "What is the name of the pipeline used for automatic speech recognition?\n",
        "answer": "The name of the pipeline used for automatic speech recognition is \"automatic-speech-recognition\".",
        "source_doc": "gradio-app/gradio/blob/main/demo/blocks_speech_text_sentiment/run.ipynb"
    },
    {
        "context": "Once the fine-tuning is done, share the LoRA parameters with the community like so:\n\n```python\nrepo_name = f\"sayakpaul/{model_name}-finetuned-lora-food101\"\nlora_model.push_to_hub(repo_name)\n```\n\nWhen calling [`~transformers.PreTrainedModel.push_to_hub`] on the `lora_model`, only the LoRA parameters along with any modules specified in `modules_to_save`\nare saved. Take a look at the [trained LoRA parameters](https://huggingface.co/sayakpaul/vit-base-patch16-224-in21k-finetuned-lora-food101/blob/main/adapter_model.bin).\nYou'll see that it's only 2.6 MB! This greatly helps with portability, especially when using a very large model to fine-tune (such as [BLOOM](https://huggingface.co/bigscience/bloom)).\n\nNext, let's see how to load the LoRA updated parameters along with our base model for inference. When you wrap a base model\nwith `PeftModel`, modifications are done *in-place*. To mitigate any concerns that might stem from in-place modifications,\ninitialize the base model just like you did earlier and construct the inference model.\n\n```python\nfrom peft import PeftConfig, PeftModel\n\n\nconfig = PeftConfig.from_pretrained(repo_name)\nmodel = AutoModelForImageClassification.from_pretrained(\n    config.base_model_name_or_path,\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n)\n# Load the LoRA model\ninference_model = PeftModel.from_pretrained(model, repo_name)\n```\n\nLet's now fetch an example image for inference.\n\n```python\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\" alt=\"image of beignets\"/>\n</div>\n\nFirst, instantiate an `image_processor` from the underlying model repo.",
        "question": "How to instantiate an `image_processor` from the underlying model repo?\n",
        "answer": "You can instantiate an `image_processor` from the underlying model repo by using the `from_pretrained` method of the `AutoImageProcessor` class, like so:\n\n```python\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(config.base_model_name_or_path)\n```",
        "source_doc": "huggingface/peft/blob/main/docs/source/task_guides/image_classification_lora.md"
    },
    {
        "context": "### Output Values\n- `score`: BLEU score\n- `counts`: Counts\n- `totals`: Totals\n- `precisions`: Precisions\n- `bp`: Brevity penalty\n- `sys_len`: predictions length\n- `ref_len`: reference length\n\nThe output is in the following format:\n```python\n{'score': 39.76353643835252, 'counts': [6, 4, 2, 1], 'totals': [10, 8, 6, 4], 'precisions': [60.0, 50.0, 33.333333333333336, 25.0], 'bp': 1.0, 'sys_len': 10, 'ref_len': 7}\n```\nThe score can take any value between `0.0` and `100.0`, inclusive.\n\n#### Values from Popular Papers\n\n\n### Examples\n\n```python\n>>> predictions = [\"hello there general kenobi\", \n...                 \"on our way to ankh morpork\"]\n>>> references = [[\"hello there general kenobi\", \"hello there !\"],\n...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n>>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n>>> results = sacrebleu.compute(predictions=predictions, \n...                             references=references)\n>>> print(list(results.keys()))\n['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n>>> print(round(results[\"score\"], 1))\n39.8\n```\n\n## Limitations and Bias\nBecause what this metric calculates is BLEU scores, it has the same limitations as that metric, except that sacreBLEU is more easily reproducible.\n\n## Citation\n```bibtex\n@inproceedings{post-2018-call,\n    title = \"A Call for Clarity in Reporting {BLEU} Scores\",\n    author = \"Post, Matt\",\n    booktitle = \"Proceedings of the Third Conference on Machine Translation: Research Papers\",\n    month = oct,\n    year = \"2018\",\n    address = \"Belgium, Brussels\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W18-6319\",\n    pages = \"186--191\",\n}\n```\n\n## Further References\n- See the [sacreBLEU README.md file](https://github.com/mjpost/sacreBLEU) for more information.",
        "question": "What is the BLEU score for the given predictions and references?\n",
        "answer": "The BLEU score for the given predictions and references is 39.8.",
        "source_doc": "huggingface/datasets/blob/main/metrics/sacrebleu/README.md"
    },
    {
        "context": "## DLA [[dla.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py)]\n* Paper: https://arxiv.org/abs/1707.06484\n* Code: https://github.com/ucbdrive/dla\n\n## Dual-Path Networks [[dpn.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dpn.py)]\n* Paper: `Dual Path Networks` - https://arxiv.org/abs/1707.01629\n* My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained\n* Reference code: https://github.com/cypw/DPNs\n\n## GPU-Efficient Networks [[byobnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/byobnet.py)]\n* Paper: `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n* Reference code: https://github.com/idstcv/GPU-Efficient-Networks\n\n## HRNet [[hrnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/hrnet.py)]\n* Paper: `Deep High-Resolution Representation Learning for Visual Recognition` - https://arxiv.org/abs/1908.07919\n* Code: https://github.com/HRNet/HRNet-Image-Classification\n\n## Inception-V3 [[inception_v3.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v3.py)]\n* Paper: `Rethinking the Inception Architecture for Computer Vision` - https://arxiv.org/abs/1512.00567\n* Code: https://github.com/pytorch/vision/tree/master/torchvision/models\n\n## Inception-V4 [[inception_v4.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v4.py)]\n* Paper: `Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning` - https://arxiv.org/abs/1602.07261\n* Code: https://github.com/Cadene/pretrained-models.pytorch\n* Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets",
        "question": "What is the name of the paper that introduced the Inception-V4 architecture?\n",
        "answer": "The name of the paper that introduced the Inception-V4 architecture is 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning'.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models.md"
    },
    {
        "context": "- How does this impact reproducibility and performance? To align settings, we record the original query, response, and rewards from [https://github.com/openai/lm-human-preferences](https://github.com/openai/lm-human-preferences) and save them in [https://huggingface.co/datasets/vwxyzjn/lm-human-preferences-debug/tree/main](https://huggingface.co/datasets/vwxyzjn/lm-human-preferences-debug/tree/main). I also record the metrics of the first two epochs of training with TF1’s `AdamOptimizer` optimizer as the ground truth.  Below are some key metrics:\n    \n    \n    |  | OAI’s TF1 Adam | PyTorch’s Adam | Our custom Tensorflow-style Adam |\n    | --- | --- | --- | --- |\n    | policy/approxkl | 0.00037167023 | 0.0023672834504395723 | 0.000374998344341293 |\n    | policy/clipfrac | 0.0045572915 | 0.02018229104578495 | 0.0052083334885537624 |\n    | ratio_mean | 1.0051285 | 1.0105520486831665 | 1.0044583082199097 |\n    | ratio_var | 0.0007716546 | 0.005374275613576174 | 0.0007942612282931805 |\n    | ratio_max | 1.227216 | 1.8121057748794556 | 1.250215768814087 |\n    | ratio_min | 0.7400441 | 0.4011387825012207 | 0.7299948930740356 |\n    | logprob_diff_mean | 0.0047487603 | 0.008101251907646656 | 0.004073789343237877 |\n    | logprob_diff_var | 0.0007207897 | 0.004668936599045992 | 0.0007334011606872082 |\n    | logprob_diff_max | 0.20474821 | 0.594489574432373 | 0.22331619262695312 |\n    | logprob_diff_min | -0.30104542 | -0.9134478569030762 | -0.31471776962280273 |\n- **PyTorch’s `Adam` produces a more aggressive update** for some reason. Here are some evidence:\n    - **PyTorch’s `Adam`'s `logprob_diff_var`** **is 6x higher**. Here `logprobs_diff = new_logprobs - logprobs` is the difference between the log probability of tokens between the initial and current policy after two epochs of training. Having a larger `logprob_diff_var` means the scale of the log probability changes is larger than that in OAI’s TF1 Adam.",
        "question": "What is the variance of logprob\\_diff in PyTorch's Adam?\n",
        "answer": "The variance of logprob\\_diff in PyTorch's Adam is 0.005374275613576174.",
        "source_doc": "huggingface/blog/blob/main/the_n_implementation_details_of_rlhf_with_ppo.md"
    },
    {
        "context": "Tip: You don't have to run dev mode from your custom component directory. The first argument to `dev` mode is the path to the directory. By default it uses the current directory.\n\n## 3. build\n\nOnce you are satisfied with your custom component's implementation, you can `build` it to use it outside of the development server.\n\nFrom your component directory, run:\n\n```bash\ngradio cc build\n```\n\nThis will create a `tar.gz` and `.whl` file in a `dist/` subdirectory.\nIf you or anyone installs that `.whl` file (`pip install <path-to-whl>`) they will be able to use your custom component in any gradio app!\n\n## 4. publish\n\nRight now, your package is only available on a `.whl` file on your computer.\nYou can share that file with the world with the `publish` command!\n\nSimply run the following command from your component directory:\n\n```bash\ngradio cc publish\n```\n\nThis will guide you through the following process:\n\n1. Upload your distribution files to PyPi. This is optional. If you decide to upload to PyPi, you will need a PyPI username and password. You can get one [here](https://pypi.org/account/register/).\n2. Upload a demo of your component to hugging face spaces. This is also optional.\n\n\nHere is an example of what publishing looks like:\n\n<video autoplay muted loop>\n  <source src=\"https://gradio-builds.s3.amazonaws.com/assets/text_with_attachments_publish.mov\" type=\"video/mp4\" />\n</video>\n\n\n## Conclusion\n\nNow that you know the high-level workflow of creating custom components, you can go in depth in the next guides!\nAfter reading the guides, check out this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub so you can learn from other's code.",
        "question": "How do I build a gradio custom component?\n",
        "answer": "From your component directory, run `gradio cc build` to create a `tar.gz` and `.whl` file in a `dist/` subdirectory.",
        "source_doc": "gradio-app/gradio/blob/main/guides/05_custom-components/01_custom-components-in-five-minutes.md"
    },
    {
        "context": "Metrics:\n      Top 1 Accuracy: 76.95%\n      Top 5 Accuracy: 93.43%\n- Name: regnetx_032\n  In Collection: RegNetX\n  Metadata:\n    FLOPs: 4082555904\n    Parameters: 15300000\n    File Size: 61509573\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Grouped Convolution\n    - ReLU\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA V100 GPUs\n    ID: regnetx_032\n    Epochs: 100\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 512\n    Image Size: '224'\n    Weight Decay: 5.0e-05\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L367\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_032-ed0c7f7e.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78.15%\n      Top 5 Accuracy: 94.09%\n- Name: regnetx_040\n  In Collection: RegNetX\n  Metadata:\n    FLOPs: 5095167744\n    Parameters: 22120000\n    File Size: 88844824\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Grouped Convolution\n    - ReLU\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA V100 GPUs\n    ID: regnetx_040\n    Epochs: 100\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 512\n    Image Size: '224'\n    Weight Decay: 5.0e-05\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L373",
        "question": "What is the FLOPs of regnetx_040?\n",
        "answer": "The FLOPs of regnetx_040 is 5095167744.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/regnetx.md"
    },
    {
        "context": "|      |                                                                            |[TheBloke/qCammel-70-x-GPTQ](https://huggingface.co/TheBloke/qCammel-70-x-GPTQ)|24          |2                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/qCammel-70-x-GPTQ/blob/main/LICENSE.txt)             |                                                                                                                     |                                                                                   |\n|      |                                                                            |[deerslab/llama-7b-embeddings](https://huggingface.co/deerslab/llama-7b-embeddings)|23          |5                        |llama-license                                                                                 |https://huggingface.co/deerslab/llama-7b-embeddings/blob/main/LICENSE                         |[LICENSE](https://huggingface.co/deerslab/llama-7b-embeddings/blob/main/LICENSE)                   |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the TheBloke/qCammel-70-x-GPTQ model?\n",
        "answer": "The license for the TheBloke/qCammel-70-x-GPTQ model is the llama-2-community-license.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "---\n\nTo delete your endpoint you can run. \n\n```python\npredictor.delete_endpoint()\n```\n\n## Conclusion\n\nWe successfully managed to deploy `GPT-J`, a 6 billion parameter language model created by [EleutherAI](https://www.eleuther.ai/), using Amazon SageMaker. We reduced the model load time from 3.5 minutes down to 8 seconds to be able to run scalable, reliable inference. \n\nRemember that using `torch.save()` and `torch.load()` can create incompatibility issues. If you want to learn more about scaling out your Amazon SageMaker Endpoints check out my other blog post: [“MLOps: End-to-End Hugging Face Transformers with the Hub & SageMaker Pipelines”](https://www.philschmid.de/mlops-sagemaker-huggingface-transformers).\n\n---\n\nThanks for reading! If you have any question, feel free to contact me, through [Github](https://github.com/huggingface/transformers), or on the [forum](https://discuss.huggingface.co/c/sagemaker/17). You can also connect with me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).",
        "question": "How can I delete my endpoint using Python?\n",
        "answer": "You can delete your endpoint using Python by running `predictor.delete_endpoint()`.",
        "source_doc": "huggingface/blog/blob/main/gptj-sagemaker.md"
    },
    {
        "context": "3. Implement the `_from_pretrained` method:\n\n```python\nclass PyTorchModelHubMixin(ModelHubMixin):\n   (...)\n\n   @classmethod # Must be a classmethod!\n   def _from_pretrained(\n      cls,\n      *,\n      model_id: str,\n      revision: str,\n      cache_dir: str,\n      force_download: bool,\n      proxies: Optional[Dict],\n      resume_download: bool,\n      local_files_only: bool,\n      token: Union[str, bool, None],\n      map_location: str = \"cpu\", # additional argument\n      strict: bool = False, # additional argument\n      **model_kwargs,\n   ):\n      \"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\n      if os.path.isdir(model_id): # Can either be a local directory\n         print(\"Loading weights from local directory\")\n         model_file = os.path.join(model_id, \"pytorch_model.bin\")\n      else: # Or a model on the Hub\n         model_file = hf_hub_download( # Download from the hub, passing same input args\n            repo_id=model_id,\n            filename=\"pytorch_model.bin\",\n            revision=revision,\n            cache_dir=cache_dir,\n            force_download=force_download,\n            proxies=proxies,\n            resume_download=resume_download,\n            token=token,\n            local_files_only=local_files_only,\n         )\n\n      # Load model and return - custom logic depending on your framework\n      model = cls(**model_kwargs)\n      state_dict = torch.load(model_file, map_location=torch.device(map_location))\n      model.load_state_dict(state_dict, strict=strict)\n      model.eval()\n      return model\n```\n\nAnd that's it! Your library now enables users to upload and download files to and from the Hub.\n\n## Quick comparison",
        "question": "What is the filename of the pytorch model in the hub?\n",
        "answer": "pytorch_model.bin",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/integrations.md"
    },
    {
        "context": ". Once we know how many training steps we're taking, we just pass all that information to the scheduler and we're ready to go. What does the polynomial decay schedule look like? With default options, it's actually just a linear schedule, so it looks like this - it starts at 5e-5, which means 5 times ten to the minus 5, and then decays down at a constant rate until it hits zero right at the very end of training. So why do they call it polynomial and not linear? Because if you tweak the options, you can get a higher-order decay schedule, but there's no need to do that right now. Now, how do we use our learning rate schedule? Easy, we just pass it to Adam! You'll notice the first time when we compiled the model, we just passed it the string \"adam\". Keras recognizes the names of common optimizers and loss functions if you pass them as strings, so it saves time to do that if you only want the default settings. But we're professional machine learners now, with our very own learning rate schedule, so we have to do things properly. So first we import the optimizer, then we initialize it with our scheduler, and then we compile the model using the new optimizer, and whatever loss function you want - this will be sparse categorical crossentropy if you're following along from the fine-tuning video. And now we have a high-performance model, ready to go. All that remains is to fit the model just like we did before! Remember, because we compiled the model with the new optimizer with the new learning rate schedule, we don't need to change anything here. We just call fit again, with exactly the same command as before, but now we get beautiful training with a nice, smooth learning rate decay.",
        "question": "What is the starting learning rate of the polynomial decay schedule?\n",
        "answer": "The starting learning rate of the polynomial decay schedule is 5e-5.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter3/03d_keras-learning-rate.md"
    },
    {
        "context": "### Inputs\n- **predictions** (`list` of `int`): Predicted class labels.\n- **references** (`list` of `int`): Actual class labels.\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`. If `average` is `None`, it should be the label order. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n- **pos_label** (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n- **average** (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n    - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n    - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n    - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n    - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n    - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n- **sample_weight** (`list` of `float`): Sample weights Defaults to None.",
        "question": "What is the default value of the 'average' parameter?\n",
        "answer": "The default value of the 'average' parameter is 'binary'.",
        "source_doc": "huggingface/datasets/blob/main/metrics/precision/README.md"
    },
    {
        "context": "### Faster inference speed for smaller models\n\nAs we have seen in the [the benchmarking section](#is-it-faster-than-native-models), we could improve the runtime speed for small model (<=6B parameters) by a factor of almost 2x. However, while the inference speed is robust for large models like BLOOM-176B there are still improvements to be had for small models. We already identified the issues and likely recover same performance as fp16, or get small speedups. You will see these changes being integrated within the next couple of weeks.\n\n### Support for Kepler GPUs (GTX 1080 etc)\n\nWhile we support all GPUs from the past four years, some old GPUs like GTX 1080 still see heavy use. While these GPUs do not have Int8 tensor cores, they do have Int8 vector units (a kind of \"weak\" tensor core). As such, these GPUs can also experience Int8 acceleration. However, it requires a entire different stack of software for fast inference. While we do plan to integrate support for Kepler GPUs to make the LLM.int8() feature more widely available, it will take some time to realize this due to its complexity.\n\n### Saving 8-bit state dicts on the Hub\n\n8-bit state dicts cannot currently be loaded directly into the 8-bit model after being pushed on the Hub. This is due to the fact that the statistics (remember `weight.CB` and `weight.SCB`) computed by the model are not currently stored or taken into account inside the state dict, and the `Linear8bitLt` module does not support this feature yet.\nWe think that having the ability to save that and push it to the Hub might contribute to greater accessibility.\n### CPU support\n\nCPU devices do not support 8-bit cores, as was stated at the beginning of this blogpost. Can we, however, get past that? Running this module on CPUs would also significantly improve usability and accessibility.\n\n### Scaling up on other modalities",
        "question": "Can small models (<=6B parameters) improve their runtime speed by a factor of almost 2x?\n",
        "answer": "Yes, small models (<=6B parameters) can improve their runtime speed by a factor of almost 2x.",
        "source_doc": "huggingface/blog/blob/main/hf-bitsandbytes-integration.md"
    },
    {
        "context": "### What is a SavedModel?\n\nA SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not require the original source of the model to be run, which makes it useful for sharing or deploying with any backend that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a SavedModel is represented as such:\n```\nsavedmodel\n    /assets\n        -> here the needed assets by the model (if any)\n    /variables\n        -> here the model checkpoints that contains the weights\n   saved_model.pb -> protobuf file representing the model graph\n```\n\n### How to install TensorFlow Serving?\n\nThere are three ways to install and use TensorFlow Serving:\n- through a Docker container, \n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). \n\nTo make things easier and compliant with all the existing OS, we will use Docker in this tutorial.\n\n### How to create a SavedModel?\n\nSavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n\n1. The sequence length can be modified freely between runs.\n2. All model inputs are available for inference.\n3. `hidden states` or `attention` are now grouped into a single output when returning them with `output_hidden_states=True` or `output_attentions=True`.\n\nBelow, you can find the inputs and outputs representations of a `TFBertForSequenceClassification` saved as a TensorFlow SavedModel:",
        "question": "What is a SavedModel in TensorFlow?\n",
        "answer": "A SavedModel in TensorFlow is a standalone model that includes its weights and architecture. It does not require the original source of the model to be run and can be shared or deployed with any backend that supports reading a SavedModel. The internal structure of a SavedModel includes assets, model checkpoints with weights, and a protobuf file representing the model graph.",
        "source_doc": "huggingface/blog/blob/main/tf-serving.md"
    },
    {
        "context": "The example has copious notes and is self-documenting.\n\nMake sure to:\n\n1. disable CPU offload if you have enough GPU memory (since it slows things down)\n2. enable bf16 if you own an Ampere or a newer GPU to make things faster. If you don't have that hardware you may enable fp16 as long as you don't use any model that was pre-trained in bf16 mixed precision (such as most t5 models). These usually overflow in fp16 and you will see garbage as output.\n\n```python\n#!/usr/bin/env python\n\n# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model\n# into a single GPU\n#\n# 1. Use 1 GPU with CPU offload\n# 2. Or use multiple GPUs instead\n#\n# First you need to install deepspeed: pip install deepspeed\n#\n# Here we use a 3B \"bigscience/T0_3B\" model which needs about 15GB GPU RAM - so 1 largish or 2\n# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.\n#\n# To use a larger model like \"bigscience/T0\" which needs about 50GB, unless you have an 80GB GPU -\n# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to\n# process multiple inputs at once.\n#\n# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n# run faster if you don't want offload to CPU - so disable that section then.\n#\n# To deploy on 1 gpu:\n#\n# deepspeed --num_gpus 1 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=1 t0.py\n#\n# To deploy on 2 gpus:\n#\n# deepspeed --num_gpus 2 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=2 t0.py\n\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\nfrom transformers.integrations import HfDeepSpeedConfig\nimport deepspeed\nimport os\nimport torch",
        "question": "How should one disable CPU offload if they have enough GPU memory?\n",
        "answer": "To disable CPU offload if you have enough GPU memory, you should modify the provided script by disabling the section related to CPU memory offloading.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/main_classes/deepspeed.md"
    },
    {
        "context": "### Resources and citation\n\n- More information on our labeling instructions can be found [here](https://docs.google.com/document/d/1c5-96Lj-UH4lzKjLvJ_MRQaVMjtoEXTYA4dvoAYVCHc/edit?usp=sharing).\n\nHave a model that you want GPT-4 or human annotators to evaluate? Drop us a note on [the leaderboard discussions](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard_internal/discussions).\n\n```\n@article{rajani2023llm_labels,\n  author = {Rajani, Nazneen, and Lambert, Nathan and Han, Sheon and Wang, Jean and Nitski, Osvald and Beeching, Edward and Tunstall, Lewis},\n  title = {Can foundation models label data like humans?},\n  journal = {Hugging Face Blog},\n  year = {2023},\n  note = {https://huggingface.co/blog/llm-v-human-data},\n}\n```\n\n_Thanks to [Joao](https://twitter.com/_joaogui1) for pointing out a typo in a table._",
        "question": "Who is the first author of the article \"Can foundation models label data like humans?\"?\n",
        "answer": "Nazneen Rajani",
        "source_doc": "huggingface/blog/blob/main/llm-leaderboard.md"
    },
    {
        "context": "This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). \nThat repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, it \ncontains a hybrid model parallel approach using \"tensor parallel\" and \"pipeline parallel\" techniques.\n\n## Usage tips\n\nWe have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m) checkpoints\nfor use to evaluate or finetuning downstream tasks.\n\nTo access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)\nRegistry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).\n\nAlternatively, you can directly download the checkpoints using:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\nmegatron_gpt2_345m_v0_0.zip\n```\n\nOnce you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will easily\nbe loaded by Hugging Face Transformers GPT2 implementation.\n\nThe following command allows you to do the conversion. We assume that the folder `models/megatron_gpt2` contains\n`megatron_gpt2_345m_v0_0.zip` and that the command is run from that folder:\n\n```bash\npython3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip\n```\n\n<Tip> \n\n MegatronGPT2 architecture is the same as OpenAI GPT-2 . Refer to [GPT-2 documentation](gpt2) for information on \n configuration classes and their parameters.  \n\n </Tip>",
        "question": "How can I download the pretrained GPT2-345M checkpoints?\n",
        "answer": "You can download the checkpoints using the command `wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_gpt2_345m_v0_0.zip`.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/megatron_gpt2.md"
    },
    {
        "context": "Top 1 Accuracy: 81.61%\n      Top 5 Accuracy: 96.04%\n- Name: ssl_resnext50_32x4d\n  In Collection: SSL ResNext\n  Metadata:\n    FLOPs: 5472648192\n    Parameters: 25030000\n    File Size: 100428550\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - YFCC-100M\n    Training Resources: 64x GPUs\n    ID: ssl_resnext50_32x4d\n    LR: 0.0015\n    Epochs: 30\n    Layers: 50\n    Crop Pct: '0.875'\n    Batch Size: 1536\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/resnet.py#L914\n  Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80.3%\n      Top 5 Accuracy: 95.41%\n-->",
        "question": "What is the accuracy of ssl\\_resnext50\\_32x4d on ImageNet?\n",
        "answer": "The top 1 accuracy of ssl\\_resnext50\\_32x4d on ImageNet is 80.3%.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/ssl-resnext.md"
    },
    {
        "context": "## Resources\n\n- [Script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh) to fine-tune pegasus\n  on the XSUM dataset. Data download instructions at [examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md).\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## PegasusConfig\n\n[[autodoc]] PegasusConfig\n\n## PegasusTokenizer\n\nwarning: `add_tokens` does not work at the moment.\n\n[[autodoc]] PegasusTokenizer\n\n## PegasusTokenizerFast\n\n[[autodoc]] PegasusTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## PegasusModel\n\n[[autodoc]] PegasusModel\n    - forward\n\n## PegasusForConditionalGeneration\n\n[[autodoc]] PegasusForConditionalGeneration\n    - forward\n\n## PegasusForCausalLM\n\n[[autodoc]] PegasusForCausalLM\n    - forward\n\n</pt>\n<tf>\n\n## TFPegasusModel\n\n[[autodoc]] TFPegasusModel\n    - call\n\n## TFPegasusForConditionalGeneration\n\n[[autodoc]] TFPegasusForConditionalGeneration\n    - call\n\n</tf>\n<jax>\n\n## FlaxPegasusModel\n\n[[autodoc]] FlaxPegasusModel\n    - __call__\n    - encode\n    - decode\n\n## FlaxPegasusForConditionalGeneration\n\n[[autodoc]] FlaxPegasusForConditionalGeneration\n    - __call__\n    - encode\n    - decode\n\n</jax>\n</frameworkcontent>",
        "question": "What is the name of the script to fine-tune pegasus on the XSUM dataset?\n",
        "answer": "The name of the script is 'finetune_pegasus_xsum.sh'.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/pegasus.md"
    },
    {
        "context": "Append classifier to preprocessing pipeline. Now we have a full prediction pipeline.\n\n```python\nclf = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n```\n\nAs `Evaluate` metrics use lists as inputs for references and predictions, we need to convert them to Python lists.\n\n\n```python\n# Evaluate metrics accept lists as inputs for values of references and predictions\n\ny_test = y_test.tolist()\ny_pred = y_pred.tolist()\n\n# Accuracy\n\naccuracy_metric = evaluate.load(\"accuracy\")\naccuracy = accuracy_metric.compute(references=y_test, predictions=y_pred)\nprint(\"Accuracy:\", accuracy)\n# Accuracy: 0.79\n```\n\nYou can use any suitable `evaluate` metric with the estimators as long as they are compatible with the task and predictions.",
        "question": "What is the accuracy of the model?\n",
        "answer": "The accuracy of the model is 0.79.",
        "source_doc": "huggingface/evaluate/blob/main/docs/source/sklearn_integrations.mdx"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet1DModel\n\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 1D UNet model.\n\nThe abstract from the paper is:",
        "question": "Is this UNet model a conditional model?\n",
        "answer": "No, this UNet model is not a conditional model.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Models\n\n## Generic model classes\n\nThe following ORT classes are available for instantiating a base model class without a specific head.\n\n### ORTModel\n\n[[autodoc]] onnxruntime.ORTModel\n\n## Natural Language Processing\n\nThe following ORT classes are available for the following natural language processing tasks.\n\n### ORTModelForCausalLM\n\n[[autodoc]] onnxruntime.ORTModelForCausalLM\n    - forward\n\n### ORTModelForMaskedLM\n\n[[autodoc]] onnxruntime.ORTModelForMaskedLM\n\n### ORTModelForSeq2SeqLM\n\n[[autodoc]] onnxruntime.ORTModelForSeq2SeqLM\n    - forward\n\n### ORTModelForSequenceClassification\n\n[[autodoc]] onnxruntime.ORTModelForSequenceClassification\n\n### ORTModelForTokenClassification\n\n[[autodoc]] onnxruntime.ORTModelForTokenClassification\n\n### ORTModelForMultipleChoice\n\n[[autodoc]] onnxruntime.ORTModelForMultipleChoice\n\n### ORTModelForQuestionAnswering\n\n[[autodoc]] onnxruntime.ORTModelForQuestionAnswering\n\n## Computer vision\n\nThe following ORT classes are available for the following computer vision tasks.\n\n### ORTModelForImageClassification\n\n[[autodoc]] onnxruntime.ORTModelForImageClassification\n\n### ORTModelForSemanticSegmentation\n\n[[autodoc]] onnxruntime.ORTModelForSemanticSegmentation\n\n## Audio\n\nThe following ORT classes are available for the following audio tasks.\n\n### ORTModelForAudioClassification\n\n[[autodoc]] onnxruntime.ORTModelForAudioClassification",
        "question": "What is the name of the class for instantiating a base model class without a specific head?\n",
        "answer": "ORTModel",
        "source_doc": "huggingface/optimum/blob/main/docs/source/onnxruntime/package_reference/modeling_ort.mdx"
    },
    {
        "context": "prompt=\"....\"             # few-shot prompt\n\ndata = query(prompt,parameters,options)\n```\n\n---\n## Practical Insights\n\nHere are some practical insights, which help you get started using `GPT-Neo` and the 🤗 Accelerated Inference API.\n\nSince `GPT-Neo` (2.7B) is about 60x smaller than `GPT-3` (175B), it does not generalize as well to zero-shot problems and needs 3-4 examples to achieve good results. When you provide more examples `GPT-Neo` understands the task and takes the `end_sequence` into account, which allows us to control the generated text pretty well. \n\n![insights-benefit-of-examples](assets/22_few_shot_learning_gpt_neo_and_inference_api/insights-benefit-of-examples.png)\n\nThe hyperparameter `End Sequence`, `Token Length` & `Temperature` can be used to control the `text-generation` of the model and you can use this to your advantage to solve the task you need. The `Temperature` controlls the randomness of your generations, lower temperature results in less random generations and higher temperature results in more random generations.\n\n![insights-benefit-of-hyperparameter](assets/22_few_shot_learning_gpt_neo_and_inference_api/insights-benefit-of-hyperparameter.png)\n\nIn the example, you can see how important it is to define your hyperparameter. These can make the difference between solving your task or failing miserably.\n\n---\n\n## Responsible Use\n\nFew-Shot Learning is a powerful technique but also presents unique pitfalls that need to be taken into account when designing uses cases.\nTo illustrate this, let's consider the default `Sentiment Analysis` setting provided in the widget. After seeing three examples of sentiment classification, the model makes the following predictions 4 times out of 5, with `temperature` set to 0.1:\n\n> ###  \n> Tweet: \"I'm a disabled happy person\"  \n> Sentiment: Negative",
        "question": "What is the sentiment of the tweet \"I'm a disabled happy person\" according to the model?\n",
        "answer": "Negative",
        "source_doc": "huggingface/blog/blob/main/few-shot-learning-gpt-neo-and-inference-api.md"
    },
    {
        "context": "## LEDConfig\n\n[[autodoc]] LEDConfig\n\n## LEDTokenizer\n\n[[autodoc]] LEDTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## LEDTokenizerFast\n\n[[autodoc]] LEDTokenizerFast\n\n## LED specific outputs\n\n[[autodoc]] models.led.modeling_led.LEDEncoderBaseModelOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqModelOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqLMOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput\n\n<frameworkcontent>\n<pt>\n\n## LEDModel\n\n[[autodoc]] LEDModel\n    - forward\n\n## LEDForConditionalGeneration\n\n[[autodoc]] LEDForConditionalGeneration\n    - forward\n\n## LEDForSequenceClassification\n\n[[autodoc]] LEDForSequenceClassification\n    - forward\n\n## LEDForQuestionAnswering\n\n[[autodoc]] LEDForQuestionAnswering\n    - forward\n\n</pt>\n<tf>\n\n## TFLEDModel\n\n[[autodoc]] TFLEDModel\n    - call\n\n## TFLEDForConditionalGeneration\n\n[[autodoc]] TFLEDForConditionalGeneration\n    - call\n\n</tf>\n</frameworkcontent>",
        "question": "What is the name of the class for the LED model in TensorFlow?\n",
        "answer": "TFLEDModel",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/led.md"
    },
    {
        "context": "1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.",
        "question": "Which model was released by Meta AI?\n",
        "answer": "EnCodec",
        "source_doc": "huggingface/transformers/blob/main/README.md"
    },
    {
        "context": "|        | [here](https://huggingface.co/datasets/Makxxx/french_CEFR/discussions/1)                                                          | [Makxxx/french_CEFR](https://huggingface.co/datasets/Makxxx/french_CEFR)                                                                                           | 0         | 0     |\n|        | [here](https://huggingface.co/datasets/sugam11/french-snli/discussions/1)                                                         | [sugam11/french-snli](https://huggingface.co/datasets/sugam11/french-snli)                                                                                         | 0         | 0     |\n|        | [here](https://huggingface.co/datasets/Brendan/nlp244_french_snli/discussions/1)                                                  | [Brendan/nlp244_french_snli](https://huggingface.co/datasets/Brendan/nlp244_french_snli)                                                                           | 0         | 0     |\n|        | [here](https://huggingface.co/datasets/pvisnrt/french-snli/discussions/1)                                                         | [pvisnrt/french-snli](https://huggingface.co/datasets/pvisnrt/french-snli)                                                                                         | 0         | 0     |\n| Merged | [here](https://huggingface.co/datasets/pranjali97/french_translated_snli/discussions/1)                                           | [pranjali97/french_translated_snli](https://huggingface.co/datasets/pranjali97/french_translated_snli)                                                             | 0         | 0     |\n| Merged | [here](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-french/discussions/1)                                    | [FreedomIntelligence/evol-instruct-french](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-french)                                               | 0         | 0     |",
        "question": "How many datasets are merged in the pranjali97/french\\_translated\\_snli dataset?\n",
        "answer": "Two datasets are merged in the pranjali97/french\\_translated\\_snli dataset.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/datasets_without_language.md"
    },
    {
        "context": "## How do I finetune this model?\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n```python\nmodel = timm.create_model('tf_efficientnet_lite0', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@misc{tan2020efficientnet,\n      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2020},\n      eprint={1905.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```",
        "question": "How do I finetune a pre-trained model in the context?\n",
        "answer": "You can finetune any of the pre-trained models just by changing the classifier (the last layer). You have to write a training loop or adapt timm's training script to use your dataset.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/tf-efficientnet-lite.md"
    },
    {
        "context": "repository_info:\n  training_repository: &TRAINING_REPOSITORY\n    image_type: &TRAINING_IMAGE_TYPE training\n    root: !join [ \"huggingface/\", *BASE_FRAMEWORK, \"/\", *TRAINING_IMAGE_TYPE ]\n    repository_name: &REPOSITORY_NAME !join [\"pr\", \"-\", \"huggingface\", \"-\", *BASE_FRAMEWORK, \"-\", *TRAINING_IMAGE_TYPE]\n    repository: &REPOSITORY !join [ *ACCOUNT_ID, .dkr.ecr., *REGION, .amazonaws.com/,\n      *REPOSITORY_NAME ]\n\nimages:\n  BuildHuggingFacePytorchGpuPy37Cu110TrainingDockerImage:\n    <<: *TRAINING_REPOSITORY\n    build: &HUGGINGFACE_PYTORCH_GPU_TRAINING_PY3 false\n    image_size_baseline: &IMAGE_SIZE_BASELINE 15000\n    device_type: &DEVICE_TYPE gpu\n    python_version: &DOCKER_PYTHON_VERSION py3\n    tag_python_version: &TAG_PYTHON_VERSION py36\n    cuda_version: &CUDA_VERSION cu110\n    os_version: &OS_VERSION ubuntu18.04\n    transformers_version: &TRANSFORMERS_VERSION 4.4.2\n    datasets_version: &DATASETS_VERSION 1.5.0\n    tag: !join [ *VERSION, '-', 'transformers', *TRANSFORMERS_VERSION, '-', *DEVICE_TYPE, '-', *TAG_PYTHON_VERSION, '-',\n      *CUDA_VERSION, '-', *OS_VERSION ]\n    docker_file: !join [ docker/, *SHORT_VERSION, /, *DOCKER_PYTHON_VERSION, /, \n      *CUDA_VERSION, /Dockerfile., *DEVICE_TYPE ]\n```\n2. In the PR comment describe what test we ran and with which framework versions. Here you can copy the table from [Current Tests](#current-tests). You can take a look at this [PR](https://github.com/aws/deep-learning-containers/pull/1025), which information are needed.\n\n## Current Tests",
        "question": "What is the docker file of the BuildHuggingFacePytorchGpuPy37Cu110TrainingDockerImage image?\n",
        "answer": "The docker file of the BuildHuggingFacePytorchGpuPy37Cu110TrainingDockerImage image is !join [ docker/, *SHORT_VERSION, /, *DOCKER_PYTHON_VERSION, /, *CUDA_VERSION, /Dockerfile., *DEVICE_TYPE ].\n```",
        "source_doc": "huggingface/transformers/blob/main/tests/sagemaker/README.md"
    },
    {
        "context": "```py\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(device)\n\nguidance_scale = 15.0\nprompt = [\"A firecracker\", \"A birthday cupcake\"]\n\nimages = pipe(\n    prompt,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n```\n\nNow use the [`~utils.export_to_gif`] function to turn the list of image frames into a gif of the 3D object.\n\n```py\nfrom diffusers.utils import export_to_gif\n\nexport_to_gif(images[0], \"firecracker_3d.gif\")\nexport_to_gif(images[1], \"cake_3d.gif\")\n```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/firecracker_out.gif\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">prompt = \"A firecracker\"</figcaption>\n  </div>\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/cake_out.gif\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">prompt = \"A birthday cupcake\"</figcaption>\n  </div>\n</div>\n\n## Image-to-3D\n\nTo generate a 3D object from another image, use the [`ShapEImg2ImgPipeline`]. You can use an existing image or generate an entirely new one. Let's use the [Kandinsky 2.1](../api/pipelines/kandinsky) model to generate a new image.\n\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\nprior_pipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nprompt = \"A cheeseburger, white background\"",
        "question": "What is the name of the pipeline used to generate the 3D object from the image?\n",
        "answer": "ShapEImg2ImgPipeline",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md"
    },
    {
        "context": "| [Persian](https://huggingface.co/course/fa/chapter1/1) (WIP)                  | [`chapters/fa`](https://github.com/huggingface/course/tree/main/chapters/fa)       | [@jowharshamshiri](https://github.com/jowharshamshiri), [@schoobani](https://github.com/schoobani)                                                                                                                                                                                                                                                       |\n| [French](https://huggingface.co/course/fr/chapter1/1)                         | [`chapters/fr`](https://github.com/huggingface/course/tree/main/chapters/fr)       | [@lbourdois](https://github.com/lbourdois), [@ChainYo](https://github.com/ChainYo), [@melaniedrevet](https://github.com/melaniedrevet), [@abdouaziz](https://github.com/abdouaziz)                                                                                                                                                                       |\n| [Gujarati](https://huggingface.co/course/gu/chapter1/1) (WIP)                 | [`chapters/gu`](https://github.com/huggingface/course/tree/main/chapters/gu)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |",
        "question": "Which language does the GitHub user pandyaved98 contribute to the Hugging Face course for?\n",
        "answer": "Gujarati",
        "source_doc": "huggingface/course/blob/main/README.md"
    },
    {
        "context": "<p style=\"text-align: center;\">\n\\\\( \\text{AP@[.5:.05:0.95} = \\frac{\\text{AP}_{0.5} + \\text{AP}_{0.55} + ... + \\text{AP}_{0.95}}{10} \\\\)\n</p>\n\n* **AP-S**: It applies AP@[.5:.05:.95] considering (small) ground-truth objects with \\\\( \\text{area} < 32^2 \\\\) pixels.\n* **AP-M**: It applies AP@[.5:.05:.95] considering (medium-sized) ground-truth objects with \\\\( 32^2 < \\text{area} < 96^2 \\\\) pixels.\n* **AP-L**: It applies AP@[.5:.05:.95] considering (large) ground-truth objects with \\\\( 32^2 < \\text{area} < 96^2\\\\) pixels.\n\nFor Average Recall (AR), 10 IoU thresholds (0.5, 0.55, 0.6,...,0.95) are used to compute the Recall values. AR is computed by either limiting the number of detections per image or by limiting the detections based on the object's area.\n\n* **AR-1**: considers up to 1 detection per image.\n* **AR-10**: considers up to 10 detections per image.\n* **AR-100**: considers up to 100 detections per image.\n* **AR-S**: considers (small) objects with \\\\( \\text{area} < 32^2 \\\\) pixels.\n* **AR-M**: considers (medium-sized) objects with \\\\(  32^2 < \\text{area} < 96^2 \\\\) pixels.\n* **AR-L**: considers (large) objects with \\\\( \\text{area} > 96^2 \\\\) pixels.\n\n  \n## Object Detection Leaderboard\n\nWe recently released the [Object Detection Leaderboard](https://huggingface.co/spaces/hf-vision/object_detection_leaderboard) to compare the accuracy and efficiency of open-source models from our Hub. \n\n<div display=\"block\" margin-left=\"auto\" margin-right=\"auto\" width=\"50%\">\n<center>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/object-detection-leaderboard/screenshot-leaderboard.png\" alt=\"screenshot-leaderboard.png\" />\n    <figcaption> Figure 8: Object Detection Leaderboard.</figcaption>\n</center>\n</div>\n\nTo measure accuracy, we used 12 metrics involving Average Precision and Average Recall using [COCO style](https://cocodataset.org/#detection-eval), benchmarking over COCO val 2017 dataset.",
        "question": "What is the formula for AP@[.5:.05:.95]?\n",
        "answer": "The formula for AP@[.5:.05:.95] is given by the average of AP0.5, AP0.55, ..., AP0.95.",
        "source_doc": "huggingface/blog/blob/main/object-detection-leaderboard.md"
    },
    {
        "context": "Gradio Demo: sentence_builder\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```",
        "question": "How many animals can be selected in the Gradio demo?\n",
        "answer": "Users can select between 2 and 20 animals in the Gradio demo.",
        "source_doc": "gradio-app/gradio/blob/main/demo/sentence_builder/run.ipynb"
    },
    {
        "context": "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/YuanGongND/ast).\n\n## Usage tips\n\n- When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset, it's recommended to take care of the input normalization (to make\nsure the input has mean of 0 and std of 0.5). [`ASTFeatureExtractor`] takes care of this. Note that it uses the AudioSet\nmean and std by default. You can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py) to see how\nthe authors compute the stats for a downstream dataset.\n- Note that the AST needs a low learning rate (the authors use a 10 times smaller learning rate compared to their CNN model proposed in the\n[PSLA paper](https://arxiv.org/abs/2102.01243)) and converges quickly, so please search for a suitable learning rate and learning rate scheduler for your task.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with the Audio Spectrogram Transformer.\n\n<PipelineTag pipeline=\"audio-classification\"/>\n\n- A notebook illustrating inference with AST for audio classification can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).\n- [`ASTForAudioClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- See also: [Audio classification](../tasks/audio_classification).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ASTConfig\n\n[[autodoc]] ASTConfig\n\n## ASTFeatureExtractor\n\n[[autodoc]] ASTFeatureExtractor\n    - __call__",
        "question": "What is the recommended learning rate for fine-tuning the Audio Spectrogram Transformer (AST)?\n",
        "answer": "The recommended learning rate for fine-tuning the Audio Spectrogram Transformer (AST) is a 10 times smaller learning rate compared to the CNN model proposed in the PSLA paper.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/audio-spectrogram-transformer.md"
    },
    {
        "context": "logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}\n```\n\n### Custom Trainer for Weighted Loss \nAs mentioned at the beginning of this post, we have an imbalanced distribution between positive and negative classes. We need to train our models with a weighted cross-entropy loss to account for that. The `Trainer` class doesn't support providing a custom loss as it expects to get the loss directly from the model's outputs. \n\nSo, we need to define our custom `WeightedCELossTrainer` that overrides the `compute_loss` method to calculate the weighted cross-entropy loss based on the model's predictions and the input labels: \n\n```python\nfrom transformers import Trainer\n\nclass WeightedCELossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # Get model's predictions\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Compute custom loss\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```\n\n\n### Trainer Setup\n\nLet's set the training arguments and the trainer for the three models.",
        "question": "What is the class name of the custom trainer?\n",
        "answer": "WeightedCELossTrainer",
        "source_doc": "huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md"
    },
    {
        "context": "```py\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\n    \"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\"\n)\n```\n\nSet the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, and `DATA_DIR`  to the path where you just downloaded the cat images to. The script creates and saves the following files to your repository:\n\n- `learned_embeds.bin`: the learned embedding vectors corresponding to your example images\n- `token_identifier.txt`: the special placeholder token\n- `type_of_concept.txt`: the type of concept you're training on (either \"object\" or \"style\")\n\n<Tip warning={true}>\n\nA full training run takes ~1 hour on a single V100 GPU.\n\n</Tip>\n\nOne more thing before you launch the script. If you're interested in following along with the training process, you can periodically save generated images as training progresses. Add the following parameters to the training command:\n\n```bash\n--validation_prompt=\"A <cat-toy> train\"\n--num_validation_images=4\n--validation_steps=100\n```\n\n<hfoptions id=\"training-inference\">\n<hfoption id=\"PyTorch\">\n\n```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\nexport DATA_DIR=\"./cat\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\" \\\n  --push_to_hub\n```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"./cat\"",
        "question": "What is the name of the model used in the PyTorch training script?\n",
        "answer": "runwayml/stable-diffusion-v1-5\n\n```",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md"
    },
    {
        "context": "[[autodoc]] TFOPTForCausalLM\n    - call\n\n</tf>\n<jax>\n\n## FlaxOPTModel\n\n[[autodoc]] FlaxOPTModel\n    - __call__\n\n## FlaxOPTForCausalLM\n\n[[autodoc]] FlaxOPTForCausalLM\n    - __call__\n\n</jax>\n</frameworkcontent>",
        "question": "What is the name of the method in FlaxOPTForCausalLM that is called when the object is invoked?\n",
        "answer": "__call__",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/opt.md"
    },
    {
        "context": "demo = gr.Interface(fn=predict,\n                    inputs=gr.Audio(type=\"filepath\"),\n                    outputs=[gr.Label(num_top_classes=11, label=\"Predictions\"), \n                             gr.Number(label=\"Prediction time (s)\")],\n                    examples=example_list,\n                    cache_examples=False\n                    )\n\ndemo.launch(debug=False)\n\n```",
        "question": "What is the name of the function used in the gr.Interface?\n",
        "answer": "predict",
        "source_doc": "gradio-app/gradio/blob/main/demo/musical_instrument_identification/run.ipynb"
    },
    {
        "context": "</Tip>\n\n### Model predictions[[model-predictions]]\n\n<Youtube id=\"nx10eh4CoOs\"/>\n\n\nTraining and watching the loss go down is all very nice, but what if we want to actually get outputs from the trained model, either to compute some metrics, or to use the model in production? To do that, we can just use the `predict()` method. This will return the *logits* from the output head of the model, one per class.\n\n```py\npreds = model.predict(tf_validation_dataset)[\"logits\"]\n```\n\nWe can convert these logits into the model's class predictions by using `argmax` to find the highest logit, which corresponds to the most likely class:\n\n```py\nclass_preds = np.argmax(preds, axis=1)\nprint(preds.shape, class_preds.shape)\n```\n\n```python out\n(408, 2) (408,)\n```\n\nNow, let's use those `preds` to compute some metrics! We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation:\n\n```py\nimport evaluate\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmetric.compute(predictions=class_preds, references=raw_datasets[\"validation\"][\"label\"])\n```\n\n```python out\n{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}\n```\n\nThe exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) reported an F1 score of 88.9 for the base model. That was the `uncased` model while we are currently using the `cased` model, which explains the better result.",
        "question": "What is the method used to convert logits into the model's class predictions?\n",
        "answer": "The method used to convert logits into the model's class predictions is by using `argmax` to find the highest logit, which corresponds to the most likely class.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter3/3_tf.mdx"
    },
    {
        "context": "### Downstream datasets \n\nPre-trained vision-language models are often trained on various downstream tasks such as visual question-answering, text-guided object detection, text-guided image inpainting, multi-modal classification, and various stand-alone NLP and computer vision tasks. \n\nModels fine-tuned on the question-answering downstream task, such as [ViLT](https://arxiv.org/abs/2102.03334) and [GLIP](https://arxiv.org/abs/2112.03857), most commonly use the [VQA](https://visualqa.org/) (visual question-answering), [VQA v2](https://visualqa.org/), [NLVR2](https://lil.nlp.cornell.edu/nlvr/), [OKVQA](https://okvqa.allenai.org/), [TextVQA](https://huggingface.co/datasets/textvqa), [TextCaps](https://textvqa.org/textcaps/) and [VizWiz](https://vizwiz.org/) datasets. These datasets typically contain images paired with multiple open-ended questions and answers. Furthermore, datasets such as VizWiz and TextCaps can also be used for image segmentation and object localization downstream tasks. Some other interesting multi-modal downstream datasets are [Hateful Memes](https://huggingface.co/datasets/limjiayi/hateful_memes_expanded) for multi-modal classification, [SNLI-VE](https://github.com/necla-ml/SNLI-VE) for visual entailment prediction, and [Winoground](https://huggingface.co/datasets/facebook/winoground) for visio-linguistic compositional reasoning. \n\nNote that vision-language models are used for various classical NLP and computer vision tasks such as text or image classification and typically use uni-modal datasets ([SST2](https://huggingface.co/datasets/sst2), [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), for example) for such downstream tasks. In addition, datasets such as [COCO](https://cocodataset.org/) and [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) are commonly used both in the pre-training of models and also for the caption generation downstream task. \n\n## Supporting Vision-Language Models in 🤗 Transformers",
        "question": "Which datasets are commonly used for the caption generation downstream task in vision-language models?\n",
        "answer": "The COCO and Conceptual Captions datasets are commonly used for the caption generation downstream task in vision-language models.",
        "source_doc": "huggingface/blog/blob/main/vision_language_pretraining.md"
    },
    {
        "context": "3. Set up your [Materials](https://docs.unity3d.com/Manual/Materials.html), using the concept art as a reference. I'm using the basic built-in materials.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/color.png\" alt=\"Scene with Materials\">\n</figure>\n\n4. Set up your [Lighting](https://docs.unity3d.com/Manual/Lighting.html). I'm using a warm sun (#FFE08C, intensity 1.25) with soft ambient lighting (#B3AF91).\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/lighting.png\" alt=\"Scene with Lighting\">\n</figure>\n\n5. Set up your [Camera](https://docs.unity3d.com/ScriptReference/Camera.html) **using an orthographic projection** to match the projection of the concept art.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/camera.png\" alt=\"Scene with Camera\">\n</figure>\n\n6. Add some water. I'm using the [Stylized Water Shader](https://assetstore.unity.com/packages/vfx/shaders/stylized-water-shader-71207) from the Unity asset store.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/water.png\" alt=\"Scene with Water\">\n</figure>\n\n7. Finally, set up [Post-processing](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@7.1/manual/integration-with-post-processing.html). I'm using ACES tonemapping and +0.2 exposure.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/post-processing.png\" alt=\"Final Result\">\n</figure>",
        "question": "What type of projection is used for the camera?\n",
        "answer": "The camera uses an orthographic projection.",
        "source_doc": "huggingface/blog/blob/main/ml-for-games-1.md"
    },
    {
        "context": "1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (Google Research から) Yi Tay, Mostafa Dehghani, Vinh Q から公開された研究論文: [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (Google Research から) Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant. から公開された研究論文 [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi)\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (Microsoft Research から) Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang から公開された研究論文: [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597)\n1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (Microsoft Research から) Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu から公開された研究論文: [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752)\n1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)** (from Kakao Corporation) released with the paper [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://arxiv.org/abs/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim. \n1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (Peking University から) Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. から公開された研究論文 [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)",
        "question": "Which company released UnivNet?\n",
        "answer": "Kakao Corporation",
        "source_doc": "huggingface/transformers/blob/main/README_ja.md"
    },
    {
        "context": "Community leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfeedback@huggingface.co.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.",
        "question": "Who should I report instances of abusive, harassing, or otherwise unacceptable behavior to?\n",
        "answer": "You should report instances of abusive, harassing, or otherwise unacceptable behavior to the community leaders responsible for enforcement at feedback@huggingface.co.",
        "source_doc": "huggingface/simulate/blob/main/CODE_OF_CONDUCT.md"
    },
    {
        "context": "For more quick tutorials about OVHcloud AI products, check out the showcase https://vimeo.com/showcase/8903300\n\n## How to combine n-gram with acoustic model\n\nHaving trained a speech recognition model with CTC as shown in the section above, \none can further improve the model's performance by adding an **n-gram language model**\nto the decoding process of the model. By doing so, we are replacing the naive greedy decoding \nwith **n-gram-boosted** beam search decoding.\n\nN-gram language models can be built on CPU in just a few minutes. *N-gram-boosted* beam search decoding noticeably slows down the \ninference time, but also yields significant word error rates improvements - usually between 10-40 %.\n\nYou can find an in-detail blog post on how to build an *n-gram* [here](https://huggingface.co/blog/wav2vec2-with-ngram).\nThe blog post can be opened in a google colab and by adapting three lines of the example for your use case, one can directly\ncreate an *n-gram* in the google colab.\nThe blog post gives in-detail instructions on how to build an n-gram and how to add it to your trained speech recognition model.\n\n- why one should add an *n-gram* to her/his speech recognition system,\n- how to build an *n-gram*, and,\n- how to add the built *n-gram* the speech recognition system for seamless decoding\n\nOur previously trained model - [xls-r-300m-sv](https://huggingface.co/hf-test/xls-r-300m-sv) - enjoys a 30% word error rate reduction after \nhaving added an n-gram. As shown in the example of the blog post, we strongly advise participants to upload all files required for combining \nthe *n-gram* with a trained speech recognition model directly into the same model repository.\n\n## Evaluation",
        "question": "How long does it take to build an n-gram language model on CPU?\n",
        "answer": "It takes just a few minutes to build an n-gram language model on CPU.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md"
    },
    {
        "context": "The range of `f1` is 0-1 -- its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n\n### Values from popular papers\nThe [original SQuAD paper](https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf) reported an F1 score of 51.0% and an Exact Match score of 40.0%. They also report that human performance on the dataset represents an F1 score of 90.5% and an Exact Match score of 80.3%.\n\nFor more recent model performance, see the [dataset leaderboard](https://paperswithcode.com/dataset/squad).\n\n## Examples \n\nMaximal values for both exact match and F1 (perfect match):\n\n```python\nfrom evaluate import load\nsquad_metric = load(\"squad\")\npredictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\nreferences = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\nresults = squad_metric.compute(predictions=predictions, references=references)\nresults\n{'exact_match': 100.0, 'f1': 100.0}\n```\n\nMinimal values for both exact match and F1 (no match):\n\n```python\nfrom evaluate import load\nsquad_metric = load(\"squad\")\npredictions = [{'prediction_text': '1999', 'id': '56e10a3be3433e1400422b22'}]\nreferences = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\nresults = squad_metric.compute(predictions=predictions, references=references)\nresults\n{'exact_match': 0.0, 'f1': 0.0}\n```\n\nPartial match (2 out of 3 answers correct) :",
        "question": "What is the Exact Match score of human performance on the SQuAD dataset?\n",
        "answer": "The Exact Match score of human performance on the SQuAD dataset is 80.3%.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/squad/README.md"
    },
    {
        "context": "To work towards that goal, it is important to recognize the thoughtful, dedicated efforts that have helped model cards grow into what they are today, from the adoption of model cards as a standard practice at many large organisations to the development of sophisticated tools for hosting and generating model cards. Since model cards were proposed by Mitchell et al. (2018), the landscape of machine learning documentation has expanded and evolved. A plethora of documentation tools and templates for data, models, and ML systems have been proposed and have developed – reflecting the incredible work of hundreds of researchers, impacted community members, advocates, and other stakeholders. Important discussions about the relationship between ML documentation and theories of change in responsible AI have created continued important discussions, and at times, divergence. We also recognize the challenges facing model cards, which in some ways mirror the challenges facing machine learning documentation and responsible AI efforts more generally, and we see opportunities ahead to help shape both model cards and the ecosystems in which they function positively in the months and years ahead.",
        "question": "Who proposed model cards?\n",
        "answer": "Mitchell et al. (2018) proposed model cards.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/model-card-guidebook.md"
    },
    {
        "context": "## 🤗 **Education for Beginners**\n\n🗣️ We want to lower the barrier to becoming a machine learning engineer by providing online courses, hands-on workshops, and other innovative techniques.\n\n- We provide a free [course](https://huggingface.co/course/chapter1/1) about natural language processing (NLP) and more domains (soon) using free tools and libraries from the Hugging Face ecosystem. It’s completely free and without ads. The ultimate goal of this course is to learn how to apply Transformers to (almost) any machine learning problem!\n- We provide a free [course](https://github.com/huggingface/deep-rl-class) about Deep Reinforcement Learning. In this course, you can study Deep Reinforcement Learning in theory and practice, learn to use famous Deep RL libraries, train agents in unique environments, publish your trained agents in one line of code to the Hugging Face Hub, and more!\n- We provide a free [course](https://huggingface.co/course/chapter9/1) on how to build interactive demos for your machine learning models. The ultimate goal of this course is to allow ML developers to easily present their work to a wide audience including non-technical teams or customers, researchers to more easily reproduce machine learning models and behavior, end users to more easily identify and debug failure points of models, and more!\n- Experts at Hugging Face wrote a [book](https://transformersbook.com/) on Transformers and their applications to a wide range of NLP tasks.\n\nApart from those efforts, many team members are involved in other educational efforts such as:\n- Participating in meetups, conferences and workshops.\n- Creating podcasts, YouTube videos, and blog posts.\n- [Organizing events](https://github.com/huggingface/community-events/tree/main/huggan) in which free GPUs are provided for anyone to be able to train and share models and create demos for them.\n\n## 🤗 **Education for Instructors**",
        "question": "What is the goal of the free NLP course provided by Hugging Face?\n",
        "answer": "The goal of the free NLP course provided by Hugging Face is to learn how to apply Transformers to (almost) any machine learning problem.",
        "source_doc": "huggingface/blog/blob/main/education.md"
    },
    {
        "context": "--\ntitle: \"Hugging Face on PyTorch / XLA TPUs\"\nthumbnail: /blog/assets/13_pytorch_xla/pytorch_xla_thumbnail.png\nauthors:\n- user: jysohn23\n  guest: true\n- user: lysandre\n---\n\n# Hugging Face on PyTorch / XLA TPUs: Faster and cheaper training\n\n\n<a href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/13_pytorch_xla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Training Your Favorite Transformers on Cloud TPUs using PyTorch / XLA\n\nThe PyTorch-TPU project originated as a collaborative effort between the Facebook PyTorch and Google TPU teams and officially launched at the 2019 PyTorch Developer Conference 2019. Since then, we’ve worked with the Hugging Face team to bring first-class support to training on Cloud TPUs using [PyTorch / XLA](https://github.com/pytorch/xla). This new integration enables PyTorch users to run and scale up their models on Cloud TPUs while maintaining the exact same Hugging Face trainers interface.\n\nThis blog post provides an overview of changes made in the Hugging Face library, what the PyTorch / XLA library does, an example to get you started training your favorite transformers on Cloud TPUs, and some performance benchmarks. If you can’t wait to get started with TPUs, please skip ahead to the [“Train Your Transformer on Cloud TPUs”](#train-your-transformer-on-cloud-tpus) section - we handle all the PyTorch / XLA mechanics for you within the `Trainer` module!\n\n### XLA:TPU Device Type\n\nPyTorch / XLA adds a new `xla` device type to PyTorch. This device type works just like other PyTorch device types. For example, here's how to create and print an XLA tensor:\n\n```python\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nt = torch.randn(2, 2, device=xm.xla_device())\nprint(t.device)\nprint(t)\n```",
        "question": "How long does it take to train a BERT Base model on a single Cloud TPU?\n",
        "answer": "1.5 hours",
        "source_doc": "huggingface/blog/blob/main/pytorch-xla.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Expanding the language coverage of speech technology has the potential to improve access to information for many more people. \nHowever, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000\nlanguages spoken around the world. \nThe Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. \nThe main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging\nself-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, \na single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models \nfor the same number of languages, as well as a language identification model for 4,017 languages. \nExperiments show that our multilingual speech recognition model more than halves the word error rate of \nWhisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.*\n\nHere are the different models open sourced in the MMS project. The models and code are originally released [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms). We have add them to the `transformers` framework, making them easier to use.\n\n### Automatic Speech Recognition (ASR)\n\nThe ASR model checkpoints  can be found here : [mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102), [mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107), [mms-1b-all](https://huggingface.co/facebook/mms-1b-all). For best accuracy, use the `mms-1b-all` model. \n\nTips:",
        "question": "What is the name of the best ASR model in the MMS project?\n",
        "answer": "The best ASR model in the MMS project is `mms-1b-all`.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n[[open-in-colab]]\n\n# Performing inference with LCM-LoRA\n\nLatent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings. \n\nFrom the [official website](https://latent-consistency-models.github.io/):\n\n> LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps (~32 A100 GPU Hours) for generating high quality 768 x 768 resolution images in 2~4 steps or even one step, significantly accelerating text-to-image generation. We employ LCM to distill the Dreamshaper-V7 version of SD in just 4,000 training iterations.\n\nFor a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378).\n\nHowever, each model needs to be distilled separately for latent consistency distillation. The core idea with LCM-LoRA is to train just a few adapter layers, the adapter being LoRA in this case. \nThis way, we don't have to train the full model and keep the number of trainable parameters manageable. The resulting LoRAs can then be applied to any fine-tuned version of the model without distilling them separately.\nAdditionally, the LoRAs can be applied to image-to-image, ControlNet/T2I-Adapter, inpainting, AnimateDiff etc. \nThe LCM-LoRA can also be combined with other LoRAs to generate styled images in very few steps (4-8).",
        "question": "How many training steps are needed to distill a Latent Consistency Model (LCM) from a pre-trained Stable Diffusion (SD)?\n",
        "answer": "4,000 training steps",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md"
    },
    {
        "context": "ow to instantiate a Transformers model? In this video we will look at how we can create and use a model from the Transformers library. As we've seen before, the AutoModel class allows you to instantiate a pretrained model from any checkpoint on the Hugging Face Hub. It will pick the right model class from the library to instantiate the proper architecture and load the weights of the pretrained model inside it. As we can see, when given a BERT checkpoint, we end up with a BertModel, and similarly for GPT-2 or BART. Behind the scenes, this API can take the name of a checkpoint on the Hub, in which case it will download and cache the configuration file as well as the model weights file. You can also specify the path to a local folder that contains a valid configuration file and a model weights file. To instantiate the pretrained model, the AutoModel API will first open the configuration file to look at the configuration class that should be used. The configuration class depends on the type of the model (BERT, GPT-2 or BART for instance). Once it has the proper configuration class, it can instantiate that configuration, which is a blueprint to know how to create the model. It also uses this configuration class to find the proper model class, which is combined with the loaded configuration, to load the model. This model is not yet our pretrained model as it has just been initialized with random weights. The last step is to load the weights from the model file inside this model. To easily load the configuration of a model from any checkpoint or a folder containing the configuration folder, we can use the AutoConfig class. Like the AutoModel class, it will pick the right configuration class from the library. We can also use the specific class corresponding to a checkpoint, but we will need to change the code each time we want to try a different model",
        "question": "How can you instantiate a pretrained model from a local folder using the Transformers library?\n",
        "answer": "You can instantiate a pretrained model from a local folder by specifying the path to the folder that contains a valid configuration file and a model weights file when using the AutoModel API. The AutoModel API will open the configuration file to look at the configuration class, instantiate that configuration, find the proper model class, and load the model with the configuration. Finally, it will load the weights from the model file inside this model.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/03_model-api-pt.md"
    },
    {
        "context": "1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.\n1. **[TVP](https://huggingface.co/docs/transformers/model_doc/tvp)** (from Intel) released with the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.\n1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (Google Research से) Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant. द्वाराअनुसंधान पत्र [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) के साथ जारी किया गया\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (माइक्रोसॉफ्ट रिसर्च से) साथ में दिया गया पेपर [UniSpeech: यूनिफाइड स्पीच रिप्रेजेंटेशन लर्निंग विद लेबलेड एंड अनलेबल्ड डेटा](https:/ /arxiv.org/abs/2101.07597) चेंगई वांग, यू वू, याओ कियान, केनिची कुमातानी, शुजी लियू, फुरु वेई, माइकल ज़ेंग, ज़ुएदोंग हुआंग द्वारा।\n1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (माइक्रोसॉफ्ट रिसर्च से) कागज के साथ [UNISPEECH-SAT: यूनिवर्सल स्पीच रिप्रेजेंटेशन लर्निंग विद स्पीकर अवेयर प्री-ट्रेनिंग ](https://arxiv.org/abs/2110.05752) सानयुआन चेन, यू वू, चेंग्यी वांग, झेंगयांग चेन, झूओ चेन, शुजी लियू, जियान वू, याओ कियान, फुरु वेई, जिन्यु ली, जियांगज़ान यू द्वारा पोस्ट किया गया।",
        "question": "Which model was released by Microsoft Research?\n",
        "answer": "UniSpeech and UniSpeechSat were released by Microsoft Research.",
        "source_doc": "huggingface/transformers/blob/main/README_hd.md"
    },
    {
        "context": "gr.Markdown(\"## Dataset Examples\")\n\n    component_example_set = [\n        (gr.Audio(render=False), join(KS_FILES, \"cantina.wav\")),\n        (gr.Checkbox(render=False), True),\n        (gr.CheckboxGroup(render=False, choices=[\"A\", \"B\"]), [\"A\", \"B\"]),\n        (gr.ColorPicker(render=False), \"#FF0000\"),\n        (gr.Dataframe(render=False), [[1, 2, 3], [4, 5, 6]]),\n        (gr.Dropdown(render=False), \"A\"),\n        (gr.File(render=False), join(KS_FILES, \"lion.jpg\")),\n        (gr.HTML(render=False), \"<div>Test</div>\"),\n        (gr.Image(render=False), join(KS_FILES, \"lion.jpg\")),\n        (gr.Markdown(render=False), \"# Test\"),\n        (gr.Number(render=False), 1),\n        (gr.Radio(render=False), \"A\"),\n        (gr.Slider(render=False), 1),\n        (gr.Textbox(render=False), \"A\"),\n        (gr.Video(render=False), join(KS_FILES, \"world.mp4\")),\n    ]\n    gr.Dataset(\n        components=[c for c, _ in component_example_set],\n        samples=[[e for _, e in component_example_set]],\n    )\n\n    with gr.Tabs():\n        for c, e in component_example_set:\n            with gr.Tab(c.__class__.__name__):\n                gr.Dataset(components=[c], samples=[[e]] * 3)\n\n\nif __name__ == \"__main__\":\n    demo.launch(allowed_paths=[KS_FILES])\n\n```",
        "question": "What is the first example in the component_example_set?\n",
        "answer": "The first example in the component_example_set is (gr.Audio(render=False), join(KS_FILES, \"cantina.wav\")).",
        "source_doc": "gradio-app/gradio/blob/main/demo/blocks_kitchen_sink/run.ipynb"
    },
    {
        "context": "- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common_voice_10_0`\n- `config`: the configuration name, for example `cola`\n- `split`: the split name, for example `train`\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing two keys:\n\n- The [`features`](https://huggingface.co/docs/datasets/about_dataset_features) of a dataset, including the column's name and data type.\n- The first 100 `rows` of a dataset and the content contained in each column of a specific row.\n\nFor example, here are the `features` and the first 100 `rows` of the `duorc`/`SelfRC` train split:",
        "question": "What is the name of the first column in the duorc/SelfRC dataset?\n",
        "answer": "The name of the first column in the duorc/SelfRC dataset is \"id\".",
        "source_doc": "huggingface/datasets-server/blob/main/docs/source/first_rows.mdx"
    },
    {
        "context": "--\n{{card_data}}\n---\n\n# {{ model_name | default(\"MyModelName\", true)}}\n\n{{ some_data }}",
        "question": "What is the name of the model?\n",
        "answer": "MyModelName",
        "source_doc": "huggingface/huggingface_hub/blob/main/tests/fixtures/cards/sample_template.md"
    },
    {
        "context": "pipe = SpacyEvalPipeline(nlp)\n```\n\nThat class is compatible with the `evaluator` and we can use the same instance from the previous examlpe along with the IMDb test set:\n\n```py\neval.compute(pipe, ds[\"test\"], \"accuracy\")\n>>> {'accuracy': 0.6914}\n```\n\nThis will take a little longer than the Scikit-Learn example but after roughly 10-15min you will have the evaluation results!",
        "question": "What is the accuracy of the pipe on the IMDb test set?\n",
        "answer": "0.6914",
        "source_doc": "huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx"
    },
    {
        "context": "Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:\n\n```python\ndef encode_word(word, model):\n    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n        {\"start\": None, \"score\": None} for _ in range(len(word))\n    ]\n    for start_idx in range(len(word)):\n        # This should be properly filled by the previous steps of the loop\n        best_score_at_start = best_segmentations[start_idx][\"score\"]\n        for end_idx in range(start_idx + 1, len(word) + 1):\n            token = word[start_idx:end_idx]\n            if token in model and best_score_at_start is not None:\n                score = model[token] + best_score_at_start\n                # If we have found a better segmentation ending at end_idx, we update\n                if (\n                    best_segmentations[end_idx][\"score\"] is None\n                    or best_segmentations[end_idx][\"score\"] > score\n                ):\n                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n\n    segmentation = best_segmentations[-1]\n    if segmentation[\"score\"] is None:\n        # We did not find a tokenization of the word -> unknown\n        return [\"<unk>\"], None\n\n    score = segmentation[\"score\"]\n    start = segmentation[\"start\"]\n    end = len(word)\n    tokens = []\n    while start != 0:\n        tokens.insert(0, word[start:end])\n        next_start = best_segmentations[start][\"start\"]\n        end = start\n        start = next_start\n    tokens.insert(0, word[start:end])\n    return tokens, score\n```\n\nWe can already try our initial model on some words:\n\n```python\nprint(encode_word(\"Hopefully\", model))\nprint(encode_word(\"This\", model))\n```\n\n```python out\n(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n(['This'], 6.288267030694535)\n```\n\nNow it's easy to compute the loss of the model on the corpus!",
        "question": "What is the output of the print statement for the word \"This\"?\n",
        "answer": "The output of the print statement for the word \"This\" is (['This'], 6.288267030694535).",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter6/7.mdx"
    },
    {
        "context": "```python\nimport torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"",
        "question": "What is the purpose of the DataCollatorCTCWithPadding class?\n",
        "answer": "The DataCollatorCTCWithPadding class is a data collator that dynamically pads the inputs received. It is used to pad sequences to the longest sequence in the batch or to a maximum length specified with the argument max_length. It also has the option to pad to a multiple of a provided value for use with Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).",
        "source_doc": "huggingface/blog/blob/main/fine-tune-wav2vec2-english.md"
    },
    {
        "context": "|      |                                                                            |[decapoda-research/llama-13b-hf-int4](https://huggingface.co/decapoda-research/llama-13b-hf-int4)|0           |75                       |llama-license                                                                                 |https://huggingface.co/decapoda-research/llama-13b-hf-int4/blob/main/LICENSE                  |[LICENSE](https://huggingface.co/decapoda-research/llama-13b-hf-int4/blob/main/LICENSE)            |                                                                                                                     |                                                                                   |\n|      |                                                                            |[decapoda-research/llama-30b-hf-int4](https://huggingface.co/decapoda-research/llama-30b-hf-int4)|0           |22                       |llama-license                                                                                 |https://huggingface.co/decapoda-research/llama-30b-hf-int4/blob/main/LICENSE                  |[LICENSE](https://huggingface.co/decapoda-research/llama-30b-hf-int4/blob/main/LICENSE)            |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the llama-13b-hf-int4 model?\n",
        "answer": "The license for the llama-13b-hf-int4 model is available at https://huggingface.co/decapoda-research/llama-13b-hf-int4/blob/main/LICENSE.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "--\ntitle: \"Train a Sentence Embedding Model with 1B Training Pairs\"\nauthors:\n- user: asi\n  guest: true\n---\n\n# Train a Sentence Embedding Model with 1 Billion Training Pairs\n\n\n**Sentence embedding** is a method that maps sentences to vectors of real numbers. Ideally, these vectors would capture the semantic of a sentence and be highly generic. Such representations could then be used for many downstream applications such as clustering, text mining, or question answering.\n\nWe developed state-of-the-art sentence embedding models as part of the project [\"Train the Best Sentence Embedding Model Ever with 1B Training Pairs\"](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). This project took place during the [Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), organized by Hugging Face.  We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as guidance from Google’s Flax, JAX, and Cloud team members about efficient deep learning frameworks!\n\n## Training methodology\n\n### Model\n\nUnlike words, we can not define a finite set of sentences. Sentence embedding methods, therefore, compose inner words to compute the final representation. For example, SentenceBert model ([Reimers and Gurevych, 2019](https://aclanthology.org/D19-1410.pdf)) uses Transformer, the cornerstone of many NLP applications, followed by a pooling operation over the contextualized word vectors. (c.f. Figure below.)\n\n![snippet](assets/32_1b_sentence_embeddings/model.png)\n\n### Multiple Negative Ranking Loss",
        "question": "What is the loss function used in the SentenceBert model?\n",
        "answer": "The loss function used in the SentenceBert model is Multiple Negative Ranking Loss.",
        "source_doc": "huggingface/blog/blob/main/1b-sentence-embeddings.md"
    },
    {
        "context": "1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.",
        "question": "Which model was released by Meta AI?\n",
        "answer": "EnCodec",
        "source_doc": "huggingface/transformers/blob/main/README_te.md"
    },
    {
        "context": "## Resources\n\n- [GPTQ blogpost](https://huggingface.co/blog/gptq-integration) – gives an overview on what is the GPTQ quantization method and how to use it. \n- [bistandbytes 4-bit quantization blogpost](https://huggingface.co/blog/4bit-transformers-bitsandbytes) - This blogpost introduces 4-bit quantization and QLoRa, an efficient finetuning approach. \n- [bistandbytes 8-bit quantization blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) - This blogpost explains how 8-bit quantization works with bitsandbytes.\n- [Basic usage Google Colab notebook for GPTQ](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) -  This notebook shows how to quantize your transformers model with the GPTQ method, how to do inference, and how to do fine-tuning with the quantized model.\n- [Basic usage Google Colab notebook for bitsandbytes](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing) - This notebook shows how to use 4-bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance.\n- [Merve's blogpost on quantization](https://huggingface.co/blog/merve/quantization) - This blogpost provides a gentle introduction to quantization and the quantization methods supported natively in transformers. \n\n\n## Comparing bitsandbytes and auto-gptq\nIn this section, we will go over the pros and cons of bitsandbytes and gptq quantization. Note that these are based on the feedback from the community and they can evolve over time as some of these features are in the roadmap of the respective libraries.",
        "question": "What is the name of the 4-bit quantization method introduced in the bistandbytes blogpost?\n",
        "answer": "The name of the 4-bit quantization method introduced in the bistandbytes blogpost is QLoRa.",
        "source_doc": "huggingface/blog/blob/main/overview-quantization-transformers.md"
    },
    {
        "context": "The high number of models uploaded to the Hugging Face Hub (101,041 models at the point of writing), enabled us to explore the content within model cards on the hub:\nWe began by analysing language model, model cards, in order to identify patterns (e.g repeated sections and subsections, with the aim of answering initial questions such as:\n\n1) How many of these models have model cards?\n   \n2) What percent of downloads had an associated model card?\n\nFrom our analysis of all the models on the hub, we noticed that the most downloads come from top 200 models.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/mc-downloads.png\"/>\n</div>\n\n\nWith a continued focus on large language models, ordered by most downloaded and only models with model cards to begin with, we noted the most recurring sections within their respective model cards. \n\nWhile some headings within model cards may differ between models, we grouped components/the theme of each section within each model cards and then mapped them to section headings that were the most recurring (mostly found in the top 200 downloaded models and with the aid/guidance of the Bloom model card)\n\n\n\n<Tip>\n\n [Checkout the User Studies](./model-cards-user-studies)\n\n </Tip>\n\n\n<Tip>\n\n [See Appendix](./model-card-appendix)\n\n </Tip>\n\n[^1]: For each tool, descriptions are excerpted from the linked paper listed in the second column.\n\n[^2]: See https://techpolicylab.uw.edu/data-statements/ .\n\n[^3]: See https://techpolicylab.uw.edu/data-statements/ .\n\n[^4]: See https://techpolicylab.uw.edu/data-statements/ .\n\n[^5]: See, e.g., the Hugging Face Hub, Google Cloud’s Model Cards https://modelcards.withgoogle.com/about .\n\n[^6]: See Appendix A.\n\n[^7]: See GSA / US Census Bureau Collaboration on Model Card Generator.",
        "question": "How many of the models on the Hugging Face Hub have model cards?\n",
        "answer": "The context does not provide a specific number for how many models on the Hugging Face Hub have model cards. However, it does mention that the analysis of all the models on the hub was done, so it can be inferred that all models on the hub were included in the analysis, regardless of whether they had model cards or not.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/model-card-landscape-analysis.md"
    },
    {
        "context": "|      |      |[RicardoLee/Llama2-chat-Chinese-50W](https://huggingface.co/RicardoLee/Llama2-chat-Chinese-50W)                                                    |13          |36      | llama2 |                                                 |[LICENSE](https://huggingface.co/RicardoLee/Llama2-chat-Chinese-50W/blob/main/LICENSE)                                  |                                                                                                    |             |\n|      |      |[ibm/roberta-large-vira-intents](https://huggingface.co/ibm/roberta-large-vira-intents)                                                            |12          |1       |                         |                                                                                   |[LICENSE](https://huggingface.co/ibm/roberta-large-vira-intents/blob/main/LICENSE)                                      |                                                                                                    |             |\n|      |      |[TheBloke/airoboros-l2-7B-gpt4-m2.0-GGUF](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GGUF)                                          |12          |3       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GGUF/blob/main/LICENSE.txt)                     |                                                                                                    |             |\n|      |      |[TheBloke/llama2-22B-daydreamer-v2-GPTQ](https://huggingface.co/TheBloke/llama2-22B-daydreamer-v2-GPTQ)                                            |11          |8       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama2-22B-daydreamer-v2-GPTQ/blob/main/LICENSE.txt)                      |                                                                                                    |             |",
        "question": "What is the name of the model with 22 billion parameters?\n",
        "answer": "TheBloke/llama2-22B-daydreamer-v2-GPTQ",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md"
    },
    {
        "context": "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization.\n\nIf you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.",
        "question": "How does the token merging method affect the optimization of the general-purpose Stable Diffusion model?\n",
        "answer": "The token merging method reduces the model capacity substantially, and the more complicated the dataset for training, the less merging ratio should be used during the optimization.",
        "source_doc": "huggingface/blog/blob/main/train-optimize-sd-intel.md"
    },
    {
        "context": "During inference:\n\n* The _quality_ of the predicted audio sample can be controlled by the `num_inference_steps` argument; higher steps give higher quality audio at the expense of slower inference.\n* The _length_ of the predicted audio sample can be controlled by varying the `audio_length_in_s` argument.\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n## AudioLDMPipeline\n[[autodoc]] AudioLDMPipeline\n\t- all\n\t- __call__\n\n## AudioPipelineOutput\n[[autodoc]] pipelines.AudioPipelineOutput",
        "question": "How can the quality of the predicted audio sample be controlled in the AudioPipeline?\n",
        "answer": "The quality of the predicted audio sample can be controlled by the `num_inference_steps` argument in the AudioPipeline; higher steps give higher quality audio at the expense of slower inference.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md"
    },
    {
        "context": "👉 The video tutorial: https://youtu.be/MEt6rrxH8W4\n\n```python\nfrom IPython.display import HTML\n\nHTML(\n    '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n)\n```\n\n## Add the Hugging Face Integration 🤗\n- In order to push our model to the Hub, we need to define a function `package_to_hub`\n\n- Add dependencies we need to push our model to the Hub\n\n```python\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\n\nmsg = Printer()\n```\n\n- Add new argument in `parse_args()` function to define the repo-id where we want to push the model.\n\n```python\n# Adding HuggingFace argument\nparser.add_argument(\n    \"--repo-id\",\n    type=str,\n    default=\"ThomasSimonini/ppo-CartPole-v1\",\n    help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\",\n)\n```\n\n- Next, we add the methods needed to push the model to the Hub\n\n- These methods will:\n  - `_evalutate_agent()`: evaluate the agent.\n  - `_generate_model_card()`: generate the model card of your agent.\n  - `_record_video()`: record a video of your agent.",
        "question": "What is the default repo-id for pushing the model to the Hugging Face Hub?\n",
        "answer": "The default repo-id for pushing the model to the Hugging Face Hub is \"ThomasSimonini/ppo-CartPole-v1\".",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx"
    },
    {
        "context": "config = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n```python\nimport torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `efficientnet_b0`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.",
        "question": "How to get the top-5 predictions class names using the model?\n",
        "answer": "The top-5 predictions class names can be obtained by using the torch.topk() function to get the top-5 probabilities and their corresponding class IDs, and then using the class IDs to get the class names from the imagenet\\_classes.txt file.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/efficientnet.md"
    },
    {
        "context": "## 3.39.0\n\n### Highlights\n\n#### Create Discord Bots from Gradio Apps 🤖 ([#4960](https://github.com/gradio-app/gradio/pull/4960) [`46e4ef67`](https://github.com/gradio-app/gradio/commit/46e4ef67d287dd68a91473b73172b29cbad064bc))\n\nWe're excited to announce that Gradio can now automatically create a discord bot from any `gr.ChatInterface` app.\n\nIt's as easy as importing `gradio_client`, connecting to the app, and calling `deploy_discord`!\n\n_🦙 Turning Llama 2 70b into a discord bot 🦙_\n\n```python\nimport gradio_client as grc\ngrc.Client(\"ysharma/Explore_llamav2_with_TGI\").deploy_discord(to_id=\"llama2-70b-discord-bot\")\n```\n\n<img src=\"https://gradio-builds.s3.amazonaws.com/demo-files/discordbots/guide/llama_chat.gif\">\n\n#### Getting started with template spaces\n\nTo help get you started, we have created an organization on Hugging Face called [gradio-discord-bots](https://huggingface.co/gradio-discord-bots) with template spaces you can use to turn state of the art LLMs powered by Gradio to discord bots.\n\nCurrently we have template spaces for:\n\n- [Llama-2-70b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/Llama-2-70b-chat-hf) powered by a FREE Hugging Face Inference Endpoint!\n- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/Llama-2-13b-chat-hf) powered by Hugging Face Inference Endpoints.\n- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/llama-2-13b-chat-transformers) powered by Hugging Face transformers.\n- [falcon-7b-instruct](https://huggingface.co/spaces/gradio-discord-bots/falcon-7b-instruct) powered by Hugging Face Inference Endpoints.\n- [gpt-3.5-turbo](https://huggingface.co/spaces/gradio-discord-bots/gpt-35-turbo), powered by openai. Requires an OpenAI key.\n\nBut once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate to try it on your own Chatbots.",
        "question": "How can I create a discord bot from a gradio app?\n",
        "answer": "You can create a discord bot from a gradio app by importing `gradio_client`, connecting to the app, and calling `deploy_discord`. For example, `grc.Client(\"ysharma/Explore_llamav2_with_TGI\").deploy_discord(to_id=\"llama2-70b-discord-bot\")`.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/spacy/en_core_web_sm\"><code>spacy/en_core_web_sm</code></a>",
        "question": "What is the size of the spacy/en_core_web_sm model?\n",
        "answer": "The size of the spacy/en\\_core\\_web\\_sm model is 110 MB.",
        "source_doc": "huggingface/blog/blob/main/spacy.md"
    },
    {
        "context": "- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https://github.com/tomchang25) in [PR 3968](https://github.com/gradio-app/gradio/pull/3968).\n- Fix the issue of the UI stuck caused by the 'selected' of DataFrame not being reset. by [@tomchang25](https://github.com/tomchang25) in [PR 3916](https://github.com/gradio-app/gradio/pull/3916).\n- Fix issue where `gr.Video()` would not work inside a `gr.Tab()` by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3891](https://github.com/gradio-app/gradio/pull/3891)\n- Fixed issue with old_value check in File. by [@tomchang25](https://github.com/tomchang25) in [PR 3859](https://github.com/gradio-app/gradio/pull/3859).\n- Fixed bug where all bokeh plots appeared in the same div by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3896](https://github.com/gradio-app/gradio/pull/3896)\n- Fixed image outputs to automatically take full output image height, unless explicitly set, by [@aliabid94](https://github.com/aliabid94) in [PR 3905](https://github.com/gradio-app/gradio/pull/3905)\n- Fix issue in `gr.Gallery()` where setting height causes aspect ratio of images to collapse by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3830](https://github.com/gradio-app/gradio/pull/3830)\n- Fix issue where requesting for a non-existing file would trigger a 500 error by [@micky2be](https://github.com/micky2be) in `[PR 3895](https://github.com/gradio-app/gradio/pull/3895)`.\n- Fix bugs with abspath about symlinks, and unresolvable path on Windows by [@micky2be](https://github.com/micky2be) in `[PR 3895](https://github.com/gradio-app/gradio/pull/3895)`.\n- Fixes type in client `Status` enum by [@10zinten](https://github.com/10zinten) in [PR 3931](https://github.com/gradio-app/gradio/pull/3931)\n- Fix `gr.ChatBot` to handle image url [tye-singwa](https://github.com/tye-signwa) in [PR 3953](https://github.com/gradio-app/gradio/pull/3953)",
        "question": "Which user fixed the issue of the UI stuck caused by the 'selected' of DataFrame not being reset?\n",
        "answer": "The issue of the UI stuck caused by the 'selected' of DataFrame not being reset was fixed by [@tomchang25](https://github.com/tomchang25).",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "### Audio Classification\n\nAudio Classification can receive `json` payloads or binary data from a `audio` directly.\n\n**JSON**\n\n```json\n{\n  \"inputs\": \"/9j/4AAQSkZJRgABAQEBLAEsAAD/2wBDAAMCAgI\"\n}\n```\n\n**Binary**\n```bash\ncurl --request POST \\\n  --url https://{ENDPOINT}/ \\\n  --header 'Content-Type: audio/x-flac' \\\n  --header 'Authorization: Bearer {HF_TOKEN}' \\\n  --data-binary '@sample.flac'\n```\n\n\n### Object Detection \n\nObject Detection can receive `json` payloads or binary data from a `image` directly.\n\n**JSON**\n\n```json\n{\n  \"inputs\": \"/9j/4AAQSkZJRgABAQEBLAEsAAD/2wBDAAMCAgI\"\n}\n```\n\n**Binary**\n\n```bash\ncurl --request POST \\\n  --url https://{ENDPOINT}/ \\\n  --header 'Content-Type: image/jpg' \\\n  --header 'Authorization: Bearer {HF_TOKEN}' \\\n  --data-binary '@test.jpg'\n```\n\n### Image Segmentation\n\nImage Segmentation can receive `json` payloads or binary data from a `image` directly.\n\n**JSON**\n\n```json\n{\n  \"inputs\": \"/9j/4AAQSkZJRgABAQEBLAEsAAD/2wBDAAMCAgI\"\n}\n```\n\n**Binary**\n\n```bash\ncurl --request POST \\\n  --url https://{ENDPOINT}/ \\\n  --header 'Content-Type: image/jpg' \\\n  --header 'Authorization: Bearer {HF_TOKEN}' \\\n  --data-binary '@test.jpg'\n```\n\n### Table Question Answering\n\n```json\n{\n  \"inputs\": {\n    \"query\": \"How many stars does the transformers repository have?\",\n    \"table\": {\n      \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n      \"Stars\": [\"36542\", \"4512\", \"3934\"],\n      \"Contributors\": [\"651\", \"77\", \"34\"],\n      \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]\n    }\n  }\n}\n```\n\n### Conversational\n\n```json\n{        \n  \"inputs\": {\n    \"past_user_inputs\": [\"Which movie is the best ?\"],\n    \"generated_responses\": [\"It's Die Hard for sure.\"],\n    \"text\": \"Can you explain why?\",\n  }\n}\n```\n\n### Text To Image",
        "question": "What type of data can Text Feature Extraction receive?\n",
        "answer": "Text Feature Extraction can receive `json` payloads.\n\nContext: ### Text Feature Extraction\n\n```json\n{\n  \"",
        "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/supported_tasks.mdx"
    },
    {
        "context": "Now, we need to pass the input image, the mask image, and the prompt\nembeddings.\n\n``` python\nimage = pipe(\n    image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds, \n    output_type=\"pt\",\n    generator=generator,\n).images\n```\n\nLet\\'s take a look at the intermediate output.\n\n``` python\npil_image = pt_to_pil(image)\npipe.watermarker.apply_watermark(pil_image, pipe.unet.config.sample_size)\n\npil_image[0]\n```\n\n![inpainted_output](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/inpainted_output.png)\n\nLooks good! The text is pretty consistent!\n\nLet\\'s free the memory so we can upscale the image\n\n``` python\ndel pipe\nflush()\n```\n\n### 3.3 Stage 2: Super Resolution \n\nFor super resolution, load the checkpoint with\n`IFInpaintingSuperResolutionPipeline`.\n\n``` python\nfrom diffusers import IFInpaintingSuperResolutionPipeline\n\npipe = IFInpaintingSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", \n    text_encoder=None, \n    variant=\"fp16\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\n```\n\nThe inpainting super resolution pipeline requires the generated image,\nthe original image, the mask image, and the prompt embeddings.\n\nLet\\'s do a final denoising run.\n\n``` python\nimage = pipe(\n    image=image,\n    original_image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds, \n    generator=generator,\n).images[0]\nimage\n```\n\n![inpainted_final_output](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/inpainted_final_output.png)\n\nNice, the model generated text without making a single\nspelling error!\n\n## Conclusion\n\nIF in 32-bit floating point precision uses 40 GB of weights in total. We\nshowed how using only open source models and libraries, IF can be run on\na free-tier Google Colab instance.",
        "question": "How many GB of weights does IF in 32-bit floating point precision use in total?\n",
        "answer": "IF in 32-bit floating point precision uses 40 GB of weights in total.",
        "source_doc": "huggingface/blog/blob/main/if.md"
    },
    {
        "context": "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Models\n\n[`PeftModel`] is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base `PeftModel` contains methods for loading and saving models from the Hub.\n\n## PeftModel\n\n[[autodoc]] PeftModel\n    - all\n\n## PeftModelForSequenceClassification\n\nA `PeftModel` for sequence classification tasks.\n\n[[autodoc]] PeftModelForSequenceClassification\n    - all\n\n## PeftModelForTokenClassification\n\nA `PeftModel` for token classification tasks.\n\n[[autodoc]] PeftModelForTokenClassification\n    - all\n\n## PeftModelForCausalLM\n\nA `PeftModel` for causal language modeling.\n\n[[autodoc]] PeftModelForCausalLM\n    - all\n\n## PeftModelForSeq2SeqLM\n\nA `PeftModel` for sequence-to-sequence language modeling.\n\n[[autodoc]] PeftModelForSeq2SeqLM\n    - all\n\n## PeftModelForQuestionAnswering\n\nA `PeftModel` for question answering.\n\n[[autodoc]] PeftModelForQuestionAnswering\n    - all\n\n## PeftModelForFeatureExtraction\n\nA `PeftModel` for getting extracting features/embeddings from transformer models.\n\n[[autodoc]] PeftModelForFeatureExtraction\n    - all\n\n## Utilities\n\n[[autodoc]] get_peft_model\n\n[[autodoc]] utils.prepare_model_for_kbit_training",
        "question": "What is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to?\n",
        "answer": "PeftModel",
        "source_doc": "huggingface/peft/blob/main/docs/source/package_reference/peft_model.md"
    },
    {
        "context": "![adam_gpt2.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/adam_gpt2.png)\n\n![adam_gpt2_xl.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/adam_gpt2_xl.png)\n\n# Limitations\n\nNoticed this work does not try to reproduce the summarization work in CNN DM or TL;DR. This was because we found the training to be time-consuming and brittle. \n\nThe particular training run we had showed poor GPU utilization (around 30%), so it takes almost 4 days to perform a training run, which is highly expensive (only AWS sells p3dn.24xlarge, and it costs $31.212 per hour)\n\nAdditionally, training was brittle. While the reward goes up, we find it difficult to reproduce the “smart copier” behavior reported by Ziegler et al. (2019). Below are some sample outputs — clearly, the agent overfits somehow. See [https://wandb.ai/openrlbenchmark/lm-human-preferences/runs/1ab47rqi/logs](https://wandb.ai/openrlbenchmark/lm-human-preferences/runs/1ab47rqi/logs?workspace=user-costa-huang) for more complete logs.\n\n![tldr1.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/tldr1.png)\n\n![tldr2.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/tldr2.png)\n\n# Conclusion\n\nIn this work, we took a deep dive into OAI’s original RLHF codebase and compiled a list of its implementation details. We also created a minimal base which reproduces the same learning curves as OAI’s original RLHF codebase, when the dataset and hyperparameters are controlled. Furthermore, we identify surprising implementation details such as the adam optimizer’s setting which causes aggressive updates in early RLHF training. \n\n# Acknowledgement\n\nThis work is supported by Hugging Face’s Big Science cluster 🤗. We also thank the helpful discussion with @lewtun and @natolambert.\n\n\n# Bibtex",
        "question": "What is the cost of AWS p3dn.24xlarge per hour?\n",
        "answer": "The cost of AWS p3dn.24xlarge per hour is $31.212.",
        "source_doc": "huggingface/blog/blob/main/the_n_implementation_details_of_rlhf_with_ppo.md"
    },
    {
        "context": "|      |                                                                            |[Enoch/llama-7b-hf](https://huggingface.co/Enoch/llama-7b-hf)|123         |0                        |llama-license                                                                                 |https://huggingface.co/Enoch/llama-7b-hf/blob/main/LICENSE                                    |[LICENSE](https://huggingface.co/Enoch/llama-7b-hf/blob/main/LICENSE)                              |                                                                                                                     |                                                                                   |\n|      |                                                                            |[TheBloke/MythoMax-L2-13B-GGUF](https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF)|119         |9                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/blob/main/LICENSE.txt)          |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the model TheBloke/MythoMax-L2-13B-GGUF?\n",
        "answer": "The license for the model TheBloke/MythoMax-L2-13B-GGUF is llama-2-community-license.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "## 0.4.0-beta.8\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#5966](https://github.com/gradio-app/gradio/pull/5966) [`9cad2127b`](https://github.com/gradio-app/gradio/commit/9cad2127b965023687470b3abfe620e188a9da6e) - Improve Audio Component. Thanks [@hannahblair](https://github.com/hannahblair)!\n\n### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.0-beta.7\n\n### Patch Changes\n\n- Updated dependencies [[`174b73619`](https://github.com/gradio-app/gradio/commit/174b736194756e23f51bbaf6f850bac5f1ca95b5), [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84)]:\n  - @gradio/wasm@0.2.0-beta.1\n\n## 0.4.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Fix deployed demos on v4 branch. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.0\n\n### Features",
        "question": "Which user contributed to the improvement of the Audio Component in version 0.4.0-beta.8?\n",
        "answer": "@hannahblair",
        "source_doc": "gradio-app/gradio/blob/main/js/audio/CHANGELOG.md"
    },
    {
        "context": "## VGG [[vgg.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py)]\n* Paper: `Very Deep Convolutional Networks For Large-Scale Image Recognition` - https://arxiv.org/pdf/1409.1556.pdf\n* Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n\n## Vision Transformer [[vision_transformer.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py)]\n* Paper: `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale` - https://arxiv.org/abs/2010.11929\n* Reference code and pretrained weights: https://github.com/google-research/vision_transformer\n\n## VovNet V2 and V1 [[vovnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vovnet.py)]\n* Paper: `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n* Reference code: https://github.com/youngwanLEE/vovnet-detectron2\n\n## Xception [[xception.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/xception.py)]\n* Paper: `Xception: Deep Learning with Depthwise Separable Convolutions` - https://arxiv.org/abs/1610.02357\n* Code: https://github.com/Cadene/pretrained-models.pytorch\n\n## Xception (Modified Aligned, Gluon) [[gluon_xception.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/gluon_xception.py)]\n* Paper: `Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation` - https://arxiv.org/abs/1802.02611\n* Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo, https://github.com/jfzhang95/pytorch-deeplab-xception/",
        "question": "What is the name of the paper that introduced Xception?\n",
        "answer": "The name of the paper that introduced Xception is 'Xception: Deep Learning with Depthwise Separable Convolutions'.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models.md"
    },
    {
        "context": "- [#5081](https://github.com/gradio-app/gradio/pull/5081) [`d7f83823`](https://github.com/gradio-app/gradio/commit/d7f83823fbd7604456b0127d689a63eed759807d) - solve how can I config root_path dynamically? #4968. Thanks [@eastonsuo](https://github.com/eastonsuo)!\n- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!\n- [#5133](https://github.com/gradio-app/gradio/pull/5133) [`61129052`](https://github.com/gradio-app/gradio/commit/61129052ed1391a75c825c891d57fa0ad6c09fc8) - Update dependency esbuild to ^0.19.0. Thanks [@renovate](https://github.com/apps/renovate)!\n- [#5125](https://github.com/gradio-app/gradio/pull/5125) [`80be7a1c`](https://github.com/gradio-app/gradio/commit/80be7a1ca44c0adef1668367b2cf36b65e52e576) - chatbot conversation nodes can contain a copy button. Thanks [@fazpu](https://github.com/fazpu)!\n- [#5048](https://github.com/gradio-app/gradio/pull/5048) [`0b74a159`](https://github.com/gradio-app/gradio/commit/0b74a1595b30df744e32a2c358c07acb7fd1cfe5) - Use `importlib` in favor of deprecated `pkg_resources`. Thanks [@jayceslesar](https://github.com/jayceslesar)!\n- [#5045](https://github.com/gradio-app/gradio/pull/5045) [`3b9494f5`](https://github.com/gradio-app/gradio/commit/3b9494f5c57e6b52e6a040ce8d6b5141f780e84d) - Lite: Fix the analytics module to use asyncio to work in the Wasm env. Thanks [@whitphx](https://github.com/whitphx)!\n- [#5046](https://github.com/gradio-app/gradio/pull/5046) [`5244c587`](https://github.com/gradio-app/gradio/commit/5244c5873c355cf3e2f0acb7d67fda3177ef8b0b) - Allow new lines in `HighlightedText` with `/n` and preserve whitespace. Thanks [@hannahblair](https://github.com/hannahblair)!",
        "question": "What is the new version of the dependency esbuild?\n",
        "answer": "The new version of the dependency esbuild is ^0.19.0.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "```python\nmodel.eval()\ntest_preds = []\n\nfor _, batch in enumerate(tqdm(test_dataloader)):\n    batch = {k: v for k, v in batch.items() if k != \"labels\"}\n    with torch.no_grad():\n        outputs = model.generate(**batch, max_new_tokens=10)\n    preds = outputs[:, max_length:].detach().cpu().numpy()\n    test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n    if len(test_preds) > 100:\n        break\ntest_preds\n```",
        "question": "What is the maximum number of new tokens generated by the model?\n",
        "answer": "The maximum number of new tokens generated by the model is 10.",
        "source_doc": "huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb"
    },
    {
        "context": "#### **1. How has ML made a positive impact on SaaS?**\nMachine learning has become truly operational in SaaS, powering multiple uses from personalization, semantic and image search, recommendations to anomaly detection, and a ton of other business scenarios. The real impact is that ML comes baked right into more and more applications. It's becoming an expectation and more often than not it's invisible to end users. \nFor example, at Elastic we invested in ML for anomaly detection, optimized for endpoint security and SIEM. It delivers some heavy firepower out of the box with an amalgamation of different techniques like time series decomposition, clustering, correlation analysis, and Bayesian distribution modeling. The big benefit for security analysts is threat detection is automated in many different ways. So anomalies are quickly bubbled up related to temporal deviations, unusual geographic locations, statistical rarity, and many other factors. That's the huge positive impact of integrating ML. \n \n#### **2. What are the biggest ML challenges within SaaS?**\nTo maximize the benefits of ML there is a double challenge of delivering value to users that are new to machine learning and also to seasoned data scientists. There's obviously a huge difference in demands for these two folks. If an ML capability is a total black box it's likely to be too rigid or simple to have a real impact. On the other hand, if you solely deliver a developer toolkit it's only useful if you have a data science team in-house. Striking the right balance is about making sure ML is open enough for the data science team to have transparency and control over models and also packing in battle-tested models that are easy to configure and deploy without being a pro.",
        "question": "What is an example of a SaaS company that has invested in ML for anomaly detection?\n",
        "answer": "Elastic",
        "source_doc": "huggingface/blog/blob/main/ml-director-insights-4.md"
    },
    {
        "context": ">>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> labels = tokenizer(\n...     \"an image of two cats chilling on a couch\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(pixel_values=pixel_values, labels=labels).loss\n```\n\nThis model was contributed by [nielsr](https://github.com/nielsrogge). This model's TensorFlow and Flax versions\nwere contributed by [ydshieh](https://github.com/ydshieh).\n\n## VisionEncoderDecoderConfig\n\n[[autodoc]] VisionEncoderDecoderConfig\n\n<frameworkcontent>\n<pt>\n\n## VisionEncoderDecoderModel\n\n[[autodoc]] VisionEncoderDecoderModel\n    - forward\n    - from_encoder_decoder_pretrained\n\n</pt>\n<tf>\n\n## TFVisionEncoderDecoderModel\n\n[[autodoc]] TFVisionEncoderDecoderModel\n    - call\n    - from_encoder_decoder_pretrained\n\n</tf>\n<jax>\n\n## FlaxVisionEncoderDecoderModel\n\n[[autodoc]] FlaxVisionEncoderDecoderModel\n    - __call__\n    - from_encoder_decoder_pretrained\n\n</jax>\n</frameworkcontent>",
        "question": "What is the id of the decoder start token?\n",
        "answer": "The id of the decoder start token is the same as the id of the cls token of the tokenizer.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/vision-encoder-decoder.md"
    },
    {
        "context": "|        [Vision Encoder decoder](model_doc/vision-encoder-decoder)        |       ✅        |         ✅         |      ✅      |\n|       [VisionTextDualEncoder](model_doc/vision-text-dual-encoder)        |       ✅        |         ✅         |      ✅      |\n|                   [VisualBERT](model_doc/visual_bert)                    |       ✅        |         ❌         |      ❌      |\n|                           [ViT](model_doc/vit)                           |       ✅        |         ✅         |      ✅      |\n|                    [ViT Hybrid](model_doc/vit_hybrid)                    |       ✅        |         ❌         |      ❌      |\n|                        [VitDet](model_doc/vitdet)                        |       ✅        |         ❌         |      ❌      |\n|                       [ViTMAE](model_doc/vit_mae)                        |       ✅        |         ✅         |      ❌      |\n|                      [ViTMatte](model_doc/vitmatte)                      |       ✅        |         ❌         |      ❌      |\n|                       [ViTMSN](model_doc/vit_msn)                        |       ✅        |         ❌         |      ❌      |\n|                          [VITS](model_doc/vits)                          |       ✅        |         ❌         |      ❌      |\n|                         [ViViT](model_doc/vivit)                         |       ✅        |         ❌         |      ❌      |\n|                      [Wav2Vec2](model_doc/wav2vec2)                      |       ✅        |         ✅         |      ✅      |\n|            [Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)            |       ✅        |         ❌         |      ❌      |\n|              [Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)               |       ✅        |         ✅         |      ✅      |\n|                         [WavLM](model_doc/wavlm)                         |       ✅        |         ❌         |      ❌      |",
        "question": "Which models support the ✅ symbol?\n",
        "answer": "The models that support the ✅ symbol are Vision Encoder decoder, VisionTextDualEncoder, ViT, VitDet, ViTMAE, ViTMatte, ViTMSN, and Wav2Vec2.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/index.md"
    },
    {
        "context": "[`hf-internal-testing`](https://huggingface.co/hf-internal-testing) or [huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images) to place these files.\nIf an external contribution, feel free to add the images to your PR and ask a Hugging Face member to migrate your images\nto this dataset.",
        "question": "Where should I place the files if I want to contribute to the huggingface/documentation-images dataset?\n",
        "answer": "If an external contribution, feel free to add the images to your PR and ask a Hugging Face member to migrate your images to this dataset.",
        "source_doc": "huggingface/diffusers/blob/main/CONTRIBUTING.md"
    },
    {
        "context": "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n## 0.2.2\n\n### Patch Changes\n\n- Updated dependencies [[`4e62b8493`](https://github.com/gradio-app/gradio/commit/4e62b8493dfce50bafafe49f1a5deb929d822103), [`e70805d54`](https://github.com/gradio-app/gradio/commit/e70805d54cc792452545f5d8eccc1aa0212a4695)]:\n  - @gradio/client@0.5.2\n  - @gradio/atoms@0.2.0\n  - @gradio/statustracker@0.2.3\n  - @gradio/upload@0.3.3\n\n## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`796145e2c`](https://github.com/gradio-app/gradio/commit/796145e2c48c4087bec17f8ec0be4ceee47170cb)]:\n  - @gradio/client@0.5.1\n\n## 0.2.0\n\n### Highlights\n\n#### new `FileExplorer` component ([#5672](https://github.com/gradio-app/gradio/pull/5672) [`e4a307ed6`](https://github.com/gradio-app/gradio/commit/e4a307ed6cde3bbdf4ff2f17655739addeec941e))\n\nThanks to a new capability that allows components to communicate directly with the server _without_ passing data via the value, we have created a new `FileExplorer` component.\n\nThis component allows you to populate the explorer by passing a glob, but only provides the selected file(s) in your prediction function.",
        "question": "What is the new component added in version 0.2.0 of gradio?\n",
        "answer": "The new component added in version 0.2.0 of gradio is `FileExplorer`.",
        "source_doc": "gradio-app/gradio/blob/main/js/file/CHANGELOG.md"
    },
    {
        "context": "In `transformers`, we simply set the parameter `num_return_sequences` to\nthe number of highest scoring beams that should be returned. Make sure\nthough that `num_return_sequences <= num_beams`\\!\n\n\n\n``` python\n# set return_num_sequences > 1\nbeam_outputs = model.generate(\n    **model_inputs,\n    max_new_tokens=40,\n    num_beams=5,\n    no_repeat_ngram_size=2,\n    num_return_sequences=5,\n    early_stopping=True\n)\n\n# now we have 3 output sequences\nprint(\"Output:\\n\" + 100 * '-')\nfor i, beam_output in enumerate(beam_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n```\n\n```\nOutput:\n----------------------------------------------------------------------------------------------------\n0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea to\n3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time to take a\n4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea.\n```\n\n\nAs can be seen, the five beam hypotheses are only marginally different\nto each other - which should not be too surprising when using only 5\nbeams.\n\nIn open-ended generation, a couple of reasons have been brought\nforward why beam search might not be the best possible option:",
        "question": "What is a reason why beam search might not be the best possible option in open-ended generation?\n",
        "answer": "One reason is that beam search can result in marginally different hypotheses that are not significantly distinct from each other.",
        "source_doc": "huggingface/blog/blob/main/how-to-generate.md"
    },
    {
        "context": "--\ntitle: ROUGE\nemoji: 🤗 \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.py\npinned: false\ntags:\n- evaluate\n- metric\ndescription: >-\n  ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\n  evaluating automatic summarization and machine translation software in natural language processing.\n  The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n  \n  Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n  \n  This metrics is a wrapper around Google Research reimplementation of ROUGE:\n  https://github.com/google-research/google-research/tree/master/rouge\n---\n\n# Metric Card for ROUGE\n\n## Metric Description\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n\nThis metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)\n\n## How to Use\nAt minimum, this metric takes as input a list of predictions and a list of references:\n```python\n>>> rouge = evaluate.load('rouge')\n>>> predictions = [\"hello there\", \"general kenobi\"]\n>>> references = [\"hello there\", \"general kenobi\"]\n>>> results = rouge.compute(predictions=predictions,\n...                         references=references)\n>>> print(results)\n{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n```",
        "question": "What is ROUGE used for?\n",
        "answer": "ROUGE is used for evaluating automatic summarization and machine translation software in natural language processing. It compares an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/rouge/README.md"
    },
    {
        "context": "Finally, there is growing research interest in [Embodied AI](https://ieeexplore.ieee.org/iel7/7433297/9741092/09687596.pdf). This is an area of AI research where state-of-the-art performance is still orders of magnitude below human performance, with much of the challenge being in representing 3D space. Given that 3D Gaussian Splatting yields a very dense representation of 3D space, what might the implications be for Embodied AI research?\n\nThese questions call attention to the method. It remains to be seen what the actual impact will be.\n\n## The future of graphics\n\nSo what does this mean for the future of graphics? Well, let's break it up into pros/cons:\n\n**Pros**\n1. High-quality, photorealistic scenes\n2. Fast, real-time rasterization\n3. Relatively fast to train\n\n**Cons**\n1. High VRAM usage (4GB to view, 12GB to train)\n2. Large disk size (1GB+ for a scene)\n3. Incompatible with existing rendering pipelines\n3. Static (for now)\n\nSo far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.\n\nThere have already been the following adaptations:\n1. [Remote viewer](https://huggingface.co/spaces/dylanebert/gaussian-viewer)\n2. [WebGPU viewer](https://github.com/cvlab-epfl/gaussian-splatting-web)\n3. [WebGL viewer](https://huggingface.co/spaces/cakewalk/splat)\n4. [Unity viewer](https://github.com/aras-p/UnityGaussianSplatting)\n5. [Optimized WebGL viewer](https://gsplat.tech/)\n\nThese rely either on remote streaming (1) or a traditional quad-based rasterization approach (2-5). While a quad-based approach is compatible with decades of graphics technologies, it may result in lower quality/performance. However, [viewer #5](https://gsplat.tech/) demonstrates that optimization tricks can result in high quality/performance, despite a quad-based approach.",
        "question": "How has 3D Gaussian Splatting been adapted for use in graphics?\n",
        "answer": "3D Gaussian Splatting has been adapted for use in graphics through remote viewing, WebGPU, WebGL, Unity, and optimized WebGL viewers. These rely on remote streaming or traditional quad-based rasterization approaches, with varying levels of quality and performance.",
        "source_doc": "huggingface/blog/blob/main/gaussian-splatting.md"
    },
    {
        "context": "ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere, Python tells you exactly where, and you fix it. This is great - it's simple and satisfying. Sometimes, though, things crash and the error is impossible to understand. This happens a lot in machine learning for a few reasons - you're working with big data structures, using big, complex libraries with a lot of moving parts, and also you're doing a lot of GPU computing. In Keras there's the added bonus problem that your models are often compiled before execution, which is great for performance but makes debugging them very difficult. This is going to be a video about what to do when you run into one of those nightmare bugs. To give you some intuitions for what can go wrong, and where to look for the source of bugs that you encounter, let's use this example script, and I'll show it to you here in two parts. First, we do all our imports, we load a dataset, we create our tokenizer and we tokenize the dataset. Next, we convert our datasets to TensorFlow datasets, so that we can run fit() on them, and then we load our model from a pretrained checkpoint, compile it and fit it.  It seems straightforward enough, but beware! This spooky code hides many dark and mysterious secrets. What happens when we run it? Well, this isn't great. What does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing - how do we even begin to debug something like that? When the error you get doesn't immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that things look right. And of course, the place to start is always to check your data. The best way to do that to grab a batch from the tf.data.Dataset that your model is training on, right at the end of the training pipeline. And we can do that like so, by looping over the dataset for one iteration and then breaking",
        "question": "What is the best way to check the data when debugging a machine learning model?\n",
        "answer": "The best way to check the data when debugging a machine learning model is to grab a batch from the tf.data.Dataset that your model is training on, right at the end of the training pipeline.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter8/04_debug-tf.md"
    },
    {
        "context": "A GNN is made of successive layers. A GNN layer represents a node as the combination (**aggregation**) of the representations of its neighbours and itself from the previous layer (**message passing**), plus usually an activation to add some nonlinearity.\n\n**Comparison to other models**: A CNN can be seen as a GNN with fixed neighbour sizes (through the sliding window) and ordering (it is not permutation equivariant). A [Transformer](https://arxiv.org/abs/1706.03762v3) without positional embeddings can be seen as a GNN on a fully-connected input graph.\n\n### Aggregation and message passing\n\nThere are many ways to aggregate messages from neighbour nodes, summing, averaging, for example. Some notable works following this idea include:\n\n- [Graph Convolutional Networks](https://tkipf.github.io/graph-convolutional-networks/) averages the normalised representation of the neighbours for a node (most GNNs are actually GCNs);\n- [Graph Attention Networks](https://petar-v.com/GAT/) learn to weigh the different neighbours based on their importance (like transformers);\n- [GraphSAGE](https://snap.stanford.edu/graphsage/) samples neighbours at different hops before aggregating their information in several steps with max pooling.\n- [Graph Isomorphism Networks](https://arxiv.org/pdf/1810.00826v3.pdf) aggregates representation by applying an MLP to the sum of the neighbours' node representations.\n\n**Choosing an aggregation**: Some aggregation techniques (notably mean/max pooling) can encounter failure cases when creating representations which finely differentiate nodes with different neighbourhoods of similar nodes (ex: through mean pooling, a neighbourhood with 4 nodes, represented as 1,1,-1,-1, averaged as 0, is not going to be different from one with only 3 nodes represented as -1, 0, 1).\n\n### GNN shape and the over-smoothing problem\n\nAt each new layer, the node representation includes more and more nodes.",
        "question": "What is the name of the problem that occurs when the node representation includes more and more nodes at each new layer?\n",
        "answer": "The over-smoothing problem",
        "source_doc": "huggingface/blog/blob/main/intro-graphml.md"
    },
    {
        "context": "```python\ninstance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\nresults = instance_segmentation(Image.open(image))\nresults\n```\n\nAs you can see below, there are multiple cars classified, and there's no classification for pixels other than pixels that belong to car and person instances.\n\n```bash\n[{'score': 0.999944,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999945,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999652,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.903529,\n  'label': 'person',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\nChecking out one of the car masks below.\n\n```python\nresults[2][\"mask\"]\n```\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>\n\nPanoptic segmentation combines semantic segmentation and instance segmentation, where every pixel is classified into a class and an instance of that class, and there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) for this.\n\n```python\npanoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\nresults = panoptic_segmentation(Image.open(image))\nresults\n```\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is classified into one of the classes.",
        "question": "What is the class of the first mask in the panoptic segmentation results?\n",
        "answer": "The class of the first mask in the panoptic segmentation results is 'car'.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md"
    },
    {
        "context": "## Conclusion\n\nWe're very excited about Llama 2 being out! In the incoming days, be ready to learn more about ways to run your own fine-tuning, execute the smallest models on-device, and many other exciting updates we're prepating for you!",
        "question": "What is Llama 2?\n",
        "answer": "Llama 2 is a fine-tuning model that has recently been released.",
        "source_doc": "huggingface/blog/blob/main/llama2.md"
    },
    {
        "context": "#### Approximating the reverse process\n\nAn encoder-decoder transformer approximates the classes of the un-noised latents, \\\\( x_0 \\\\), conditioned on the prompt, \\\\( y \\\\). The encoder is a CLIP text encoder with frozen weights. The decoder transformer provides unmasked global attention to all latent pixels and outputs the log probabilities of the categorical distribution over vector embeddings. The decoder transformer predicts the entire distribution of un-noised latents in one forward pass, providing global self-attention over \\\\( x_t \\\\). Framing the problem as conditional sequence to sequence over discrete values provides some intuition for why the encoder-decoder transformer is a good fit. \n\nThe AR models section provides additional context on VQ-Diffusion's architecture in comparison to AR transformer based models.\n\n[Taming Transformers](https://arxiv.org/abs/2012.09841) provides a good discussion on converting raw pixels to discrete tokens in a compressed latent space so that transformers become computationally feasible for image data.\n\n### VQ-Diffusion in Context\n\n#### Diffusion Models\n\nContemporary diffusion models are mostly continuous. In the forward process, continuous diffusion models iteratively add Gaussian noise. The reverse process is approximated via \\\\( p_{\\theta}(x_{t-1} | x_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t)) \\\\). In the simpler case of [DDPM](https://arxiv.org/abs/2006.11239), the covariance matrix is fixed, a U-Net is trained to predict the noise in \\\\( x_t \\\\), and \\\\( x_{t-1} \\\\) is derived from the noise. \n\nThe approximate reverse process is structurally similar to the discrete reverse process. However in the discrete case, there is no clear analog for predicting the noise in \\\\( x_t \\\\), and directly predicting the distribution for \\\\( x_0 \\\\) is a more clear objective.",
        "question": "What is the main difference between continuous and discrete diffusion models?\n",
        "answer": "The main difference is that continuous diffusion models iteratively add Gaussian noise in the forward process, while discrete diffusion models do not have a clear analog for predicting the noise in \\\\( x_t \\\\), and directly predicting the distribution for \\\\( x_0 \\\\) is a more clear objective in the reverse process.",
        "source_doc": "huggingface/blog/blob/main/vq-diffusion.md"
    },
    {
        "context": "Unfortunately, inference is slow since each of our 4 class names must be fed through the large model for every\nsequence to be classified. But with our unlabeled data we can distill the model to a small distilbert classifier to\nmake future inference much faster.\n\nTo run the script, we will need to put each training example (text only) from AG's News on its own line in\n`agnews/train_unlabeled.txt`, and each of the four class names in the newline-separated `agnews/class_names.txt`.\nThen we can run distillation with the following command:\n\n```bash\npython distill_classifier.py \\\n--data_file ./agnews/unlabeled.txt \\\n--class_names_files ./agnews/class_names.txt \\\n--teacher_name_or_path roberta-large-mnli \\\n--hypothesis_template \"This text is about {}.\" \\\n--output_dir ./agnews/distilled\n```\n\nThe script will generate a set of soft zero-shot predictions from `roberta-large-mnli` for each example in\n`agnews/unlabeled.txt`. It will then train a student distilbert classifier on the teacher predictions and\nsave the resulting model in `./agnews/distilled`.\n\nThe resulting model can then be loaded and used like any other pre-trained classifier:\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"./agnews/distilled\")\ntokenizer = AutoTokenizer.from_pretrained(\"./agnews/distilled\")\n```\n\nand even used trivially with a `TextClassificationPipeline`:\n\n```python\n>>> distilled_classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n>>> distilled_classifier(sequence)\n[[{'label': 'the world', 'score': 0.14899294078350067},\n  {'label': 'sports', 'score': 0.03205857425928116},\n  {'label': 'business', 'score': 0.05943061783909798},\n  {'label': 'science/tech', 'score': 0.7595179080963135}]]\n```\n\n> Tip: pass `device=0` when constructing a pipeline to run on a GPU",
        "question": "How can the inference process be made faster with unlabeled data?\n",
        "answer": "The inference process can be made faster with unlabeled data by distilling the model to a small distilbert classifier using the unlabeled data.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/zero-shot-distillation/README.md"
    },
    {
        "context": "The other option is to run a script locally. While this can be more difficult to set up, it also means that you have more control over the training run and probably access to better GPUs than you would have in a google colab. \nFor small datasets, it is usually totally sufficient to train your model\nin a google colab. For larger and thus more memory-intensive datasets, it is probably\nbetter to fine-tune the model locally.\n\nFor each option, we explain in detail how to fine-tune XLSR-Wav2Vec2 in the following.\n\n### Google colab setup\n\n**Note**: Instead of reading the following section, you can simply watch [this](https://www.youtube.com/watch?v=UynYn2C3tI0&ab_channel=PatrickvonPlaten) video, where Patrick explains how to adapt the google colab for your specific language.\n\n**1.**: If you plan on training XLSR-Wav2Vec2 in a google colab, you should first make sure to have a valid gmail account. You can sign up for a gmail account [here](https://accounts.google.com/signup/v2/webcreateaccount?hl=en&flowName=GlifWebSignIn&flowEntry=SignUp). \nHaving successfully signed up for gmail, you can now sign in to your account to make sure you are logged in when opening new tabs in your browser.\n\n**2.**: Next, head over to the official [Fine-Tune XLSR-Wav2Vec2 with 🤗 Transformes](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLSR_Wav2Vec2_on_Turkish_ASR_with_%F0%9F%A4%97_Transformers.ipynb) google colab. The first thing you should do is to make a copy of it - click `->File->Save a copy in Drive`. This should save a copy of the google colab in your google drive.",
        "question": "How do I make a copy of the Fine-Tune XLSR-Wav2Vec2 with 🤗 Transformers google colab?\n",
        "answer": "To make a copy of the Fine-Tune XLSR-Wav2Vec2 with 🤗 Transformers google colab, click `->File->Save a copy in Drive` in the google colab.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md"
    },
    {
        "context": "\"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_test.json\": {\n                \"num_bytes\": 5023228,\n                \"checksum\": null\n            }\n        },\n        \"download_size\": 34462660,\n        \"dataset_size\": 340657958,\n        \"size_in_bytes\": 375120618\n    }\n}\n```",
        "question": "What is the checksum of the file that contains the \"validation\" split of the \"duorc\" dataset?\n",
        "answer": "null\n```",
        "source_doc": "huggingface/datasets-server/blob/main/docs/source/info.mdx"
    },
    {
        "context": "1. `rescale_betas_zero_snr=True`, rescales the noise schedule to zero terminal signal-to-noise ratio (SNR)\n2. `timestep_spacing=\"trailing\"`, starts sampling from the last timestep\n\n```py\nfrom diffusers import DiffusionPipeline, DDIMScheduler\n\npipeline = DiffusionPipeline.from_pretrained(\"ptx0/pseudo-journey-v2\", use_safetensors=True)\n\n# switch the scheduler in the pipeline to use the DDIMScheduler\npipeline.scheduler = DDIMScheduler.from_config(\n    pipeline.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing=\"trailing\"\n)\npipeline.to(\"cuda\")\n```\n\nFinally, in your call to the pipeline, set `guidance_rescale` to prevent overexposure:\n\n```py\nprompt = \"A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k\"\nimage = pipeline(prompt, guidance_rescale=0.7).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/zero_snr.png\"/>\n</div>",
        "question": "What does `timestep_spacing=\"trailing\"` do in the context?\n",
        "answer": "`timestep_spacing=\"trailing\"` starts sampling from the last timestep in the context.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/control_brightness.md"
    },
    {
        "context": "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Intel Ice Lake Xeon 8380 Specifications\" src=\"assets/35_bert_cpu_scaling_part_2/intel_xeon_8380_specs.svg\"></medium-zoom>\n  <figcaption>Figure 3. Intel Ice Lake Xeon 8380 Specifications</figcaption>\n</figure>\n<br>\n\n\n### Establishing the baseline\n\nAs mentioned previously, the baselines will be composed of two different setups: \n-\tOut-of-the-box: We are running the workloads as-is, without any tuning\n-\tOptimized: We apply the various knobs present in [Blog #1](https://hf.co/blog/bert-cpu-scaling-part-1#2-benchmarking-methodology)\n\nAlso, from the comments we had about the previous blog post, we wanted to change the way we present the framework within the resulting benchmarks. \nAs such, through the rest of this second blog post, we will split framework benchmarking results according to the following:\n- Frameworks using “eager” mode for computations (PyTorch, TensorFlow)\n- Frameworks using “graph” mode for computations (TorchScript, TensorFlow Graph, Intel Tensorflow)\n\n\n#### Baseline: Eager frameworks latencies \n\nFrameworks operating in eager mode usually discover the actual graph while executing it. \nMore precisely, the actual computation graph is not known beforehand and you gradually (_eagerly_) execute one operator\nwhich will become the input of the next one, etc. until you reach leaf nodes (outputs).\n\nThese frameworks usually provide more flexibility in the algorithm you implement at the cost of increased runtime overhead\nand slightly potential more memory usage to keep track of all the required elements for the backward pass.",
        "question": "What is the number of",
        "answer": "The number of operations of eager frameworks in the baseline is 1,100,000,000 for PyTorch and 1,100,000,000 for TensorFlow.\n\n\n#### Baseline: Graph frameworks number of operations\n\nNumber of operations is the number of operations used by the model to perform the inference.\n\nOutput:::\nFactoid question: What is the number of",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-2.md"
    },
    {
        "context": "Let's try the impossible - let's train [t5-3b](https://huggingface.co/t5-3b) on a 24GB RTX-3090 card.\n\nFirst let's try to finetune the huge `t5-3b` using the normal single GPU setup:\n```\nexport BS=1\nCUDA_VISIBLE_DEVICES=0 ./finetune_trainer.py \\\n--model_name_or_path t5-3b --n_train 60 --n_val 10 \\\n--per_device_eval_batch_size $BS --per_device_train_batch_size $BS \\\n--task translation_en_to_ro --fp16 [...]\n```\nNo cookie, even with BS=1 we get:\n```\nRuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity;\n21.37 GiB already allocated; 45.69 MiB free; 22.05 GiB reserved in total by PyTorch)\n```\n\nNote, as earlier I'm showing only the important parts and the full command line arguments can be found\n[here](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).\n\nNow update your `transformers` to v4.2.0 or higher, then install DeepSpeed:\n```\npip install deepspeed\n```\n\nand let's try again, this time adding DeepSpeed to the command line:\n```\nexport BS=20\nCUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 ./finetune_trainer.py \\\n--model_name_or_path t5-3b --n_train 60 --n_val 10 \\\n--per_device_eval_batch_size $BS --per_device_train_batch_size $BS \\\n--task translation_en_to_ro --fp16 --deepspeed ds_config_1gpu.json [...]\n```\net voila! We get a batch size of 20 trained just fine. I could probably push it even further. The program failed with OOM at ``BS=30``.\n\nHere are the relevant results:\n```\n2021-01-12 19:06:31 | INFO | __main__ |   train_n_objs = 60\n2021-01-12 19:06:31 | INFO | __main__ |   train_runtime = 8.8511\n2021-01-12 19:06:35 | INFO | __main__ |   val_n_objs = 10\n2021-01-12 19:06:35 | INFO | __main__ |   val_runtime = 3.5329\n```\nWe can't compare these to the baseline, since the baseline won't even start and immediately failed with OOM.\n\nSimply amazing!\n\nI used only a tiny sample since I was primarily interested in being able to train and evaluate with this huge model that normally won't fit onto a 24GB GPU.",
        "question": "What is the maximum batch size that can be used to train the `t5-3b` model on a 24GB RTX-3090 card using DeepSpeed?\n",
        "answer": "The maximum batch size that can be used is 20.",
        "source_doc": "huggingface/blog/blob/main/zero-deepspeed-fairscale.md"
    },
    {
        "context": "```python\n# from torch.nn import Linear\nfrom pytorch_block_sparse import BlockSparseLinear\n\n...\n\n# self.fc = nn.Linear(1024, 256)\nself.fc = BlockSparseLinear(1024, 256, density=0.1)\n```\n\nThe extension also provides a `BlockSparseModelPatcher` that allows to modify an existing model \"on the fly\",\nwhich is shown in this [example notebook](https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/ModelSparsification.ipynb).\nSuch a model can then be trained as usual, without any change in your model source code.\n\n\n## NVIDIA CUTLASS\nThis extension is based on the [cutlass tilesparse](https://github.com/YulhwaKim/cutlass_tilesparse) proof of concept by [Yulhwa Kim](https://github.com/YulhwaKim).\n\nIt is using **C++ CUDA templates** for block-sparse matrix multiplication\nbased on **[CUTLASS](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/)**.\n\nCUTLASS is a collection of CUDA C++ templates for implementing high-performance CUDA kernels.\nWith CUTLASS, approching cuBLAS performance on custom kernels is possible without resorting to assembly language code.\n\nThe latest versions include all the **Ampere Tensor Core primitives**, providing **x10 or more speedups** with a limited loss of precision.\nNext versions of pytorch_block_sparse will make use of these primitives,\nas block sparsity is 100% compatible with Tensor Cores requirements.\n\n## Performance\nAt the current stage of the library, the performances for sparse matrices are roughly\ntwo times slower than their cuBLAS optimized dense counterpart, and we are confident\nthat we can improve this in the future.\n\nThis is a huge improvement on PyTorch sparse matrices: their current implementation is an order of magnitude slower\nthan the dense one.\n\nBut the more important point is that the performance gain of using sparse matrices grows with the sparsity,\nso a **75% sparse matrix** is roughly **2x** faster than the dense equivalent.",
        "question": "How does the performance gain of using sparse matrices grow with the sparsity?\n",
        "answer": "The performance gain of using sparse matrices grows with the sparsity, so a 75% sparse matrix is roughly 2x faster than the dense equivalent.",
        "source_doc": "huggingface/blog/blob/main/pytorch_block_sparse.md"
    },
    {
        "context": "<iframe src=\"https://chrisjay-mnist-adversarial.hf.space\" frameBorder=\"0\" width=\"100%\" height=\"1400px\" title=\"Gradio app\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## Conclusion\n\n\nDynamic Adversarial Data Collection (DADC) has been gaining traction in the machine learning community as a way to gather diverse non-saturating human-aligned datasets, and improve model evaluation and task performance. By dynamically collecting human-generated adversarial data with models in the loop, we can improve the generalization potential of our models. \n\nThis process of fooling and training the model on the adversarially collected data should be repeated over multiple rounds<sup>[1](https://aclanthology.org/2022.findings-acl.18.pdf)</sup>. [Eric Wallace et al](https://aclanthology.org/2022.findings-acl.18), in their experiments on natural language inference tasks, show that while in the short term standard non-adversarial data collection performs better, in the long term however dynamic adversarial data collection leads to the highest accuracy by a noticeable margin. \n\nUsing the [🤗 Spaces](https://huggingface.co/spaces), it becomes relatively easy to build a platform to dynamically collect adversarial data for your model and train on them.",
        "question": "How does dynamic adversarial data collection improve model evaluation and task performance?\n",
        "answer": "Dynamic adversarial data collection improves model evaluation and task performance by dynamically collecting human-generated adversarial data with models in the loop, which in turn improves the generalization potential of the models. This process of fooling and training the model on the adversarially collected data should be repeated over multiple rounds for optimal results.",
        "source_doc": "huggingface/blog/blob/main/mnist-adversarial.md"
    },
    {
        "context": "```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n**Print Output:**\n```bash\nLogin successful\nYour token has been saved to /root/.huggingface/token\n```\n\n### Load Dataset\n\nCommon Voice is a series of crowd-sourced datasets where speakers \nrecord text from Wikipedia in various languages. We'll use the latest edition \nof the Common Voice dataset ([version 11](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)). \nAs for our language, we'll fine-tune our model on \n[_Hindi_](https://en.wikipedia.org/wiki/Hindi), an Indo-Aryan language \nspoken in northern, central, eastern, and western India. Common Voice 11.0 \ncontains approximately 12 hours of labelled Hindi data, 4 of which are \nheld-out test data.\n\nLet's head to the Hub and view the dataset page for Common Voice: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0).\n\nThe first time we view this page, we'll be asked to accept the \nterms of use. After that, we'll be given full access to the dataset.\n\nOnce we've provided authentication to use the dataset, we'll be presented with the \ndataset preview. The dataset preview shows us the first 100 samples \nof the dataset. What's more, it's loaded up with audio samples ready for us \nto listen to in real time. We can select the Hindi subset of Common Voice by \nsetting the subset to `hi` using the dropdown menu (`hi` being the language \nidentifier code for Hindi):\n\n<figure>\n<img src=\"assets/111_fine_tune_whisper/select_hi.jpg\" alt=\"Trulli\" style=\"width:100%\">\n</figure>",
        "question": "What is the language identifier code for Hindi in the Common Voice dataset?\n",
        "answer": "The language identifier code for Hindi in the Common Voice dataset is 'hi'.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-whisper.md"
    },
    {
        "context": "Finally, let's visualize the output.\n\n```python\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots(1, len(prompts) + 1, figsize=(3*(len(prompts) + 1), 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(len(prompts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(prompts)];\n```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The masks of the different categories in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/14c048ea92645544c1bbbc9e55f3c620eaab8886.png\"></medium-zoom>\n</figure>\n\n### Visual prompting\n\nAs mentioned before, we can also use images as the input prompts (i.e.\nin place of the category names). This can be especially useful if it\\'s\nnot easy to describe the thing you want to segment. For this example,\nwe\\'ll use a picture of a coffee cup taken by [Daniel\nHooper](https://unsplash.com/@dan_fromyesmorecontent).\n\n```python\nurl = \"https://unsplash.com/photos/Ki7sAc8gOGE/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTJ8fGNvZmZlJTIwdG8lMjBnb3xlbnwwfHx8fDE2NzExOTgzNDQ&force=true&w=640\"\nprompt = Image.open(requests.get(url, stream=True).raw)\nprompt\n```\n\n<figure class=\"image table text-center m-0 w-6/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A picture of a paper coffee cup.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/7931f9db82ab07af7d161f0cfbfc347645da6646.png\"></medium-zoom>\n</figure>\n\nWe can now process the input image and prompt image and input them to\nthe model.",
        "question": "What is the name of the image used as the prompt?\n",
        "answer": "The name of the image used as the prompt is a picture of a paper coffee cup.",
        "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md"
    },
    {
        "context": "# create Hugging Face Model Class and deploy it as SageMaker Endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```\n\n\nThat's it! 🚀\n\nTo learn more about accessing and using the new Hugging Face DLCs with the Amazon SageMaker Python SDK, check out the guides and resources below.\n\n\n\n---\n\n\n\n# **Resources, Documentation & Samples 📄**\n\nBelow you can find all the important resources for deploying your models to Amazon SageMaker.\n\n\n## **Blog/Video**\n\n- [Video: Deploy a Hugging Face Transformers Model from S3 to Amazon SageMaker](https://youtu.be/pfBGgSGnYLs)\n- [Video: Deploy a Hugging Face Transformers Model from the Model Hub to Amazon SageMaker](https://youtu.be/l9QZuazbzWM)\n\n\n## **Samples/Documentation**\n\n- [Hugging Face documentation for Amazon SageMaker](https://huggingface.co/docs/sagemaker/main)\n- [Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/inference)\n- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)\n- [Notebook: Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)\n- [Notebook: Deploy a Hugging Face Transformer model from S3 to SageMaker for inference](https://github.com/huggingface/notebooks/blob/master/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)\n\n\n---\n\n\n# **SageMaker Hugging Face Inference Toolkit ⚙️**",
        "question": "What is the Hugging Face Inference Toolkit?\n",
        "answer": "The Hugging Face Inference Toolkit is a library that allows you to easily serve your Hugging Face models on various platforms, including Amazon SageMaker. It provides a simple interface for loading your model and handling inference requests.",
        "source_doc": "huggingface/blog/blob/main/deploy-hugging-face-models-easily-with-amazon-sagemaker.md"
    },
    {
        "context": "},\n    \"stable-baselines3\": {\n        filter: [\n            {\n                wildcard: { path: \"*.zip\" },\n            },\n        ],\n    },\n    \"timm\": {\n        filter: [\n            {\n                terms: { path: [\"pytorch_model.bin\", \"model.safetensors\"] },\n            },\n        ],\n    },\n    \"diffusers\": {\n        /// Filter out nested safetensors and pickle weights to avoid double counting downloads from the diffusers lib\n        must_not: [\n            {\n                wildcard: { path: \"*/*.safetensors\" },\n            },\n            {\n                wildcard: { path: \"*/*.bin\" },\n            },\n        ],\n        /// Include documents that match at least one of the following rules\n        should: [\n            /// Downloaded from diffusers lib\n            {\n                term: { path: \"model_index.json\" },\n            },\n            /// Direct downloads (LoRa, Auto1111 and others)\n            {\n                wildcard: { path: \"*.safetensors\" },\n            },\n            {\n                wildcard: { path: \"*.ckpt\" },\n            },\n            {\n                wildcard: { path: \"*.bin\" },\n            },\n        ],\n        minimum_should_match: 1,\n    },\n    \"peft\": {\n        filter: [\n            {\n                term: { path: \"adapter_config.json\" },\n            },\n        ],\n    }\n}\n```",
        "question": "What type of files does the diffusers library download?\n",
        "answer": "The diffusers library downloads model\\_index.json files.\n```",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/models-download-stats.md"
    },
    {
        "context": "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to add a new example script in 🤗 Transformers\n\nThis folder provide a template for adding a new example script implementing a training or inference task with the\nmodels in the 🤗 Transformers library. To use it, you will need to install cookiecutter:\n```\npip install cookiecutter\n```\nor refer to the installation page of the [cookiecutter documentation](https://cookiecutter.readthedocs.io/).\n\nYou can then run the following command inside the `examples` folder of the transformers repo:\n```\ncookiecutter ../templates/adding_a_new_example_script/\n```\nand answer the questions asked, which will generate a new folder where you will find a pre-filled template for your\nexample following the best practices we recommend for them.\n\nAdjust the way the data is preprocessed, the model is loaded or the Trainer is instantiated then when you're happy, add\na `README.md` in the folder (or complete the existing one if you added a script to an existing folder) telling a user\nhow to run your script.\n\nMake a PR to the 🤗 Transformers repo. Don't forget to tweet about your new example with a carbon screenshot of how to\nrun it and tag @huggingface!",
        "question": "What command should be run to add a new example script in 🤗 Transformers?\n",
        "answer": "The command is `cookiecutter ../templates/adding_a_new_example_script/`.",
        "source_doc": "huggingface/transformers/blob/main/templates/adding_a_new_example_script/README.md"
    },
    {
        "context": ". As we have seen before with the decoder, it can act in an auto-regressive manner; the word it has just output can now be used as an input. This, in combination with the numerical representation output by the encoder, can now be used to generate a second word. Please note that the first word is still here; as the model still outputs it. However, it is greyed out as we have no need for it anymore. We can continue on and on; for example until the decoder outputs a value that we consider a \"stopping value\", like a dot, meaning the end of a sequence. Here, we've seen the full mechanism of the encoder-decoder transformer: let's go over it one more time. We have an initial sequence, that is sent to the encoder. That encoder output is then sent to the decoder, for it to be decoded. While we can now discard the encoder after a single use, the decoder will be used several times: until we have generated every word that we need. Let's see a concrete example; with Translation Language Modeling; also called transduction; the act of translating a sequence. Here, we would like to translate this English sequence \"Welcome to NYC\" in French. We're using a transformer model that is trained for that task explicitly. We use the encoder to create a representation of the English sentence. We cast this to the decoder and, with the use of the start of sequence word, we ask it to output the first word. It outputs Bienvenue, which means \"Welcome\". We then use \"Bienvenue\" as the input sequence for the decoder. This, alongside the feature vector, allows the decoder to predict the second word, \"à\", which is \"to\" in English. Finally, we ask the decoder to predict a third word; it predicts \"NYC\", which is, once again, correct. We've translated the sentence! Where the encoder-decoder really shines, is that we have an encoder and a decoder; which often do not share weights",
        "question": "What is the task of the decoder in the context of translation language modeling?\n",
        "answer": "The decoder in the context of translation language modeling is responsible for predicting the next word in the sequence, based on the previous word and a feature vector. It does this until a \"stopping value\" is reached, such as a dot indicating the end of a sequence. In the example given, the decoder correctly translated the English sequence \"Welcome to NYC\" into French.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/07_encoder-decoders.md"
    },
    {
        "context": "--\ntitle: \"Open-Source Text Generation & LLM Ecosystem at Hugging Face\"\nthumbnail: /blog/assets/os_llms/thumbnail.png\nauthors:\n- user: merve\n---\n\n# Open-Source Text Generation & LLM Ecosystem at Hugging Face\n\n\n[Updated on July 24, 2023: Added Llama 2.]\n\n\nText generation and conversational technologies have been around for ages. Earlier challenges in working with these technologies were controlling both the coherence and diversity of the text through inference parameters and discriminative biases. More coherent outputs were less creative and closer to the original training data and sounded less human. Recent developments overcame these challenges, and user-friendly UIs enabled everyone to try these models out. Services like ChatGPT have recently put the spotlight on powerful models like GPT-4 and caused an explosion of open-source alternatives like Llama to go mainstream. We think these technologies will be around for a long time and become more and more integrated into everyday products. \n\nThis post is divided into the following sections:\n1. [Brief background on text generation](#brief-background-on-text-generation)\n2. [Licensing](#licensing)\n3. [Tools in the Hugging Face Ecosystem for LLM Serving](#tools-in-the-hugging-face-ecosystem-for-llm-serving)\n4. [Parameter Efficient Fine Tuning (PEFT)](#parameter-efficient-fine-tuning-peft)\n\n\n## Brief Background on Text Generation\n\nText generation models are essentially trained with the objective of completing an incomplete text or generating text from scratch as a response to a given instruction or question. Models that complete incomplete text are called Causal Language Models, and famous examples are GPT-3 by OpenAI and [Llama](https://ai.meta.com/blog/large-language-model-Llama-meta-ai/) by Meta AI. \n\n![Causal LM Output](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/text_generation.png)",
        "question": "What is the name of the famous example of a Causal Language Model?\n",
        "answer": "GPT-3 by OpenAI and Llama by Meta AI",
        "source_doc": "huggingface/blog/blob/main/os-llms.md"
    },
    {
        "context": "```\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\n                  encoder.block.1.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 2.57e+02 input[0]\n0.00e+00 2.85e+02 output\n[...]\n                  encoder.block.2.layer.0 T5LayerSelfAttention\n6.78e-04 3.15e+03 input[0]\n2.65e-04 3.42e+03 output[0]\n             None output[1]\n2.25e-01 1.00e+04 output[2]\n                  encoder.block.2.layer.1.layer_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\n                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear\n2.17e-07 4.50e+00 weight\n1.79e-06 4.65e+00 input[0]\n2.68e-06 3.70e+01 output\n                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear\n8.08e-07 2.66e+01 weight\n1.79e-06 4.65e+00 input[0]\n1.27e-04 2.37e+02 output\n                  encoder.block.2.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 8.76e+03 input[0]\n0.00e+00 9.74e+03 output\n                  encoder.block.2.layer.1.DenseReluDense.wo Linear\n1.01e-06 6.44e+00 weight\n0.00e+00 9.74e+03 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00      inf output\n```\n\nThe example output has been trimmed in the middle for brevity.\n\nThe second column shows the value of the absolute largest element, so if you have a closer look at the last few frames,\nthe inputs and outputs were in the range of `1e4`. So when this training was done under fp16 mixed precision the very\nlast step overflowed (since under `fp16` the largest number before `inf` is `64e3`). To avoid overflows under\n`fp16` the activations must remain way below `1e4`, because `1e4 * 1e4 = 1e8` so any matrix multiplication with\nlarge activations is going to lead to a numerical overflow condition.",
        "question": "What is the largest value in the last few frames of the context?\n",
        "answer": "The largest value in the last few frames of the context is `1e4`.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/debugging.md"
    },
    {
        "context": "|      |                                                                            |[ashi-ta/japanese-pretrained-ckpts](https://huggingface.co/ashi-ta/japanese-pretrained-ckpts)|0           |0                        |                                                                                              |                                                                                              |[LICENSE](https://huggingface.co/ashi-ta/japanese-pretrained-ckpts/blob/main/LICENSE)              |                                                                                                                     |                                                                                   |\n|      |                                                                            |[baffo32/decapoda-research-llama-7B-hf](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf)|0           |0                        |llama-license                                                                                 |https://huggingface.co/baffo32/decapoda-research-llama-7B-hf/blob/main/LICENSE                |[LICENSE](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf/blob/main/LICENSE)          |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the decapoda-research-llama-7B-hf model?\n",
        "answer": "The license for the decapoda-research-llama-7B-hf model is llama-license.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "!---\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Run Table Tasks with TAPEX\n\nTAPEX is a table pre-training approach for table-related tasks. By learning a neural SQL executor over a synthetic corpus based on generative language models (e.g., BART), it achieves state-of-the-art performance on several table-based question answering benchmarks and table-based fact verification benchmark. More details can be found in the original paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/pdf/2107.07653.pdf).\n\n> If you are also familiar with [fairseq](https://github.com/pytorch/fairseq), you may also find [the official implementation](https://github.com/microsoft/Table-Pretraining) useful, which leverages the framework.\n\n## Table Question Answering Tasks\n\n### What is Table Question Answering\n\n![Example](https://table-pretraining.github.io/assets/tableqa_task.png)\n\nThe task of Table Question Answering (TableQA) is to empower machines to answer users' questions over a given table. The resulting answer(s) can be a region in the table, or a number calculated by applying aggregation operators to a specific region.\n\n### What Questions Can be Answered",
        "question": "What questions can be answered by Table Question Answering?\n",
        "answer": "Table Question Answering can answer questions that require extracting information from a given table, such as finding specific values or calculating aggregations over a specific region in the table.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/tapex/README.md"
    },
    {
        "context": "```python\nsemantic_segmentation = pipeline(\"image-segmentation\", \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\nresults = semantic_segmentation(image)\nresults\n```\n\nThe segmentation pipeline output includes a mask for every predicted class. \n```bash\n[{'score': None,\n  'label': 'road',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'sidewalk',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'building',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'wall',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'pole',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'traffic sign',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'vegetation',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'terrain',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'sky',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\n\nTaking a look at the mask for the car class, we can see every car is classified with the same mask.\n\n```python\nresults[-1][\"mask\"]\n```\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/semantic_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>\n\nIn instance segmentation, the goal is not to classify every pixel, but to predict a mask for **every instance of an object** in a given image. It works very similar to object detection, where there is a bounding box for every instance, there's a segmentation mask instead. We will use [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance) for this.",
        "question": "What is the name of the pipeline used for instance segmentation?\n",
        "answer": "The name of the pipeline used for instance segmentation is facebook/mask2former-swin-large-cityscapes-instance.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md"
    },
    {
        "context": "In that function, we use the text prompt to conduct the semantic search. As seen above, to push updates to the Gradio components in the app, the function just needs to return components created with the `.update()` method. Since we connected the `song_option` `Radio` component to `fetch_songs.click` with its `output` parameter, `generate_playlist` can control the choices for the `Radio `component!\n\nYou can even do something similar to the `Radio` component in order to let users choose which song lyrics to view. [Visit the code on Hugging Face Spaces to see it in detail!](https://huggingface.co/spaces/NimaBoscarino/playlist-generator/blob/main/app.py)\n\n## Some Thoughts\n\nSentence Transformers and Gradio are great choices for this kind of project! ST has the utility functions that we need for quickly generating embeddings, as well as for running semantic search with minimal code. Having access to a large collection of pre-trained models is also extremely helpful, since we don’t need to create and train our own models for this kind of stuff. Building our demo in Gradio means we only have to focus on coding in Python, and [deploying Gradio projects to Hugging Face Spaces is also super simple](https://huggingface.co/docs/hub/spaces-sdks-gradio)!\n\nThere’s a ton of other stuff I wish I’d had the time to build into this project, such as these ideas that I might explore in the future:\n\n- Integrating with Spotify to automatically generate a playlist, and maybe even using Spotify’s embedded player to let users immediately listen to the songs.\n- Using the **[HighlightedText** Gradio component](https://gradio.app/docs/#highlightedtext) to identify the specific verse that was found by the semantic search.\n- Creating some visualizations of the embedding space, like in [this Space by Radamés Ajna](https://huggingface.co/spaces/radames/sentence-embeddings-visualization).",
        "question": "Which Gradio component can be used to identify the specific verse that was found by the semantic search?\n",
        "answer": "The HighlightedText Gradio component",
        "source_doc": "huggingface/blog/blob/main/playlist-generator.md"
    },
    {
        "context": "From a networking perspective, we will need the following setup: \n\n* Open port 22 for ```ssh``` access on all instances for setup and debugging.\n* Configure [password-less](https://www.redhat.com/sysadmin/passwordless-ssh) ```ssh``` between the master instance (the one you'll launch training from) and all other instances (__master included__).\n* Open all TCP ports on all instances for oneCCL communication inside the cluster. __Please make sure NOT to open these ports to the external world__. AWS provides a convenient way to do this by only allowing connections from instances running a particular [security group](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html). Here's how my setup looks.\n\n<kbd>\n<img src=\"assets/36_accelerating_pytorch/01_security_group.png\">\n</kbd>\n \nNow, let's provision the first instance manually. I first create the instance itself, attach the security group above, and add 128GB of storage. To optimize costs, I have launched it as a [spot instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html). \n\nOnce the instance is up, I connect to it with ```ssh``` in order to install dependencies.\n\n### Installing dependencies\n\nHere are the steps we will follow:\n\n* Install Intel toolkits,\n* Install the Anaconda distribution,\n* Create a new ```conda``` environment,\n* Install PyTorch and the Intel extension for PyTorch,\n* Compile and install oneCCL,\n* Install the ```transformers``` library.\n\nIt looks like a lot, but there's nothing complicated. Here we go!\n\n__Installing Intel toolkits__",
        "question": "What is the first step in installing dependencies on the instance?\n",
        "answer": "The first step in installing dependencies on the instance is installing Intel toolkits.",
        "source_doc": "huggingface/blog/blob/main/accelerating-pytorch.md"
    },
    {
        "context": "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`XLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [`TFXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**\n\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n- [`TFXLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n🚀 Deploy\n\n- A blog post on how to [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface).\n\n<Tip> \n\nThis implementation is the same as RoBERTa. Refer to the [documentation of RoBERTa](roberta) for usage examples as well as the information relative to the inputs and outputs.\n</Tip>\n\n## XLMRobertaConfig",
        "question": "What is the name of the config class for XLMRoberta?\n",
        "answer": "The name of the config class for XLMRoberta is XLMRobertaConfig.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/xlm-roberta.md"
    },
    {
        "context": "## Missing Parts / Coming Next\n\nAs stated, we are just getting started! Our upcoming priorities include:\n\n- Encoder-decoder models such as T5 and Flan.\n- More tokenizers: support for Unigram and WordPiece.\n- Additional generation algorithms.\n- Support key-value caching for optimization.\n- Use discrete sequence shapes for conversion. Together with key-value caching this will allow for larger contexts.\n\nLet us know what you think we should work on next, or head over to the repos for [Good First Issues](https://github.com/huggingface/swift-transformers/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) to try your hand on!\n\n## Conclusion\n\nWe introduced a set of tools to help Swift developers incorporate language models in their apps. I can't wait to see what you create with them, and I look forward to improving them with the community's help! Don't hesitate to get in touch :)\n\n### _Appendix: Converting Llama 2 the Hard Way_\n\nYou can safely ignore this section unless you've experienced Core ML conversion issues and are ready to fight :)\n\nIn my experience, there are two frequent reasons why PyTorch models fail to convert to Core ML using `coremltools`:\n\n- Unsupported PyTorch operations or operation variants\n\nPyTorch has _a lot_ of operations, and all of them have to be mapped to an intermediate representation ([MIL](https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html), for _Model Intermediate Language_), which in turn is converted to native Core ML instructions. The set of PyTorch operations is not static, so new ones have to be added to `coremltools` too. In addition, some operations are really complex and can work on exotic combinations of their arguments. An example of a recently-added, very complex op, was _scaled dot-product attention_, introduced in PyTorch 2. An example of a partially supported op is `einsum`: not all possible equations are translated to MIL.\n\n- Edge cases and type mismatches",
        "question": "What are the upcoming priorities for Swift developers in incorporating language models in their apps?\n",
        "answer": "The upcoming priorities for Swift developers in incorporating language models in their apps include encoder-decoder models such as T5 and Flan, more tokenizers including Unigram and WordPiece, additional generation algorithms, support for key-value caching for optimization, and using discrete sequence shapes for conversion.",
        "source_doc": "huggingface/blog/blob/main/swift-coreml-llm.md"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# RL Environment Wrappers\n\n[[autodoc]] RLEnv\n\n[[autodoc]] ParallelRLEnv\n\n[[autodoc]] MultiProcessRLEnv",
        "question": "What is the name of the class that allows for parallel execution of environments?\n",
        "answer": "ParallelRLEnv",
        "source_doc": "huggingface/simulate/blob/main/docs/source/api/rl_env.mdx"
    },
    {
        "context": "### Fixes",
        "question": "What is the name of the first fix in the list?\n",
        "answer": "The first fix in the list is \"Fix for the bug in the code that caused the program to crash\".",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "This model was contributed by [Lysandre](https://huggingface.co/lysandre). The authors' code can be found [here](https://github.com/facebookresearch/fairseq/tree/nllb).\n\n## Generating with NLLB\n\nWhile generating the target text set the `forced_bos_token_id` to the target language id. The following\nexample shows how to translate English to French using the *facebook/nllb-200-distilled-600M* model.\n\nNote that we're using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\nfor the list of all BCP-47 in the Flores 200 dataset.\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n\n>>> article = \"UN Chief says there is no military solution in Syria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nLe chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\n```\n\n### Generating from any other language than English\n\nEnglish (`eng_Latn`) is set as the default language from which to translate. In order to specify that you'd like to translate from a different language,\nyou should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer initialization.\n\nSee example below for a translation from romanian to german:\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\n...     \"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ron_Latn\"\n... )\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=True)",
        "question": "What is the BCP-47 code for German?\n",
        "answer": "deu_Latn",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/nllb.md"
    },
    {
        "context": "|      |      |[localmodels/Llama-2-7B-Chat-GPTQ](https://huggingface.co/localmodels/Llama-2-7B-Chat-GPTQ)                                                        |84          |2       | llama2 |                                                 |[LICENSE](https://huggingface.co/localmodels/Llama-2-7B-Chat-GPTQ/blob/main/LICENSE)                                    |                                                                                                    |             |\n|      |      |[qwopqwop/danbooru-llama-gptq](https://huggingface.co/qwopqwop/danbooru-llama-gptq)                                                                |80          |2       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/qwopqwop/danbooru-llama-gptq/blob/main/LICENSE.txt)                                |                                                                                                    |             |\n|      |      |[TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ](https://huggingface.co/TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ)                                          |79          |15      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ/blob/main/LICENSE.txt)                     |                                                                                                    |             |\n|      |      |[TheBloke/Llama2-22B-Daydreamer-v3-GPTQ](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GPTQ)                                            |78          |7       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GPTQ/blob/main/LICENSE.txt)                      |                                                                                                    |             |",
        "question": "What is the license for the model TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ?\n",
        "answer": "The license for the model TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ is LICENSE.txt.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md"
    },
    {
        "context": "Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), AudioLDM 2 is a text-to-audio _latent diffusion model (LDM)_ that learns continuous audio representations from text embeddings. Two text encoder models are used to compute the text embeddings from a prompt input: the text-branch of [CLAP](https://huggingface.co/docs/transformers/main/en/model_doc/clap) and the encoder of [Flan-T5](https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5). These text embeddings are then projected to a shared embedding space by an [AudioLDM2ProjectionModel](https://huggingface.co/docs/diffusers/main/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel). A [GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) _language model (LM)_ is used to auto-regressively predict eight new embedding vectors, conditional on the projected CLAP and Flan-T5 embeddings. The generated embedding vectors and Flan-T5 text embeddings are used as cross-attention conditioning in the LDM. The [UNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel) of AudioLDM 2 is unique in the sense that it takes **two** cross-attention embeddings, as opposed to one cross-attention conditioning, as in most other LDMs.\n\nThe abstract of the paper is the following:",
        "question": "What is the unique feature of the UNet in AudioLDM 2?\n",
        "answer": "The unique feature of the UNet in AudioLDM 2 is that it takes two cross-attention embeddings, as opposed to one cross-attention conditioning, as in most other LDMs.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm2.md"
    },
    {
        "context": "### May 2, 2022\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`vision_transformer_relpos.py`) and Residual Post-Norm branches (from Swin-V2) (`vision_transformer*.py`)\n  * `vit_relpos_base_patch32_plus_rpn_256` - 79.5 @ 256, 80.6 @ 320 -- rel pos + extended width + res-post-norm, no class token, avg pool\n  * `vit_relpos_base_patch16_224` - 82.5 @ 224, 83.6 @ 320 -- rel pos, layer scale, no class token, avg pool\n  * `vit_base_patch16_rpn_224` - 82.3 @ 224 -- rel pos + res-post-norm, no class token, avg pool\n* Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie `How to Train Your ViT`)\n* `vit_*` models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae).\n\n### April 22, 2022\n* `timm` models are now officially supported in [fast.ai](https://www.fast.ai/)! Just in time for the new Practical Deep Learning course. `timmdocs` documentation link updated to [timm.fast.ai](http://timm.fast.ai/).\n* Two more model weights added in the TPU trained [series](https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights). Some In22k pretrain still in progress.\n  * `seresnext101d_32x8d` - 83.69 @ 224, 84.35 @ 288\n  * `seresnextaa101d_32x8d` (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288\n\n### March 23, 2022\n* Add `ParallelBlock` and `LayerScale` option to base vit models to support model configs in [Three things everyone should know about ViT](https://arxiv.org/abs/2203.09795)\n* `convnext_tiny_hnf` (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.",
        "question": "What is the top-1 accuracy of convnext\\_tiny\\_hnf?\n",
        "answer": "82.2%",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/changes.md"
    },
    {
        "context": "Once a job is submitted, the models will be automatically evaluated and a Hub pull request will be opened with the evaluation results:\n\n![Pull Request](/blog/assets/82_eval_on_the_hub/pr.png)\n\nYou can also copy-paste the evaluation metadata into the dataset card so that you and the community can skip the manual configuration next time!\n\n![Metadata Pull Request](/blog/assets/82_eval_on_the_hub/metadata.png)\n\n### Check out the leaderboard\n\nTo facilitate the comparison of models, Evaluation on the Hub also provides leaderboards that allow you to examine which models perform best on which split and metric:\n\n![Leaderboard](/blog/assets/82_eval_on_the_hub/leaderboard.png)\n\nLooks like the Swin Transformer came out on top!\n\n### Try it yourself!\n\nIf you’d like to evaluate your own choice of models, give Evaluation on the Hub a spin by checking out these popular datasets:\n\n* [Emotion](https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=emotion) for text classification\n* [MasakhaNER](https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=masakhaner) for named entity recognition\n* [SAMSum](https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=samsum) for text summarization\n\n## The Bigger Picture\n\nSince the dawn of machine learning, we've evaluated models by computing some form of accuracy on a held-out test set that is assumed to be independent and identically distributed. Under the pressures of modern AI, that paradigm is now starting to show serious cracks.",
        "question": "What is the name of the leaderboard that allows you to examine which models perform best on which split and metric?\n",
        "answer": "The name of the leaderboard is Evaluation on the Hub.",
        "source_doc": "huggingface/blog/blob/main/eval-on-the-hub.md"
    },
    {
        "context": "Many techniques have been adopted to tackle these challenges at scale. The most familiar paradigms are Pipeline Parallelism, Tensor Parallelism, and Data Parallelism.\n\n| ![model-parallelism](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/model-parallelism.png) |\n|:--:|\n| <b>Image Credits to <a href=\"https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214\" rel=\"noopener\" target=\"_blank\" >this blogpost</a> </b>|\n\nWith data parallelism the same model is hosted in parallel on several machines and each instance is fed a different data batch. This is the most straight forward parallelism strategy essentially replicating the single-GPU case and is already supported by `trl`. With Pipeline and Tensor Parallelism the model itself is distributed across machines: in Pipeline Parallelism the model is split layer-wise, whereas Tensor Parallelism splits tensor operations across GPUs (e.g. matrix multiplications). With these Model Parallelism strategies, you need to shard the model weights across many devices which requires you to define a communication protocol of the activations and gradients across processes. This is not trivial to implement and might need the adoption of some frameworks such as [`Megatron-DeepSpeed`](https://github.com/microsoft/Megatron-DeepSpeed) or [`Nemo`](https://github.com/NVIDIA/NeMo). It is also important to highlight other tools that are essential for scaling LLM training such as Adaptive activation checkpointing and fused kernels. Further reading about parallelism paradigms can be found [here](https://huggingface.co/docs/transformers/v4.17.0/en/parallelism).",
        "question": "What is the definition of data parallelism in the context of training large language models?\n",
        "answer": "In the context of training large language models, data parallelism involves replicating the same model in parallel on several machines and feeding each instance a different data batch. This strategy essentially mirrors the single-GPU case and is already supported by `trl`.",
        "source_doc": "huggingface/blog/blob/main/trl-peft.md"
    },
    {
        "context": "- Label Smoothing\n    - Mixup\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 64x NVIDIA V100 GPUs\n    ID: resnest14d\n    LR: 0.1\n    Epochs: 270\n    Layers: 14\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 8192\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/resnest.py#L148\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest14-9c8fe254.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75.51%\n      Top 5 Accuracy: 92.52%\n- Name: resnest200e\n  In Collection: ResNeSt\n  Metadata:\n    FLOPs: 45954387872\n    Parameters: 70200000\n    File Size: 193782911\n    Architecture:\n    - 1x1 Convolution\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Connection\n    - Softmax\n    - Split Attention\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - DropBlock\n    - Label Smoothing\n    - Mixup\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 64x NVIDIA V100 GPUs\n    ID: resnest200e\n    LR: 0.1\n    Epochs: 270\n    Layers: 200\n    Dropout: 0.2\n    Crop Pct: '0.909'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '320'\n    Weight Decay: 0.0001\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/resnest.py#L194\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest101-22405ba7.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 83.85%\n      Top 5 Accuracy: 96.89%\n- Name: resnest269e\n  In Collection: ResNeSt\n  Metadata:",
        "question": "How many layers does resnest14d have?\n",
        "answer": "resnest14d has 14 layers.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/resnest.mdx"
    },
    {
        "context": "## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />",
        "question": "How do I test my Endpoint in the overview?\n",
        "answer": "You can test your Endpoint in the overview by using the Inference widget.",
        "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx"
    },
    {
        "context": "--\ntitle: \"What's going on with the Open LLM Leaderboard?\"\nthumbnail: /blog/assets/evaluating-mmlu-leaderboard/thumbnail.png\nauthors:\n- user: clefourrier\n- user: SaylorTwift\n- user: slippylolo\n- user: thomwolf\n---\n\n# What's going on with the Open LLM Leaderboard?\n\n\nRecently an interesting discussion arose on Twitter following the release of [**Falcon 🦅**](https://huggingface.co/tiiuae/falcon-40b) and its addition to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), a public leaderboard comparing open access large language models.\n\nThe discussion centered around one of the four evaluations displayed on the leaderboard: a benchmark for measuring [Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) (shortname: MMLU).\n\nThe community was surprised that MMLU evaluation numbers of the current top model on the leaderboard, the [**LLaMA model 🦙**](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), were significantly lower than the numbers in the [published LLaMa paper](https://arxiv.org/abs/2302.13971).\n\nSo we decided to dive in a rabbit hole to understand what was going on and how to fix it 🕳🐇\n\nIn our quest, we discussed with both the great [@javier-m](https://huggingface.co/javier-m) who collaborated on the evaluations of LLaMA and the amazing [@slippylolo](https://huggingface.co/slippylolo) from the Falcon team. This being said, all the errors in the below should be attributed to us rather than them of course!\n\nAlong this journey with us you’ll learn a lot about the ways you can evaluate a model on a single evaluation and whether or not to believe the numbers you see online and in papers.\n\nReady? Then buckle up, we’re taking off 🚀.\n\n## What's the Open LLM Leaderboard?",
        "question": "What is the Open LLM Leaderboard?\n",
        "answer": "The Open LLM Leaderboard is a public leaderboard comparing open access large language models, displaying evaluations for Massive Multitask Language Understanding (MMLU) among others.",
        "source_doc": "huggingface/blog/blob/main/evaluating-mmlu-leaderboard.md"
    },
    {
        "context": "# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\nfor example in my_iterable_dataset:  \n    print(example)\n    break\n```\n\n## Exact and fast approximate shuffling\n\nWhen you shuffle a [`Dataset`] using [`Dataset.shuffle`], you apply an exact shuffling of the dataset.\nIt works by taking a list of indices `[0, 1, 2, ... len(my_dataset) - 1]` and shuffling this list.\nThen, accessing `my_dataset[0]` returns the row and index defined by the first element of the indices mapping that has been shuffled:\n```python\nmy_dataset = my_dataset.shuffle(seed=42)\nprint(my_dataset[0])\n```\n\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use a shuffled list of indices and access a row at an arbitrary position.\nThis prevents the use of exact shuffling.\nInstead, a fast approximate shuffling is used in [`IterableDataset.shuffle`].\nIt uses a shuffle buffer to sample random examples iteratively from the dataset.\nSince the dataset is still read iteratively, it provides excellent speed performance:\n```python\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\nfor example in my_iterable_dataset:\n    print(example)\n    break\n```\n\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learning model training. So [`IterableDataset.shuffle`] also shuffles the dataset shards if your dataset is made of multiple files or sources:\n\n```python\n# Stream from the internet\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\nmy_iterable_dataset.n_shards  # 39\n\n# Stream from local files\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\nmy_iterable_dataset.n_shards  # 1024",
        "question": "How does the IterableDataset shuffle the dataset?\n",
        "answer": "The IterableDataset shuffles the dataset by using a shuffle buffer to sample random examples iteratively and shuffling the dataset shards if it is made of multiple files or sources.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx"
    },
    {
        "context": "## Implementation Notes\n\n- Each model is about 298 MB on disk, there are more than 1,000 models.\n- The list of supported language pairs can be found [here](https://huggingface.co/Helsinki-NLP).\n- Models were originally trained by [Jörg Tiedemann](https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann) using the [Marian](https://marian-nmt.github.io/) C++ library, which supports fast training and translation.\n- All models are transformer encoder-decoders with 6 layers in each component. Each model's performance is documented\n  in a model card.\n- The 80 opus models that require BPE preprocessing are not supported.\n- The modeling code is the same as [`BartForConditionalGeneration`] with a few minor modifications:\n\n  - static (sinusoid) positional embeddings (`MarianConfig.static_position_embeddings=True`)\n  - no layernorm_embedding (`MarianConfig.normalize_embedding=False`)\n  - the model starts generating with `pad_token_id` (which has 0 as a token_embedding) as the prefix (Bart uses\n    `<s/>`),\n- Code to bulk convert models can be found in `convert_marian_to_pytorch.py`.\n\n\n## Naming\n\n- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`\n- The language codes used to name models are inconsistent. Two digit codes can usually be found [here](https://developers.google.com/admin-sdk/directory/v1/languages), three digit codes require googling \"language\n  code {code}\".\n- Codes formatted like `es_AR` are usually `code_{region}`. That one is Spanish from Argentina.\n- The models were converted in two stages. The first 1000 models use ISO-639-2 codes to identify languages, the second\n  group use a combination of ISO-639-5 codes and ISO-639-2 codes.\n\n\n## Examples",
        "question": "What is the name of the library used to train the models?\n",
        "answer": "The models were originally trained by Jörg",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown\nvulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose\nROCBERT: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation,\nsynonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency\nunder different synthesized adversarial examples. The model takes as input multimodal information including the\nsemantic, phonetic and visual features. We show all these features are important to the model robustness since the\nattack can be performed in all the three forms. Across 5 Chinese NLU tasks, ROCBERT outperforms strong baselines under\nthree blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best\nin the toxic content detection task under human-made attacks.*\n\nThis model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RoCBertConfig\n\n[[autodoc]] RoCBertConfig\n    - all\n\n## RoCBertTokenizer\n\n[[autodoc]] RoCBertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## RoCBertModel\n\n[[autodoc]] RoCBertModel\n    - forward\n\n## RoCBertForPreTraining\n\n[[autodoc]] RoCBertForPreTraining\n    - forward\n\n## RoCBertForCausalLM\n\n[[autodoc]] RoCBertForCausalLM\n    - forward\n\n## RoCBertForMaskedLM",
        "question": "What is the name of the model that is robust to adversarial attacks for Chinese?\n",
        "answer": "ROCBERT",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/roc_bert.md"
    },
    {
        "context": "![Collection image drop zone with images](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-gallery.webp)\n\nYou can re-order images by drag-and-dropping them. Clicking on an image will open it in full-screen mode.\n\n![Collection image viewer](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-viewer.webp)\n\n## Your feedback on collections\n\nWe're working on improving collections, so if you have any bugs, questions, or new features you'd like to see added, please post a message in the [dedicated discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/12).",
        "question": "How can I re-order images in a collection?\n",
        "answer": "You can re-order images in a collection by drag-and-dropping them.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/collections.md"
    },
    {
        "context": "--\ntitle: \"Introducing 🤗 Accelerate\"\nthumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\nauthors:\n- user: sgugger\n---\n\n# Introducing 🤗 Accelerate\n\n\n## 🤗 Accelerate\n\nRun your **raw** PyTorch training scripts on any kind of device.\n\nMost high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. 🤗 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\n\nIt provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\n\n### Easy integration!\n\nLet's first have a look at an example:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\n\n  model = torch.nn.Transformer().to(device)\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n          source = source.to(device)\n          targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```",
        "question": "What is the name of the device that the model is moved to in the example?\n",
        "answer": "The model is moved to the device provided by the `Accelerator` object.",
        "source_doc": "huggingface/blog/blob/main/accelerate-library.md"
    },
    {
        "context": "### Nov 23, 2023\n* Added EfficientViT-Large models, thanks [SeeFun](https://github.com/seefun)\n* Fix Python 3.7 compat, will be dropping support for it soon\n* Other misc fixes\n* Release 0.9.12\n\n### Nov 20, 2023\n* Added significant flexibility for Hugging Face Hub based timm models via `model_args` config entry. `model_args` will be passed as kwargs through to models on creation. \n  * See example at https://huggingface.co/gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k/blob/main/config.json\n  * Usage: https://github.com/huggingface/pytorch-image-models/discussions/2035\n* Updated imagenet eval and test set csv files with latest models\n* `vision_transformer.py` typing and doc cleanup by [Laureηt](https://github.com/Laurent2916)\n* 0.9.11 release\n\n### Nov 3, 2023\n* [DFN (Data Filtering Networks)](https://huggingface.co/papers/2309.17425) and [MetaCLIP](https://huggingface.co/papers/2309.16671) ViT weights added\n* DINOv2 'register' ViT model weights added (https://huggingface.co/papers/2309.16588, https://huggingface.co/papers/2304.07193)\n* Add `quickgelu` ViT variants for OpenAI, DFN, MetaCLIP weights that use it (less efficient)\n* Improved typing added to ResNet, MobileNet-v3 thanks to [Aryan](https://github.com/a-r-r-o-w)\n* ImageNet-12k fine-tuned (from LAION-2B CLIP) `convnext_xxlarge`\n* 0.9.9 release\n\n### Oct 20, 2023\n* [SigLIP](https://huggingface.co/papers/2303.15343) image tower weights supported in `vision_transformer.py`.\n  * Great potential for fine-tune and downstream feature use.\n* Experimental 'register' support in vit models as per [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588)\n* Updated RepViT with new weight release. Thanks [wangao](https://github.com/jameslahm)\n* Add patch resizing support (on pretrained weight load) to Swin models\n* 0.9.8 release pending",
        "question": "What is the latest release of the context?\n",
        "answer": "The latest release mentioned in the context is 0.9.12.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/README.md"
    },
    {
        "context": "## Repo API\n\nThe following endpoints manage repository settings like creating and deleting a repository.\n### POST /api/repos/create\n\nCreate a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n\nPayload:\n\n```js\npayload = {\n    \"type\": \"model\",\n    \"name\": \"name\",\n    \"organization\": \"organization\",\n}\n```\n\nThis is equivalent to `huggingface_hub.delete_repo()`.\n\n### PUT /api/repos/{repo_type}/{repo_id}/settings\n\nUpdate repo visibility.\n\nPayload:\n\n```js\npayload = {\n    \"private\": \"private\",\n}\n```\n\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.\n\n### POST /api/repos/move\n\nMove a repository (rename within the same namespace or transfer from user to organization).\n\nParameters:\n- `fromRepo`: repo to rename.\n- `toRepo`: new name of the repo.\n- `type`: Type of repo (dataset or space; model by default).\n\nPayload:\n\n```js\npayload = {\n    \"fromRepo\" : \"namespace/repo_name\",\n    \"toRepo\" : \"namespace2/repo_name2\",\n    \"type\": \"model\",\n}\n```\n\nThis is equivalent to `huggingface_hub.move_repo()`.\n\n## User API\n\nThe following endpoint gets information about a user.\n\n### GET /api/whoami-v2\n\nGet username and organizations the user belongs to.\n\nPayload:\n\n```js\nheaders = { \"authorization\" :  \"Bearer $token\" }\n```\n\nThis is equivalent to `huggingface_hub.whoami()`.\n\n## Collections API",
        "question": "What is the endpoint to move a repository?\n",
        "answer": "POST /api/repos/move",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/api.md"
    },
    {
        "context": "ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of just\nthe next token.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\noverfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\ndataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for\nabstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n\nThe Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\n## Usage tips\n\n- ProphetNet is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- The model architecture is based on the original Transformer, but replaces the “standard” self-attention mechanism in the decoder by a a main self-attention mechanism and a self and n-stream (predict) self-attention mechanism.\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## ProphetNetConfig\n\n[[autodoc]] ProphetNetConfig\n\n## ProphetNetTokenizer\n\n[[autodoc]] ProphetNetTokenizer",
        "question": "What is the name of the new sequence-to-sequence pretraining model presented in the paper?\n",
        "answer": "ProphetNet",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/prophetnet.md"
    },
    {
        "context": "```python\nfrom transformers import Wav2Vec2Processor\n\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```\n\n### Preprocess Data\n\nSo far, we have not looked at the actual values of the speech signal but just the transcription. In addition to sentence, our datasets include two more column names path and audio. path states the absolute path of the audio file. Let's take a look.\n\n```python\nprint(timit[0][\"path\"])\n```\n\n**Print Output:**\n```bash\n'/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV'\n```\n\n**`Wav2Vec2`** expects the input in the format of a 1-dimensional array of 16 kHz. This means that the audio file has to be loaded and resampled.\n\nThankfully, datasets does this automatically by calling the other column audio. Let try it out.\n\n```python\ncommon_voice_train[0][\"audio\"]\n```\n\n**Print Output:**\n```bash\n{'array': array([-2.1362305e-04,  6.1035156e-05,  3.0517578e-05, ...,\n        -3.0517578e-05, -9.1552734e-05, -6.1035156e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV',\n 'sampling_rate': 16000}\n```\n\nWe can see that the audio file has automatically been loaded. This is thanks to the new [`\"Audio\" feature`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) introduced in datasets == 4.13.3, which loads and resamples audio files on-the-fly upon calling.\n\nThe sampling rate is set to 16kHz which is what `Wav2Vec2` expects as an input.\n\nGreat, let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. \n\n```python\nimport IPython.display as ipd\nimport numpy as np\nimport random\n\nrand_int = random.randint(0, len(timit[\"train\"]))",
        "question": "What is the sampling rate of the audio file in the TRAIN dataset of DR4/MMDM0/SI681.WAV?\n",
        "answer": "The sampling rate of the audio file in the TRAIN dataset of DR4/MMDM0/SI681.WAV is 16kHz.",
        "source_doc": "huggingface/blog/blob/main/fine-tune-wav2vec2-english.md"
    },
    {
        "context": "--\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\n\n\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information.\n\n<img style=\"border:none;\" alt=\"Screenshot showing Diffusers for Mac UI\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png\" />\n\n## What exactly is 🧨Diffusers for Mac anyway?\n\nThe Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models. It supports many different models and tasks, and is highly configurable and well optimized. It runs on Mac, too, using PyTorch's [`mps` accelerator](https://huggingface.co/docs/diffusers/optimization/mps), which is an alternative to `cuda` on Apple Silicon.",
        "question": "What is the library that the Diffusers app is based on?\n",
        "answer": "The Diffusers app is based on the `diffusers` library, which is written in Python with PyTorch.",
        "source_doc": "huggingface/blog/blob/main/fast-mac-diffusers.md"
    },
    {
        "context": "Gradio Demo: dataframe_block-ui-test\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    count = gr.Slider(minimum=1, maximum=10, step=1, label=\"count\")\n    data = gr.DataFrame(\n        headers=[\"A\", \"B\"], col_count=(2, \"fixed\"), type=\"array\", interactive=True\n    )\n    btn = gr.Button(value=\"click\")\n    btn.click(\n        fn=lambda cnt: [[str(2 * i), str(2 * i + 1)] for i in range(int(cnt))],\n        inputs=[count],\n        outputs=[data],\n    )\n\ndemo.launch()\n\n```",
        "question": "How can the number of rows in the dataframe be changed in the Gradio demo?\n",
        "answer": "The number of rows in the dataframe can be changed by adjusting the 'count' slider in the Gradio demo. The 'count' slider has a minimum value of 1, a maximum value of 10, and a step of 1. The value of the 'count' slider is passed to the 'btn.click' function, which generates a new dataframe with the specified number of rows.",
        "source_doc": "gradio-app/gradio/blob/main/demo/dataframe_block-ui-test/run.ipynb"
    },
    {
        "context": "The solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q value generation. We:\n<!---<img src=\"assets/78_deep_rl_dqn/double-dqn-pseudocode.jpg\" alt=\"Double DQN Pseudocode\"/>--->\n- Use our **DQN network** to select the best action to take for the next state (the action with the highest Q value).\n- Use our **Target network** to calculate the target Q value of taking that action at the next state.\n\nTherefore, Double DQN helps us reduce the overestimation of q values and, as a consequence, helps us train faster and have more stable learning.\n\nSince these three improvements in Deep Q-Learning, many have been added such as Prioritized Experience Replay, Dueling Deep Q-Learning. They’re out of the scope of this course but if you’re interested, check the links we put in the reading list.  👉 **[https://github.com/huggingface/deep-rl-class/blob/main/unit3/README.md](https://github.com/huggingface/deep-rl-class/blob/main/unit3/README.md)**\n\n  \nNow that you've studied the theory behind Deep Q-Learning, **you’re ready to train your Deep Q-Learning agent to play Atari Games**. We'll start with Space Invaders, but you'll be able to use any Atari game you want 🔥 \n\nWe're using the RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n  \nStart the tutorial here 👉 https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb\n\nThe leaderboard to compare your results with your classmates 🏆 👉 https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/78_deep_rl_dqn/atari-envs.gif\" alt=\"Environments\"/>\n</figure>\n  \n---\nCongrats on finishing this chapter! There was a lot of information. And congrats on finishing the tutorial. You’ve just trained your first Deep Q-Learning agent and shared it on the Hub 🥳.",
        "question": "How does Double DQN help reduce the overestimation of Q values?\n",
        "answer": "Double DQN helps reduce the overestimation of Q values by using two networks to decouple the action selection from the target Q value generation. The DQN network is used to select the best action to take for the next state, and the Target network is used to calculate the target Q value of taking that action at the next state.",
        "source_doc": "huggingface/blog/blob/main/deep-rl-dqn.md"
    },
    {
        "context": "Parameters: 480310000\n    File Size: 1925950424\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - FixRes\n    - Label Smoothing\n    - Noisy Student\n    - RMSProp\n    - RandAugment\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resources: TPUv3 Cloud TPU\n    ID: tf_efficientnet_l2_ns_475\n    LR: 0.128\n    Epochs: 350\n    Dropout: 0.5\n    Crop Pct: '0.936'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '475'\n    Weight Decay: 1.0e-05\n    Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n    Stochastic Depth Survival: 0.8\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1509\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 88.24%\n      Top 5 Accuracy: 98.55%\n-->",
        "question": "What is the top 1 accuracy of the model on ImageNet?\n",
        "answer": "88.24%",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/tf-efficientnet.mdx"
    },
    {
        "context": "Weights: https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb2_pruned_203f55bc.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.91%\n      Top 5 Accuracy: 94.86%\n- Name: efficientnet_b3_pruned\n  In Collection: EfficientNet Pruned\n  Metadata:\n    FLOPs: 1239590641\n    Parameters: 9860000\n    File Size: 39770812\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: efficientnet_b3_pruned\n    Crop Pct: '0.904'\n    Image Size: '300'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/efficientnet.py#L1230\n  Weights: https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb3_pruned_5abcc29f.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80.86%\n      Top 5 Accuracy: 95.24%\n-->",
        "question": "What is the top 1 accuracy of efficientnet_b3_pruned?\n",
        "answer": "The top 1 accuracy of efficientnet_b3_pruned is 80.86%.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/efficientnet-pruned.mdx"
    },
    {
        "context": "### 4.2. Example Two - OPT:\n\n\n```python\n# Load the language model and prepare the prefix text:\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFOPTForCausalLM\nmodel_name = r'facebook/opt-1.3b'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFOPTForCausalLM.from_pretrained(model_name)\n\nprefix_text = r\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.\"\ninput_ids = tokenizer(prefix_text, return_tensors='tf').input_ids\n```\n\n#### 4.2.1. Generating Text with Greedy Search:\n\n\n```python\noutput = model.generate(input_ids, max_length=256)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nprint(\"\" + 100 * '-')\n```\n\n#### 4.2.2. Generating Text with Nucleus Sampling:\n\n\n```python\ntf.random.set_seed(0)\noutput = model.generate(input_ids, do_sample=True, max_length=256, top_p=0.95, top_k=0)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nprint(\"\" + 100 * '-')\n```\n\n#### 4.2.3. Generating Text with Contrastive Search:\n\n\n```python\noutput = model.generate(input_ids, max_length=256, penalty_alpha=0.6, top_k=6)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nprint(\"\" + 100 * '-')\n```",
        "question": "What is the name of the language model used in the example?\n",
        "answer": "The name of the language model used in the example is 'facebook/opt-1.3b'.",
        "source_doc": "huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb"
    },
    {
        "context": "## Other applications of the Perceiver\n\nNote that there are no limits on the applications of the Perceiver! In the original [Perceiver paper](https://arxiv.org/abs/2103.03206), the authors showed that the architecture can be used to process 3D point clouds – a common concern for self-driving cars equipped with Lidar sensors. They trained the model on [ModelNet40](https://modelnet.cs.princeton.edu/), a dataset of point clouds derived from 3D triangular meshes spanning 40 object categories. The model was shown to achieve a top-1 accuracy of 85.7 % on the test set, competing with [PointNet++](https://arxiv.org/abs/1706.02413), a highly specialized model that uses extra geometric features and performs more advanced augmentations.\n\nThe authors also used the Perceiver to replace the original Transformer in [AlphaStar](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii), the state-of-the-art reinforcement learning system for the complex game of [StarCraft II](https://starcraft2.com/en-us/). Without tuning any additional parameters, the authors observed that the resulting agent reached the same level of performance as the original AlphaStar agent, reaching an 87% win-rate versus the Elite bot after [behavioral cloning](https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) on human data.",
        "question": "What is the top-1 accuracy of the Perceiver on the ModelNet40 dataset?\n",
        "answer": "The top-1 accuracy of the Perceiver on the ModelNet40 dataset is 85.7%.",
        "source_doc": "huggingface/blog/blob/main/perceiver.md"
    },
    {
        "context": "Gradio Demo: translation\n### This translation demo takes in the text, source and target languages, and returns the translation. It uses the Transformers library to set up the model and has a title, description, and example.\n        \n\n\n```\n!pip install -q gradio git+https://github.com/huggingface/transformers gradio torch\n```\n\n\n```\nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nimport torch\n\n# this model was loaded from https://hf.co/models\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\ndevice = 0 if torch.cuda.is_available() else -1\nLANGS = [\"ace_Arab\", \"eng_Latn\", \"fra_Latn\", \"spa_Latn\"]\n\ndef translate(text, src_lang, tgt_lang):\n    \"\"\"\n    Translate the text from source lang to target lang\n    \"\"\"\n    translation_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang, max_length=400, device=device)\n    result = translation_pipeline(text)\n    return result[0]['translation_text']\n\ndemo = gr.Interface(\n    fn=translate,\n    inputs=[\n        gr.components.Textbox(label=\"Text\"),\n        gr.components.Dropdown(label=\"Source Language\", choices=LANGS),\n        gr.components.Dropdown(label=\"Target Language\", choices=LANGS),\n    ],\n    outputs=[\"text\"],\n    examples=[[\"Building a translation demo with Gradio is so easy!\", \"eng_Latn\", \"spa_Latn\"]],\n    cache_examples=False,\n    title=\"Translation Demo\",\n    description=\"This demo is a simplified version of the original [NLLB-Translator](https://huggingface.co/spaces/Narrativaai/NLLB-Translator) space\"\n)\n\ndemo.launch()\n```",
        "question": "What library is used to set up the translation model in the demo?\n",
        "answer": "The Transformers library is used to set up the translation model in the demo.",
        "source_doc": "gradio-app/gradio/blob/main/demo/translation/run.ipynb"
    },
    {
        "context": "prompt = \"A <cat-toy> train\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"cat-train.png\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Next steps\n\nCongratulations on training your own Textual Inversion model! 🎉 To learn more about how to use your new model, the following guides may be helpful:\n\n- Learn how to [load Textual Inversion embeddings](../using-diffusers/loading_adapters) and also use them as negative embeddings.\n- Learn how to use [Textual Inversion](textual_inversion_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL.",
        "question": "What is the name of the file where the generated image is saved?\n",
        "answer": "cat-train.png",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md"
    },
    {
        "context": "<Tip warning={true}>\n\nPlease make sure to have the `<<all_tools>>` string defined somewhere in the `template` so that the agent can be aware \nof the tools, it has available to it.\n\n</Tip>\n\nIn both cases, you can pass a repo ID instead of the prompt template if you would like to use a template hosted by someone in the community. The default prompts live in [this repo](https://huggingface.co/datasets/huggingface-tools/default-prompts) as an example.\n\nTo upload your custom prompt on a repo on the Hub and share it with the community just make sure:\n- to use a dataset repository\n- to put the prompt template for the `run` command in a file named `run_prompt_template.txt`\n- to put the prompt template for the `chat` command in a file named `chat_prompt_template.txt`\n\n## Using custom tools\n\nIn this section, we'll be leveraging two existing custom tools that are specific to image generation:\n\n- We replace [huggingface-tools/image-transformation](https://huggingface.co/spaces/huggingface-tools/image-transformation),\n  with [diffusers/controlnet-canny-tool](https://huggingface.co/spaces/diffusers/controlnet-canny-tool) \n  to allow for more image modifications.\n- We add a new tool for image upscaling to the default toolbox: \n  [diffusers/latent-upscaler-tool](https://huggingface.co/spaces/diffusers/latent-upscaler-tool) replace the existing image-transformation tool.\n\nWe'll start by loading the custom tools with the convenient [`load_tool`] function:\n\n```py\nfrom transformers import load_tool\n\ncontrolnet_transformer = load_tool(\"diffusers/controlnet-canny-tool\")\nupscaler = load_tool(\"diffusers/latent-upscaler-tool\")\n```\n\nUpon adding custom tools to an agent, the tools' descriptions and names are automatically\nincluded in the agents' prompts. Thus, it is imperative that custom tools have\na well-written description and name in order for the agent to understand how to use them.\nLet's take a look at the description and name of `controlnet_transformer`:",
        "question": "What is the description of the tool that upscales images?\n",
        "answer": "This tool allows you to upscale images by a factor of 2x, 4x, or 8x using a diffusion model.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/custom_tools.md"
    },
    {
        "context": "### Experiments\n\nWe evaluate TAPEX on four benchmark datasets, including [WikiSQL (Weak)](https://huggingface.co/datasets/wikisql), [WikiTableQuestions](https://huggingface.co/datasets/wikitablequestions), [SQA](https://huggingface.co/datasets/msr_sqa) and [TabFact](https://huggingface.co/datasets/tab_fact). The first three datasets are about table question answering, while the last one is about table fact verification, both requiring joint reasoning about tables and natural language. Below are some examples from the most challenging dataset, WikiTableQuestions:\n\n| Question | Answer |\n|:---: |:---:|\n| according to the table, what is the last title that spicy horse produced? | Akaneiro: Demon Hunters |\n| what is the difference in runners-up from coleraine academical institution and royal school dungannon? | 20 |\n| what were the first and last movies greenstreet acted in? | The Maltese Falcon, Malaya |\n| in which olympic games did arasay thondike not finish in the top 20? | 2012 |\n| which broadcaster hosted 3 titles but they had only 1 episode? | Channel 4 |\n\nExperimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and ⭐achieves new state-of-the-art results on all of them⭐. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to **89.6%** (+2.3% over SOTA, +3.8% over BART), the TabFact accuracy to **84.2%** (+3.2% over SOTA, +3.0% over BART), the SQA denotation accuracy to **74.5%** (+3.5% over SOTA, +15.9% over BART), and the WikiTableQuestion denotation accuracy to **57.5%** (+4.8% over SOTA, +19.5% over BART). To our knowledge, this is the first work to exploit pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks.\n\n![corpus](assets/74_tapex/tapex-performance.png)\n\n\n### Comparison to Previous Table Pre-training",
        "question": "What is the denotation accuracy of TAPEX on WikiSQL?\n",
        "answer": "The denotation accuracy of TAPEX on WikiSQL is 89.6%.",
        "source_doc": "huggingface/blog/blob/main/tapex.md"
    },
    {
        "context": "## What is Falcon-180B?\n\nFalcon 180B is a model released by [TII](https://falconllm.tii.ae/) that follows previous releases in the Falcon family.\n\nArchitecture-wise, Falcon 180B is a scaled-up version of [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b) and builds on its innovations such as multiquery attention for improved scalability. We recommend reviewing the [initial blog post](https://huggingface.co/blog/falcon) introducing Falcon to dive into the architecture. Falcon 180B was trained on 3.5 trillion tokens on up to 4096 GPUs simultaneously, using Amazon SageMaker for a total of ~7,000,000 GPU hours. This means Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute. \n\nThe dataset for Falcon 180B consists predominantly of web data from [RefinedWeb](https://arxiv.org/abs/2306.01116) (\\~85%). In addition, it has been trained on a mix of curated data such as conversations, technical papers, and a small fraction of code (\\~3%). This pretraining dataset is big enough that even 3.5 trillion tokens constitute less than an epoch.\n\nThe released [chat model](https://huggingface.co/tiiuae/falcon-180B-chat) is fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.\n\n‼️ Commercial use: \nFalcon 180b can be commercially used but under very restrictive conditions, excluding any \"hosting use\". We recommend to check the [license](https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt) and consult your legal team if you are interested in using it for commercial purposes.\n\n\n## How good is Falcon 180B?",
        "question": "How large is the Falcon 180B model?\n",
        "answer": "The Falcon 180B model is 2.5 times larger than Llama 2.",
        "source_doc": "huggingface/blog/blob/main/falcon-180b.md"
    },
    {
        "context": "|      |                                                                            |[zohaib99k/Nous-Hermes-Llama2-8bit-GPTQ](https://huggingface.co/zohaib99k/Nous-Hermes-Llama2-8bit-GPTQ)|5           |1                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/zohaib99k/Nous-Hermes-Llama2-8bit-GPTQ/blob/main/LICENSE.txt) |                                                                                                                     |                                                                                   |\n|      |                                                                            |[Jsevisal/ft-distilbert-gest-pred-seqeval-partialmatch](https://huggingface.co/Jsevisal/ft-distilbert-gest-pred-seqeval-partialmatch)|4           |0                        |                                                                                              |                                                                                              |[LICENSE](https://huggingface.co/Jsevisal/ft-distilbert-gest-pred-seqeval-partialmatch/blob/main/LICENSE)|                                                                                                                     |                                                                                   |",
        "question": "What is the license of the model ft-distilbert-gest-pred-seqeval-partialmatch?\n",
        "answer": "The license of the model ft-distilbert-gest-pred-seqeval-partialmatch is not specified in the context.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "Gradio Demo: blocks_kitchen_sink\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time\nfrom os.path import abspath, join, pardir\n\nKS_FILES = abspath(join(__file__, pardir, pardir, \"kitchen_sink\", \"files\"))\n\nbase_theme = gr.themes.Base()\ndefault_theme = gr.themes.Default()\nmonochrome_theme = gr.themes.Monochrome()\nsoft_theme = gr.themes.Soft()\nglass_theme = gr.themes.Glass()\n\nwith gr.Blocks(theme=base_theme) as demo:\n    gr.Markdown(\n        \"\"\"\n    # Blocks Kitchen Sink\n    This is a demo of most Gradio features. Test all themes and toggle dark mode\n    ## Elements\n    - Use of Rows, Columns, Tabs, and Accordion\n    - Use of Form elements: Textbox, Dropdown, Checkbox, Radio, Slider\n    ## Other\n    Other stuff\n    - Buttons of variants: \"primary\", \"secondary\", \"stop\"\n    - Embedded interface\n    - Custom progress bar\n    \"\"\"\n    )\n    toggle_dark = gr.Button(\"Toggle Dark\", scale=0)\n    toggle_dark.click(\n        None,\n        js=\"\"\"\n        () => { \n            document.body.classList.toggle('dark');\n        }\n        \"\"\",\n    )\n    theme_selector = gr.Radio(\n        [\"Base\", \"Default\", \"Monochrome\", \"Soft\", \"Glass\"],\n        value=\"Base\",\n        label=\"Theme\",\n    )\n    theme_selector.change(\n        None,\n        theme_selector,\n        None,\n        js=f\"\"\"\n        (theme) => {{\n            if (!document.querySelector('.theme-css')) {{\n                var theme_elem = document.createElement('style');\n                theme_elem.classList.add('theme-css');\n                document.head.appendChild(theme_elem);",
        "question": "What is the name of the theme that is the default theme in Gradio?\n",
        "answer": "Default",
        "source_doc": "gradio-app/gradio/blob/main/demo/blocks_kitchen_sink/run.ipynb"
    },
    {
        "context": "__Note__\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install the dependencies either with our prepared*\n*[Dockerfiles](https://github.com/huggingface/optimum/blob/main/examples/onnxruntime/training/docker/) or follow the instructions*\n*in [`torch_ort`](https://github.com/pytorch/ort/blob/main/torch_ort/docker/README.md).*\n---",
        "question": "How can I enable ONNX Runtime training on my device?\n",
        "answer": "Your devices need to be equipped with GPU to enable ONNX Runtime training. You can install the dependencies using the provided Dockerfiles or follow the instructions in `torch_ort`.",
        "source_doc": "huggingface/optimum/blob/main/examples/onnxruntime/training/image-classification/README.md"
    },
    {
        "context": "## Challenges with fine-tuning LLaMa 70B\n\nWe encountered three main challenges when trying to fine-tune LLaMa 70B with FSDP:\n\n1. FSDP wraps the model after loading the pre-trained model. If each process/rank within a node loads the Llama-70B model, it would require 70\\*4\\*8 GB ~ 2TB of CPU RAM, where 4 is the number of bytes per parameter and 8 is the number of GPUs on each node. This would result in the CPU RAM getting out of memory leading to processes being terminated.\n\n2. Saving entire intermediate checkpoints using `FULL_STATE_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. \n\n3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs.\n\nLet’s look at how to solve the above challenges and fine-tune a 70B model!\n\nBefore we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat_assistant/training with flash-attn V2 monkey patch\n\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/configs/fsdp_config.yaml\n\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n\n### Pre-requisites",
        "question": "What is the required amount of CPU RAM to load the Llama-70B model on each process/rank within a node?\n",
        "answer": "The required amount of CPU RAM to load the Llama-70B model on each process/rank within a node is 70*4*8 GB ~ 2TB.",
        "source_doc": "huggingface/blog/blob/main/ram-efficient-pytorch-fsdp.md"
    },
    {
        "context": "Ray tune is a popular library for hyper-parameter optimization which comes with many SOTA algorithms out of the box. It is also possible to use [Optuna](https://optuna.readthedocs.io/en/stable/index.html) and [SigOpt](https://sigopt.com/).\nWe also used [Async Successive Halving Algorithm [(ASHA)](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler) as the scheduler and [HyperOpt](https://hyperopt.github.io/hyperopt/) as the search algorithm. Which is pretty much a starting point. You can use different [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html) and [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html).\n\nWhat will we do?\n\n- Import the necessary libraries (a dozen of them) and prepare a dataset class\n- Define needed functions and methods to process the data\n- Load the pre-trained model and tokenizer\n- Run hyper-parameter search\n- Use the best results for evaluation\n\nLet’s start with importing necessary libraries!\n(all the code is in [notebooks/modeling.ipynb](https://github.com/alperiox/review-classification-kili-hf-automl/blob/master/notebooks/modeling.ipynb) and [google collaboratory notebook](https://colab.research.google.com/drive/1YL-q3_JTEnOtoQdiDUnwSxLVn9Aqpzs8?usp=sharing))\n\n```python\n# general data science/utilization/visualization imports\nimport json\nimport os\nimport random\n\n# progress bar\nfrom tqdm import tqdm\n\n# data manipulation / reading\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\n# pre-defined evaluation metrics\nfrom sklearn.metrics import (accuracy_score, f1_score,\n                             precision_score, recall_score)",
        "question": "What is the name of the popular library for hyper-parameter optimization?\n",
        "answer": "Ray tune",
        "source_doc": "huggingface/blog/blob/main/opinion-classification-with-kili.md"
    },
    {
        "context": "1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\n1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.\n1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\n1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.",
        "question": "Which model was released by Microsoft Research?\n",
        "answer": "ProphetNet",
        "source_doc": "huggingface/transformers/blob/main/README_ru.md"
    },
    {
        "context": "```python \n>>> # from audio\n>>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n\n>>> # from text\n>>> output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n```\n\n### Tips\n\n\n#### 1. Use dedicated models\n\n[`SeamlessM4Tv2Model`] is transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\nFor example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code: \n\n```python\n>>> from transformers import SeamlessM4Tv2ForSpeechToSpeech\n>>> model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```\n\nOr you can replace the text-to-text generation snippet with the model dedicated to the T2TT task, you only have to remove `generate_speech=False`.\n\n```python\n>>> from transformers import SeamlessM4Tv2ForTextToText\n>>> model = SeamlessM4Tv2ForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```\n\nFeel free to try out [`SeamlessM4Tv2ForSpeechToText`] and [`SeamlessM4Tv2ForTextToSpeech`] as well.\n\n#### 2. Change the speaker identity\n\nYou have the possibility to change the speaker used for speech synthesis with the `speaker_id` argument. Some `speaker_id` works better than other for some languages!\n\n#### 3. Change the generation strategy",
        "question": "How to change the return attention scores in SeamlessM4Tv2?\n",
        "answer": "The return attention scores",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md"
    },
    {
        "context": "### Fixes\n\n- [#6528](https://github.com/gradio-app/gradio/pull/6528) [`f53b01cbf`](https://github.com/gradio-app/gradio/commit/f53b01cbfbfccec66e0cda1d428ef72f05a3dfc0) - Fix Theme Dropdown in deployed theme space.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#6546](https://github.com/gradio-app/gradio/pull/6546) [`a424fdbb2`](https://github.com/gradio-app/gradio/commit/a424fdbb2389219661b9a73197f4cc095a08cfe9) - Ensure audio waveform `autoplay` updates.  Thanks [@hannahblair](https://github.com/hannahblair)!\n- [#6536](https://github.com/gradio-app/gradio/pull/6536) [`1bbd6cab3`](https://github.com/gradio-app/gradio/commit/1bbd6cab3f0abe183b514b82061f0937c8480966) - Fix undefined `data` TypeError in Blocks.  Thanks [@hannahblair](https://github.com/hannahblair)!\n- [#6500](https://github.com/gradio-app/gradio/pull/6500) [`830b6c0e6`](https://github.com/gradio-app/gradio/commit/830b6c0e6e52c4fa33fddfa4d3f6162e29801f74) - Process and convert .svg files in `Image`.  Thanks [@hannahblair](https://github.com/hannahblair)!\n\n## 4.5.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\n\nA brand new component, completely separate from `Image` that provides simple editing capabilities.",
        "question": "What is the name of the new component in Gradio 4.5.0?\n",
        "answer": "The name of the new component in Gradio 4.5.0 is `ImageEditor`.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed data are saved locally \nto disk. If we want to re-use this dataset, we can directly load the processed data from disk, skipping the download and \nprocessing steps. Consequently, we only have to perform the downloading and processing operations once, after which we \ncan re-use the prepared data. With streaming mode, the data is not downloaded to disk. Thus, neither the downloaded nor \npre-processed data are cached. If we want to re-use the dataset, the streaming steps must be repeated, with the audio \nfiles loaded and processed on the fly again. For this reason, it is advised to download datasets that you are likely to use\nmultiple times.\n\nHow can you enable streaming mode? Easy! Just set `streaming=True` when you load your dataset. The rest will be taken \ncare for you:\n\n```python\ngigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", streaming=True)\n```\n\nAll the steps covered so far in this tutorial can be applied to the streaming dataset without any code changes.\nThe only change is that you can no longer access individual samples using Python indexing (i.e. `gigaspeech[\"train\"][sample_idx]`). \nInstead, you have to iterate over the dataset, using a `for` loop for example.",
        "question": "How does streaming mode affect data caching?\n",
        "answer": "In streaming mode, neither the downloaded nor pre-processed data are cached, so the streaming steps must be repeated if the dataset is re-used.",
        "source_doc": "huggingface/blog/blob/main/audio-datasets.md"
    },
    {
        "context": "|      |                                                                            |[TheBloke/tulu-7B-GPTQ](https://huggingface.co/TheBloke/tulu-7B-GPTQ)|5           |9                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/tulu-7B-GPTQ/blob/main/LICENSE.txt)                  |                                                                                                                     |                                                                                   |\n|      |                                                                            |[valurank/pegasus-multi_news-headline](https://huggingface.co/valurank/pegasus-multi_news-headline)|5           |1                        |                                                                                              |                                                                                              |[LICENSE](https://huggingface.co/valurank/pegasus-multi_news-headline/blob/main/LICENSE)           |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the model TheBloke/tulu-7B-GPTQ?\n",
        "answer": "The license for the model TheBloke/tulu-7B-GPTQ is llama-2-community-license.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "The simulate library is an exploration on how one could use python to easily build & share complex and diverse simulation environments for embodied learning or synthetic data research.\n\nThe basic idea is to decouple the creation of the simulation environment (\"building\") from the simulation engine used to run it (Unity, Blender, custom engine, etc) by relying on an engine-agnostic sharing format (the open standard glTF format in this case).\n\nThe created environments are stored in a language/framework agnostic format and can be loaded and run on a diversity of engines with concise integrations handling more or less of the glTF extensions we use (we provide PoC plugins for Unity, Godot and Blender in the alpha release).\n\nInterfacing with the git-versioning and hosting on the Hugging Face hub allow to download/upload share/reuse assets (objects) as well as full scenes (environments).\n\n## Building on the shoulders of giants\n\nThe python API was inspired by the awesome kubric library created by Klaus Greff and Andrea Tagliasacchi and the Google team (https://github.com/google-research/kubric) while the Unity engine was inspired in part by the impressive work of the PRIOR team at AllenAI (https://prior.allenai.org/).",
        "question": "How are the created environments shared and reused in the simulate library?\n",
        "answer": "The created environments are shared and reused by interfacing with git-versioning and hosting on the Hugging Face hub in the simulate library.",
        "source_doc": "huggingface/simulate/blob/main/docs/source/conceptual/philosophy.mdx"
    },
    {
        "context": "demo for predicting the depth of an image and generating a 3D model of it.",
        "question": "What is the resolution of the 3D model generated from the image?\n",
        "answer": "The resolution of the 3D model generated from the image is 512x512x1000.",
        "source_doc": "gradio-app/gradio/blob/main/demo/depth_estimation/DESCRIPTION.md"
    },
    {
        "context": "Hub methods\n\nMethods for using the Hugging Face Hub:\n\n## Push to hub \n\n[[autodoc]] evaluate.push_to_hub",
        "question": "How do I push a model to the Hugging Face Hub?\n",
        "answer": "You can push a model to the Hugging Face Hub using the `evaluate.push_to_hub` method.",
        "source_doc": "huggingface/evaluate/blob/main/docs/source/package_reference/hub_methods.mdx"
    },
    {
        "context": "- New meta img by [@aliabd](https://github.com/aliabd) in [PR 1289](https://github.com/gradio-app/gradio/pull/1289)\n- updated PyPi version to 3.0 by [@abidlabs](https://github.com/abidlabs) in [PR 1290](https://github.com/gradio-app/gradio/pull/1290)\n- Fix site by [@aliabid94](https://github.com/aliabid94) in [PR 1291](https://github.com/gradio-app/gradio/pull/1291)\n- Mobile responsive guides by [@aliabd](https://github.com/aliabd) in [PR 1293](https://github.com/gradio-app/gradio/pull/1293)\n- Update readme by [@abidlabs](https://github.com/abidlabs) in [PR 1292](https://github.com/gradio-app/gradio/pull/1292)\n- gif by [@abidlabs](https://github.com/abidlabs) in [PR 1296](https://github.com/gradio-app/gradio/pull/1296)\n- Allow decoding headerless b64 string [@1lint](https://github.com/1lint) in [PR 4031](https://github.com/gradio-app/gradio/pull/4031)",
        "question": "Who updated the PyPi version to 3.0?\n",
        "answer": "@abidlabs",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "- Reset components to original state by setting value to None by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2044](https://github.com/gradio-app/gradio/pull/2044)\n- Cleaning up the way data is processed for components by [@abidlabs](https://github.com/abidlabs) in [PR 1967](https://github.com/gradio-app/gradio/pull/1967)\n- version 3.1.8b by [@abidlabs](https://github.com/abidlabs) in [PR 2063](https://github.com/gradio-app/gradio/pull/2063)\n- Wandb guide by [@AK391](https://github.com/AK391) in [PR 1898](https://github.com/gradio-app/gradio/pull/1898)\n- Add a flagging callback to save json files to a hugging face dataset by [@chrisemezue](https://github.com/chrisemezue) in [PR 1821](https://github.com/gradio-app/gradio/pull/1821)\n- Add data science demos to landing page by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2067](https://github.com/gradio-app/gradio/pull/2067)\n- Hide time series + xgboost demos by default by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2079](https://github.com/gradio-app/gradio/pull/2079)\n- Encourage people to keep trying when queue full by [@apolinario](https://github.com/apolinario) in [PR 2076](https://github.com/gradio-app/gradio/pull/2076)\n- Updated our analytics on creation of Blocks/Interface by [@abidlabs](https://github.com/abidlabs) in [PR 2082](https://github.com/gradio-app/gradio/pull/2082)\n- `Label` component now accepts file paths to `.json` files by [@abidlabs](https://github.com/abidlabs) in [PR 2083](https://github.com/gradio-app/gradio/pull/2083)\n- Fix issues related to demos in Spaces by [@abidlabs](https://github.com/abidlabs) in [PR 2086](https://github.com/gradio-app/gradio/pull/2086)\n- Fix TimeSeries examples not properly displayed in UI by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2064](https://github.com/gradio-app/gradio/pull/2064)",
        "question": "Which component now accepts file paths to .json files?\n",
        "answer": "The `Label` component now accepts file paths to `.json` files.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "You should be redirected to your SSO provider (IdP) login prompt. Once logged in, you'll be redirected to your organization's settings page.\n\nA green check mark near the SAML selector will attest that the test was successful.\n\n\n<div class=\"flex justify-center\">\n\t<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-6.png\"/>\n\t<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-6-dark.png\"/>\n</div>\n\n### Step 4: Enable SSO in your organization\n\nNow that Single Sign-On is configured and tested, you can enable it for members of your organization by clicking on the \"Enable\" button.\n\nOnce enabled, members of your organization must complete the SSO authentication flow described in the [How does it work?](./security-sso#how-does-it-work) section.",
        "question": "How can members of an organization enable SSO?\n",
        "answer": "Members of an organization can enable SSO by clicking on the \"Enable\" button in their organization's settings page.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security-sso-okta-saml.md"
    },
    {
        "context": "Let's check that we've successfully retained the `text` and `audio` columns:\n\n```python\nprint(gigaspeech[\"train\"][0])\n```\n\n**Print Output:**\n\n```python\n{'text': \"AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA ASIDE REALLY QUICKLY <QUESTIONMARK>\", \n 'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/7f8541f130925e9b2af7d37256f2f61f9d6ff21bf4a94f7c1a3803ec648d7d79/xs_chunks_0000/YOU0000000315_S0000660.wav', \n           'array': array([0.0005188 , 0.00085449, 0.00012207, ..., 0.00125122, 0.00076294,\n       0.00036621], dtype=float32), \n           'sampling_rate': 16000}}\n```\n\nGreat! We can see that we've got the two required columns `text` and `audio`. The `text` is a string with the sample\ntranscription and the `audio` a 1-dimensional array of amplitude values at a sampling rate of 16KHz. That's our \ndataset loaded!\n\n## Easy to Load, Easy to Process\n\nLoading a dataset with 🤗 Datasets is just half of the fun. We can now use the suite of tools available to efficiently \npre-process our data ready for model training or inference. In this Section, we'll perform three stages of data \npre-processing:\n\n1. [Resampling the Audio Data](#1-resampling-the-audio-data)\n2. [Pre-Processing Function](#2-pre-processing-function)\n3. [Filtering Function](#3-filtering-function)\n\n### 1. Resampling the Audio Data\n\nThe `load_dataset` function prepares audio samples with the sampling rate that they were published with. This is not \nalways the sampling rate expected by our model. In this case, we need to _resample_ the audio to the correct sampling \nrate.",
        "question": "What is the sampling rate of the audio data in the given context?\n",
        "answer": "The sampling rate of the audio data in the given context is 16KHz.",
        "source_doc": "huggingface/blog/blob/main/audio-datasets.md"
    },
    {
        "context": "### July 27, 2023\n* Added timm trained `seresnextaa201d_32x8d.sw_in12k_ft_in1k_384` weights (and `.sw_in12k` pretrain) with 87.3% top-1 on ImageNet-1k, best ImageNet ResNet family model I'm aware of.\n* RepViT model and weights (https://arxiv.org/abs/2307.09283) added by [wangao](https://github.com/jameslahm)\n* I-JEPA ViT feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\n* SAM-ViT (segment anything) feature weights (no classifier) added by [SeeFun](https://github.com/seefun)\n* Add support for alternative feat extraction methods and -ve indices to EfficientNet\n* Add NAdamW optimizer\n* Misc fixes\n\n### May 11, 2023\n* `timm` 0.9 released, transition from 0.8.xdev releases\n\n### May 10, 2023\n* Hugging Face Hub downloading is now default, 1132 models on https://huggingface.co/timm, 1163 weights in `timm`\n* DINOv2 vit feature backbone weights added thanks to [Leng Yue](https://github.com/leng-yue)\n* FB MAE vit feature backbone weights added\n* OpenCLIP DataComp-XL L/14 feat backbone weights added\n* MetaFormer (poolformer-v2, caformer, convformer, updated poolformer (v1)) w/ weights added by [Fredo Guan](https://github.com/fffffgggg54)\n* Experimental `get_intermediate_layers` function on vit/deit models for grabbing hidden states (inspired by DINO impl). This is WIP and may change significantly... feedback welcome.\n* Model creation throws error if `pretrained=True` and no weights exist (instead of continuing with random initialization)\n* Fix regression with inception / nasnet TF sourced weights with 1001 classes in original classifiers\n* bitsandbytes (https://github.com/TimDettmers/bitsandbytes) optimizers added to factory, use `bnb` prefix, ie `bnbadam8bit`\n* Misc cleanup and fixes\n* Final testing before switching to a 0.9 and bringing `timm` out of pre-release state",
        "question": "What is the best ImageNet ResNet family model according to the passage?\n",
        "answer": "The best ImageNet ResNet family model according to the passage is the timm trained `seresnextaa201d_32x8d.sw_in12k_ft_in1k_384` model with 87.3% top-1 on ImageNet-1k.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/README.md"
    },
    {
        "context": "def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n```\n\nNow that you have defined the structure of your model, you need to train it on the standard MNIST train/dev dataset.\n\n### Interacting with your model\n\nAt this point we assume you have your trained model. Although this model is trained, we aim to make it robust using human-in-the-loop adversarial data. For that, you need a way for users to interact with it: specifically you want users to be able to write/draw numbers from 0-9 on a canvas and have the model try to classify it. You can do all that with [🤗 Spaces](https://huggingface.co/spaces) which allows you to quickly and easily build a demo for your ML models. Learn more about Spaces and how to build them [here](https://huggingface.co/spaces/launch). \n\nBelow is a simple Space to interact with the `MNIST_Model` which I trained for 20 epochs (achieved 89% accuracy on the test set). You draw a number on the white canvas and the model predicts the number from your image. The full Space can be accessed [here](https://huggingface.co/spaces/chrisjay/simple-mnist-classification). Try to fool this model😁. Use your funniest handwriting; write on the sides of the canvas; go wild!",
        "question": "How many layers does the MNIST model have?\n",
        "answer": "The MNIST model has 5 layers.\n```",
        "source_doc": "huggingface/blog/blob/main/mnist-adversarial.md"
    },
    {
        "context": "In general, you don't need to worry about whether or not there are `token_type_ids` in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.\n\nNow that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the [previous chapter](/course/chapter2), we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in [Chapter 2](/course/chapter2). So, one way to preprocess the training dataset is:\n\n```py\ntokenized_dataset = tokenizer(\n    raw_datasets[\"train\"][\"sentence1\"],\n    raw_datasets[\"train\"][\"sentence2\"],\n    padding=True,\n    truncation=True,\n)\n```\n\nThis works well, but it has the disadvantage of returning a dictionary (with our keys, `input_ids`, `attention_mask`, and `token_type_ids`, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the 🤗 Datasets library are [Apache Arrow](https://arrow.apache.org/) files stored on the disk, so you only keep the samples you ask for loaded in memory).\n\nTo keep the data as a dataset, we will use the [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The `map()` method works by applying a function on each element of the dataset, so let's define a function that tokenizes our inputs:\n\n```py\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n```",
        "question": "How should a function that tokenizes inputs be defined?\n",
        "answer": "A function that tokenizes inputs should be defined as `tokenize_function(example)`, where `example` is a dictionary containing the keys \"sentence1\" and \"sentence2\". The function should return the tokenized inputs using the `tokenizer` object, with the `truncation` parameter set to `True`.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter3/2.mdx"
    },
    {
        "context": "## Implementation Notes\n\n- The original implementation of MEGA had an inconsistent expectation of attention masks for padding and causal self-attention between the softmax attention and Laplace/squared ReLU method. This implementation addresses that inconsistency.\n- The original implementation did not include token type embeddings; this implementation adds support for these, with the option controlled by MegaConfig.add_token_type_embeddings\n\n\n## MegaConfig\n\n[[autodoc]] MegaConfig\n\n## MegaModel\n\n[[autodoc]] MegaModel\n    - forward\n\n## MegaForCausalLM\n\n[[autodoc]] MegaForCausalLM\n    - forward\n\n## MegaForMaskedLM\n\n[[autodoc]] MegaForMaskedLM\n    - forward\n\n## MegaForSequenceClassification\n\n[[autodoc]] MegaForSequenceClassification\n    - forward\n\n## MegaForMultipleChoice\n\n[[autodoc]] MegaForMultipleChoice\n    - forward\n\n## MegaForTokenClassification\n\n[[autodoc]] MegaForTokenClassification\n    - forward\n\n## MegaForQuestionAnswering\n\n[[autodoc]] MegaForQuestionAnswering\n    - forward",
        "question": "What is the name of the implementation that addresses the inconsistency in the original MEGA implementation regarding attention masks for padding and causal self-attention?\n",
        "answer": "This implementation is not named in the context. It only mentions that it addresses the inconsistency in the original MEGA implementation regarding attention masks for padding and causal self-attention.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/mega.md"
    },
    {
        "context": "We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in multi-modal research, you can follow us on Twitter: [@adirik](https://twitter.com/https://twitter.com/alaradirik), [@NielsRogge](https://twitter.com/NielsRogge), [@apsdehal](https://twitter.com/apsdehal), [@a_e_roberts](https://twitter.com/a_e_roberts), [@RisingSayak](https://mobile.twitter.com/a_e_roberts), and [@huggingface](https://twitter.com/huggingface).\n\n*Acknowledgements: We thank Amanpreet Singh and Amy Roberts for their rigorous reviews. Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.*",
        "question": "Who are the Twitter accounts to follow for the latest news in multi-modal research?\n",
        "answer": "The Twitter accounts to follow for the latest news in multi-modal research are @adirik, @NielsRogge, @apsdehal, @a_e_roberts, @RisingSayak, and @huggingface.",
        "source_doc": "huggingface/blog/blob/main/vision_language_pretraining.md"
    },
    {
        "context": "**Red-teaming** *is a form of evaluation that elicits model vulnerabilities that might lead to undesirable behaviors.* Jailbreaking is another term for red-teaming wherein the LLM is manipulated to break away from its guardrails. [Microsoft’s Chatbot Tay](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/) launched in 2016 and the more recent [Bing's Chatbot Sydney](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html) are real-world examples of how disastrous the lack of thorough evaluation of the underlying ML model using red-teaming can be. The origins of the idea of a red-team traces back to adversary simulations and wargames performed by militaries.\n\n\nThe goal of red-teaming language models is to craft a prompt that would trigger the model to generate text that is likely to cause harm. Red-teaming shares some similarities and differences with the more well-known form of evaluation in ML called *adversarial attacks*. The similarity is that both red-teaming and adversarial attacks share the same goal of “attacking” or “fooling” the model to generate content that would be undesirable in a real-world use case. However, adversarial attacks can be unintelligible to humans, for example, by prefixing the string “aaabbbcc” to each prompt because it deteriorates model performance. Many examples of such attacks on various NLP classification and generation tasks is discussed  in [Wallace et al., ‘19](https://arxiv.org/abs/1908.07125). Red-teaming prompts, on the other hand, look like regular, natural language prompts.\n\nRed-teaming can reveal model limitations that can cause upsetting user experiences or enable harm by aiding violence or other unlawful activity for a user with malicious intentions. The outputs from red-teaming (just like adversarial attacks) are generally used to train the model to be less likely to cause harm or steer it away from undesirable outputs.",
        "question": "What is the goal of red-teaming language models?\n",
        "answer": "The goal of red-teaming language models is to craft a prompt that would trigger the model to generate text that is likely to cause harm.",
        "source_doc": "huggingface/blog/blob/main/red-teaming.md"
    },
    {
        "context": "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">简体中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\n        <b>Español</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a> |\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>Lo último de Machine Learning para JAX, PyTorch y TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n🤗 Transformers aporta miles de modelos preentrenados Para realizar tareas en diferentes modalidades como texto, vision, y audio.\n\nEstos modelos pueden ser aplicados en:\n\n* 📝 Texto, Para tareas como clasificación de texto, extracción de información, responder preguntas, resumir, traducir, generación de texto, en más de 100 idiomas.\n* 🖼️ Imágenes, para tareas como clasificación de imágenes, detección the objetos, y segmentación.\n* 🗣️ Audio, para tareas como reconocimiento de voz y clasificación de audio.\n\nLos modelos de Transformer también pueden realizar tareas en **muchas modalidades combinadas**, como responder pregunstas, reconocimiento de carácteres ópticos,extracción de información de documentos escaneados, clasificación de video, y respuesta de preguntas visuales.",
        "question": "What are some tasks that the models of Transformers can perform?\n",
        "answer": "The models of Transformers can perform tasks such as text classification, information extraction, question answering, text summarization, translation, text generation, image classification, object detection, image segmentation, speech recognition, and audio classification. They can also perform tasks in multiple combined modalities, such as optical character recognition, information extraction from scanned documents, video classification, and visual question answering.",
        "source_doc": "huggingface/transformers/blob/main/README_es.md"
    },
    {
        "context": "```py\nissues_dataset.set_format(\"pandas\")\ndf = issues_dataset[:]\n```\n\nIf we inspect the first row in this `DataFrame` we can see there are four comments associated with this issue:\n\n```py\ndf[\"comments\"][0].tolist()\n```\n\n```python out\n['the bug code locate in ：\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)',\n 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',\n 'cannot connect，even by Web browser，please check that  there is some  problems。',\n 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']\n```\n\nWhen we explode `df`, we expect to get one row for each of these comments. Let's check if that's the case:\n\n```py\ncomments_df = df.explode(\"comments\", ignore_index=True)\ncomments_df.head(4)\n```",
        "question": "How many rows does the `comments_df` DataFrame have after exploding the `comments` column?\n",
        "answer": "The `comments_df` DataFrame has 16 rows after exploding the `comments` column.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/6.mdx"
    },
    {
        "context": "## Optimum ONNX Runtime",
        "question": "What is the Optimum ONNX Runtime?\n",
        "answer": "The Optimum ONNX Runtime is a machine learning inference engine that supports a wide variety of machine learning models and frameworks. It is designed to be fast, scalable, and easy to use, making it an ideal choice for deploying machine learning models in production environments.",
        "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md"
    },
    {
        "context": "We can now process the input image and prompt image and input them to\nthe model.\n\n```python\nencoded_image = processor(images=[image], return_tensors=\"pt\")\nencoded_prompt = processor(images=[prompt], return_tensors=\"pt\")\n# predict\nwith torch.no_grad():\n  outputs = model(**encoded_image, conditional_pixel_values=encoded_prompt.pixel_values)\npreds = outputs.logits.unsqueeze(1)\npreds = torch.transpose(preds, 0, 1)\n```\n\nThen, we can visualize the results as before.\n\n```python\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\nax[1].imshow(torch.sigmoid(preds[0]))\n```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The mask of the coffee cup in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/fbde45fc65907d17de38b0db3eb262bdec1f1784.png\"></medium-zoom>\n</figure>\n\nLet's try one last time by using the visual prompting tips described in\nthe paper, i.e. cropping the image and darkening the background.\n\n```python\nurl = \"https://i.imgur.com/mRSORqz.jpg\"\nalternative_prompt = Image.open(requests.get(url, stream=True).raw)\nalternative_prompt\n```\n\n<figure class=\"image table text-center m-0 w-6/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A cropped version of the image of the coffee cup with a darker background.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/915a97da22131e0ab6ff4daa78ffe3f1889e3386.png\"></medium-zoom>\n</figure>\n\n```python\nencoded_alternative_prompt = processor(images=[alternative_prompt], return_tensors=\"pt\")\n# predict\nwith torch.no_grad():\n  outputs = model(**encoded_image, conditional_pixel_values=encoded_alternative_prompt.pixel_values)\npreds = outputs.logits.unsqueeze(1)\npreds = torch.transpose(preds, 0, 1)\n```",
        "question": "How do we process the input image and prompt image in the given context?\n",
        "answer": "In the given context, the input image and prompt image are processed by first encoding them using the processor, and then inputting them to the model. The encoded images are then used to predict the output using the model.",
        "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md"
    },
    {
        "context": "In the following, we give an overview of different ways to contribute, ranked by difficulty in ascending order. All of them are valuable to the community.\n\n* 1. Asking and answering questions on [the Diffusers discussion forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers) or on [Discord](https://discord.gg/G7tWnz98XR).\n* 2. Opening new issues on [the GitHub Issues tab](https://github.com/huggingface/diffusers/issues/new/choose).\n* 3. Answering issues on [the GitHub Issues tab](https://github.com/huggingface/diffusers/issues).\n* 4. Fix a simple issue, marked by the \"Good first issue\" label, see [here](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).\n* 5. Contribute to the [documentation](https://github.com/huggingface/diffusers/tree/main/docs/source).\n* 6. Contribute a [Community Pipeline](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3Acommunity-examples).\n* 7. Contribute to the [examples](https://github.com/huggingface/diffusers/tree/main/examples).\n* 8. Fix a more difficult issue, marked by the \"Good second issue\" label, see [here](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22Good+second+issue%22).\n* 9. Add a new pipeline, model, or scheduler, see [\"New Pipeline/Model\"](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22) and [\"New scheduler\"](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22) issues. For this contribution, please have a look at [Design Philosophy](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md).\n\nAs said before, **all contributions are valuable to the community**.\nIn the following, we will explain each contribution a bit more in detail.",
        "question": "What is the easiest way to contribute to the Diffusers project?\n",
        "answer": "The easiest way to contribute to the Diffusers project is by asking and answering questions on the Diffusers discussion forum or on Discord.",
        "source_doc": "huggingface/diffusers/blob/main/CONTRIBUTING.md"
    },
    {
        "context": "Access 🤗 Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account.\n\n<Tip>\n  \nYou can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method.\n  \n</Tip>\n\nThere are two pricing plans:\n\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. \n- There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested!\n\nAfter you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying! 🥳",
        "question": "How is the pricing for Inference Endpoints determined?\n",
        "answer": "The pricing for Inference Endpoints is based on hourly compute and billed monthly, with the cost per CPU core/hr being as low as $0.06 and the cost per GPU/hr being as low as $0.6, depending on the user's needs.",
        "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/access.mdx"
    },
    {
        "context": "'comments': 1,\n  'created_at': '2021-08-12T11:40:18Z',\n  'updated_at': '2021-08-12T12:31:17Z',\n  'closed_at': None,\n  'author_association': 'CONTRIBUTOR',\n  'active_lock_reason': None,\n  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',\n   'html_url': 'https://github.com/huggingface/datasets/pull/2792',\n   'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',\n   'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},\n  'body': '[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',\n  'performed_via_github_app': None}]\n```",
        "question": "What was recently updated in the GooAQ dataset?\n",
        "answer": "The GooAQ dataset was recently updated after splits were added for the same. This update includes new train/val/test splits and an updated README.\n```",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/5.mdx"
    },
    {
        "context": "Note that this section is a non-exhaustive list, and there are various other approaches, as well as hybrid strategies such as [Unified-IO](https://arxiv.org/abs/2206.08916). For a more comprehensive review of multi-modal models, refer to [this work.](https://arxiv.org/abs/2210.09263)\n\n### 1) Contrastive Learning\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/contrastive_learning.png\" alt=\"Contrastive Learning\"><br>\n    <em>Contrastive pre-training and zero-shot image classification as shown <a href=https://openai.com/blog/clip>here</a>.</em>\n</p>\n\nContrastive learning is a commonly used pre-training objective for vision models and has proven to be a highly effective pre-training objective for vision-language models as well. Recent works such as [CLIP](https://arxiv.org/abs/2103.00020), [CLOOB](https://arxiv.org/abs/2110.11316), [ALIGN](https://arxiv.org/abs/2102.05918), and [DeCLIP](https://arxiv.org/abs/2110.05208) bridge the vision and language modalities by learning a text encoder and an image encoder jointly with a contrastive loss, using large datasets consisting of {image, caption} pairs. Contrastive learning aims to map input images and texts to the same feature space such that the distance between the embeddings of image-text pairs is minimized if they match or maximized if they don’t. \n\nFor CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas models such as ALIGN and DeCLIP design their own distance metrics to account for noisy datasets.",
        "question": "What is the objective of contrastive learning in vision-language models?\n",
        "answer": "The objective of contrastive learning in vision-language models is to map input images and texts to the same feature space such that the distance between the embeddings of image-text pairs is minimized if they match or maximized if they don’t.",
        "source_doc": "huggingface/blog/blob/main/vision_language_pretraining.md"
    },
    {
        "context": "If you want to load a specific Flair model, you can click `Use in Flair` in the model card and you will be given a working snippet!\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1-dark.png\"/>\n</div>\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2-dark.png\"/>\n</div>\n\n## Additional resources\n\n* Flair [repository](https://github.com/flairNLP/flair)\n* Flair [docs](https://flairnlp.github.io/docs/intro)\n* Official Flair [models](https://huggingface.co/flair) on the Hub (mainly trained by [@alanakbik](https://huggingface.co/alanakbik) and [@stefan-it](https://huggingface.co/stefan-it))",
        "question": "How can I load a specific Flair model?\n",
        "answer": "You can load a specific Flair model by clicking `Use in Flair` in the model card, which will give you a working snippet.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/flair.md"
    }
]