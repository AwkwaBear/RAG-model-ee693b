{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YErqpfH9jVI"
   },
   "source": [
    "# CLAUDE RAG Evaluation\n",
    "Modified Notebook Authored by:\n",
    "\n",
    "- [Anthony Gasbarro](https://github.com/AwkwaBear/RAG-model-ee693b)\n",
    "- Chris Aguilar\n",
    "- Maxwell Pauly\n",
    "- (_Original by: [Aymeric Roucher](https://huggingface.co/m-ric)_)\n",
    "\n",
    "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "\n",
    "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
    "So let's see how to evaluate our RAG system.\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    "\n",
    "For our evaluation pipeline, we will need:\n",
    "1. An evaluation dataset with question - answer couples (QA couples)\n",
    "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    "\n",
    "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
    "1. The evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
    "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ü§ñ will then perform the evaluation on this synthetic dataset.\n",
    "\n",
    "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Conda Environment Setup\n",
    "- In command line:\n",
    "    - `conda create -n <PICK SOME ENVIRONMENT NAME>`\n",
    "    - `conda install python=3.12 pytorch pytorch-cuda transformers accelerate sentence-transformers faiss-gpu openpyxl python-dotenv -c pytorch -c nvidia -c conda-forge -y`\n",
    "- Create open this notebook in VScode and set the jupyter interpreter to `<PICK SOME ENVIRONMENT NAME>`\n",
    "\n",
    "## Run the pip install below for the rest of the required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCKBvOcp9jVK"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille ipywidgets jupyter plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_lJFbYm9jVL"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm.auto import tqdm # importing a library for progress bars\n",
    "import pandas as pd # panas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "from huggingface_hub import notebook_login \n",
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from huggingface_hub import InferenceClient\n",
    "import random\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import glob\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# print(f\"Hugging Face API Token: {os.getenv(\"HF_API_TOKEN\")} \")\n",
    "# print(f\"OpenAI API Key: {os.getenv(\"OPENAI_API_KEY\")} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oIlNZ1Mn9jVL"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm # importing a library for progress bars\n",
    "import pandas as pd # panas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) # setting the maximum column width for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32a63bdede444d7a512032f5881e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeW8P62J9jVM"
   },
   "source": [
    "### Load your knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YRbm5tNF9jVM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset (Original)\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section that saves the loaded text and source data into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./huggingface_doc_dataset.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "ds = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "# Convert the dataset to a list of dictionaries\n",
    "data_list = [doc for doc in ds]\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = \"./huggingface_doc_dataset.json\"\n",
    "\n",
    "# Write the dataset content to a JSON file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy9CKj0M9jVM"
   },
   "source": [
    "# 1. Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
    "\n",
    "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkoEgiDg9jVM"
   },
   "source": [
    "### 1.1. Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3gTOlRKO9jVM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:00<00:00, 56007.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section to save the processed documents into a file and load for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents saved to ./processed_documents.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define output file paths\n",
    "output_json_file_path = \"./processed_documents.json\"\n",
    "\n",
    "# Write the processed documents to a JSON file\n",
    "with open(output_json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"text\": doc.page_content, \"source\": doc.metadata[\"source\"]} for doc in docs_processed], f)\n",
    "\n",
    "print(f\"Processed documents saved to {output_json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjrNhcCh9jVN"
   },
   "source": [
    "### 1.2. Setup agents for question generation\n",
    "\n",
    "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GoRySj3Q9jVN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hIM_DJRo9jVN"
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVFc-lVy9jVN"
   },
   "source": [
    "Now let's generate our QA couples.\n",
    "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
    "\n",
    "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8fteqDDD9jVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 250 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [36:14<00:00,  8.70s/it] \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 250 # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(\n",
    "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    )\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving initial generated QA documents to a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA couples saved to ./initial_generated_qa_couples.json\n"
     ]
    }
   ],
   "source": [
    "# Save outputs to a JSON file\n",
    "output_file_path = './initial_generated_qa_couples.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Generated QA couples saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint section for Generated QA couples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?</td>\n",
       "      <td>How much RAM does a 30B parameters model require in 4bit?\\n</td>\n",
       "      <td>A 30B parameters model requires around 16G of RAM in 4bit.</td>\n",
       "      <td>huggingface/blog/blob/main/2023-in-llms.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             context  \\\n",
       "0  So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?   \n",
       "\n",
       "                                                      question  \\\n",
       "0  How much RAM does a 30B parameters model require in 4bit?\\n   \n",
       "\n",
       "                                                       answer  \\\n",
       "0  A 30B parameters model requires around 16G of RAM in 4bit.   \n",
       "\n",
       "                                   source_doc  \n",
       "0  huggingface/blog/blob/main/2023-in-llms.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the JSON file\n",
    "input_file_path = './initial_generated_qa_couples.json'\n",
    "\n",
    "# Load the outputs from the JSON file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    outputs = json.load(json_file)\n",
    "\n",
    "# Convert the loaded data into a pandas DataFrame\n",
    "qa_df = pd.DataFrame(outputs)\n",
    "\n",
    "# Display the first entry of the DataFrame to check if it loaded correctly\n",
    "display(qa_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aUlOUDv59jVN",
    "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?</td>\n",
       "      <td>How much RAM does a 30B parameters model require in 4bit?\\n</td>\n",
       "      <td>A 30B parameters model requires around 16G of RAM in 4bit.</td>\n",
       "      <td>huggingface/blog/blob/main/2023-in-llms.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             context  \\\n",
       "0  So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?   \n",
       "\n",
       "                                                      question  \\\n",
       "0  How much RAM does a 30B parameters model require in 4bit?\\n   \n",
       "\n",
       "                                                       answer  \\\n",
       "0  A 30B parameters model requires around 16G of RAM in 4bit.   \n",
       "\n",
       "                                   source_doc  \n",
       "0  huggingface/blog/blob/main/2023-in-llms.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KG4dNtg9jVN"
   },
   "source": [
    "### 1.3. Setup critique agents\n",
    "\n",
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
    "\n",
    "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "- **Groundedness:** can the question be answered from the given context?\n",
    "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
    "\n",
    "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
    "We also build a critique agent for this criteria:\n",
    "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
    "\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    "\n",
    "üí° ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
    "\n",
    "We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "05aSgTGs9jVO"
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "b9tbk7ME9jVO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 189/189 [1:57:48<00:00, 37.40s/it]  \n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output[\"context\"], question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            \n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect and save evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>groundedness_eval</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_eval</th>\n",
       "      <th>standalone_score</th>\n",
       "      <th>standalone_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?</td>\n",
       "      <td>How much RAM does a 30B parameters model require in 4bit?\\n</td>\n",
       "      <td>A 30B parameters model requires around 16G of RAM in 4bit.</td>\n",
       "      <td>huggingface/blog/blob/main/2023-in-llms.md</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context provides some information about the relationship between model precision and memory requirements, but it does not provide specific information about the memory requirements for a 30B parameters model in 4bit. The estimate provided is based on a general trend and may not be accurate.\\n\\n</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This question is specific to the Hugging Face ecosystem, as it asks about the memory requirements of a model with a specific number of parameters when using 4-bit quantization. It is also relevant to NLP developers who are working with large models and need to optimize their memory usage.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is context-independent, as it refers to a generic model with a specific number of parameters and a specific quantization format.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip download of urls in postprcess.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6289](https://github.com/gradio-app/gradio/pull/6289) [`5668036ee`](https://github.com/gradio-app/gradio/commit/5668036eef89051c1dbc5a74dc20988a3012ccbd) - Fix file upload on windows.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6290](https://github.com/gradio-app/gradio/pull/6290) [`e8216be94`](https://github.com/gradio-app/gradio/commit/e8216be948f76ce064595183d11e9148badf9421) - ensure `gr.Dataframe` updates as expected.  Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 4.1.0\\n\\n### Features</td>\n",
       "      <td>Which user contributed to the fix of file upload on windows in gradio version 4.1.1?\\n</td>\n",
       "      <td>Freddy Aboulton</td>\n",
       "      <td>gradio-app/gradio/blob/main/gradio/CHANGELOG.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context clearly states that there were fixes in gradio version 4.1.1, and one of the fixes was for file upload on windows. The context also clearly states that the user who contributed to this fix was @freddyaboulton. Therefore, the question is clearly and unambiguously answerable with the context.\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This question is not useful for machine learning developers building NLP applications with the Hugging Face Transformers library, as it is not related to the library or its usage. The question is about a specific contribution to the Gradio library, which is a separate library for building user interfaces for machine learning models.\\n</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question is not context-independent, as it requires knowledge of the version history of Gradio and the specific changes made in that version.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>| 4.12.3                  | TensorFlow 2.5.1           | inference | GPU    | 3.7            |\\n| 4.12.3                  | PyTorch 1.9.1              | inference | CPU    | 3.8            |\\n| 4.12.3                  | TensorFlow 2.5.1           | inference | CPU    | 3.7            |\\n| 4.12.3                  | PyTorch 1.9.1              | inference | Inferentia    | 3.7            |\\n| 4.17.0                  | PyTorch 1.10.2              | inference | GPU    | 3.8            |\\n| 4.17.0                  | TensorFlow 2.6.3           | inference | GPU    | 3.8            |\\n| 4.17.0                  | PyTorch 1.10.2              | inference | CPU    | 3.8            |\\n| 4.17.0                  | TensorFlow 2.6.3           | inference | CPU    | 3.8            |\\n| 4.26.0                  | PyTorch 1.13.1              | inference | CPU    | 3.9            |\\n| 4.26.0                  | PyTorch 1.13.1              | inference | GPU    | 3.9            |</td>\n",
       "      <td>What is the version of TensorFlow used for inference on a CPU in version 4.12.3?\\n</td>\n",
       "      <td>TensorFlow 2.5.1</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/sagemaker/reference.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a table with several rows, each row containing four pieces of information: a version number, a deep learning framework, a task, and a hardware type. The question asks for the version of TensorFlow used for inference on a CPU in version 4.12.3. The context contains two rows with version number 4.12.3, and one of them has TensorFlow as the deep learning framework and CPU as the hardware type. Therefore, the answer can be obtained unambiguously from the context.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>```py\\n&gt;&gt;&gt; ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\\n\\n\\n&gt;&gt;&gt; def preprocess_function(examples):\\n...     first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\\n...     question_headers = examples[\"sent2\"]\\n...     second_sentences = [\\n...         [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\\n...     ]\\n\\n...     first_sentences = sum(first_sentences, [])\\n...     second_sentences = sum(second_sentences, [])\\n\\n...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\\n...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\\n```\\n\\nTo apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\\n\\n```py\\ntokenized_swag = swag.map(preprocess_function, batched=True)\\n```\\n\\nü§ó Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\\n\\n`DataCollatorForMultipleChoice` flattens all the model inputs, applies padding, and then unflattens the results:\\n\\n&lt;frameworkcontent&gt;\\n&lt;pt&gt;\\n```py\\n&gt;&gt;&gt; from dataclasses import dataclass\\n&gt;&gt;&gt; from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\\n&gt;&gt;&gt; from typing import Optional, Union\\n&gt;&gt;&gt; import torch\\n\\n\\n&gt;&gt;&gt; @dataclass\\n... class DataCollatorForMultipleChoice:\\n...     \"\"\"\\n...     Data collator that will dynamically pad the inputs for multiple choice received.\\n...     \"\"\"</td>\n",
       "      <td>What is the name of the dataclass in the DataCollatorForMultipleChoice?\\n</td>\n",
       "      <td>DataCollatorForMultipleChoice</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The context does not provide the name of the dataclass in the DataCollatorForMultipleChoice.\\n\\n</td>\n",
       "      <td>3.0</td>\n",
       "      <td>This question is asking about a specific class in the Hugging Face Transformers library, DataCollatorForMultipleChoice, and its dataclass. This information can be useful for developers who are working with this class and want to understand its structure and properties. However, the question is quite specific and may not be relevant to all developers working with NLP and Hugging Face.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question asks for the name of a specific dataclass, which is DataCollatorWithPadding. The question is context-independant, since the name of the dataclass is explicitly mentioned.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# WavLM\\n\\n## Overview\\n\\nThe WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\\nMichael Zeng, Furu Wei.\\n\\nThe abstract from the paper is the following:</td>\n",
       "      <td>What is the architecture of the WavLM model?\\n</td>\n",
       "      <td>The WavLM model is built on top of the Transformer architecture.</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/wavlm.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a clear and unambiguous answer to the question, as it directly states that the WavLM model is a large-scale self-supervised pre-training model for full stack speech processing. The context also provides a citation to the paper where the model was proposed.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about the architecture of the WavLM model, which is a popular model for speech processing tasks. Knowing the architecture of a model can be very useful for developers who want to understand how the model works, how to fine-tune it, or how to implement it in their own applications. Therefore, I would rate this question as a 5, since it is extremely useful for machine learning developers working with NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question asks about the architecture of the WavLM model, which is a well-defined technical term in the field of machine learning and deep learning. The answer provides a clear and concise description of the architecture of the WavLM model, including its use of Transformers and convolutions, and its relationship to the wav2vec 2.0 model. The answer also provides a citation to the original paper where the model was introduced. Therefore, I would rate this question as a 5, since it is highly context-independent and the answer provides all the necessary information for an operator to understand the architecture of the WavLM model.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \"Linked Models\" button should appear in the top right corner of the Space, as shown here: \\n\\n    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\\n    \\n    *Note*:  You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference).\\n\\n\\nYour Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ü§ó</td>\n",
       "      <td>How can I add a linked model to my Space?\\n</td>\n",
       "      <td>You can add a linked model to your Space by updating them manually in the README metadata for the Space, as described in the Hugging Face documentation. Alternatively, Hugging Face will automatically detect and link a model to your Space as soon as it is built.</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/spaces-add-to-arxiv.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a clear and unambiguous answer to the question of how to add a linked model to a Space. The instructions are straightforward and easy to follow.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Timeline of popular Transformer model releases:\\n\\n&lt;figure class=\"image table text-center m-0 w-full\"&gt;\\n  &lt;medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Transformer model timeline\" src=\"assets/52_bert_101/transformers-timeline.png\"&gt;&lt;/medium-zoom&gt;\\n  &lt;figcaption&gt;&lt;a href=\"https://huggingface.co/course/chapter1/4\"&gt;Source&lt;/a&gt;&lt;/figcaption&gt;\\n&lt;/figure&gt;\\n\\n#### 2.4.1 How do Transformers work?\\n\\nTransformers work by leveraging attention, a powerful deep-learning algorithm, first seen in computer vision models.\\n\\n‚ÄîNot all that different from how we humans process information through attention. We are incredibly good at forgetting/ignoring mundane daily inputs that don‚Äôt pose a threat or require a response from us. For example, can you remember everything you saw and heard coming home last Tuesday? Of course not! Our brain‚Äôs memory is limited and valuable. Our recall is aided by our ability to forget trivial inputs. \\n\\nSimilarly, Machine Learning models need to learn how to pay attention only to the things that matter and not waste computational resources processing irrelevant information. Transformers create differential weights signaling which words in a sentence are the most critical to further process.\\n\\n&lt;figure class=\"image table text-center m-0 w-full\"&gt;\\n  &lt;medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Encoder and Decoder\" src=\"assets/52_bert_101/encoder-and-decoder-transformers-blocks.png\"&gt;&lt;/medium-zoom&gt;\\n&lt;/figure&gt;\\n\\nA transformer does this by successively processing an input through a stack of transformer layers, usually called the encoder. If necessary, another stack of transformer layers - the decoder - can be used to predict a target output. ‚ÄîBERT however, doesn‚Äôt use a decoder. Transformers are uniquely suited for unsupervised learning because they can efficiently process millions of data points.</td>\n",
       "      <td>How does the Transformer model process information?\\n</td>\n",
       "      <td>The Transformer model processes information by using attention, a deep Learning algorithm that helps it focus on the most critical parts of the input. It does this by creating differential weights signaling which words in a sentence are the most important to further process.</td>\n",
       "      <td>huggingface/blog/blob/main/bert-101.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a timeline of popular Transformer model releases, including BERT, which doesn't use a decoder. The Transformer model's attention mechanism is explained, which is the core of how Transformers process information. The explanation is clear and concise, providing a good understanding of how Transformers process information.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\\nThis question is useful for understanding the fundamental principles of the Transformer model and how it differs from traditional RNNs. The answer provides a clear explanation of the self-attention mechanism and how it allows the model to capture long-range dependencies and relationships between words in a sentence. The answer also highlights the advantages of the self-attention mechanism over traditional RNNs, making it a valuable resource for those looking to understand the Transformer model and its applications.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Getting Started with Repositories\\n\\nThis beginner-friendly guide will help you get the basic skills you need to create and manage your repository on the Hub. Each section builds on the previous one, so feel free to choose where to start!\\n\\n## Requirements\\n\\nThis document shows how to handle repositories through the web interface as well as through the terminal. There are no requirements if working with the UI. If you want to work with the terminal, please follow these installation instructions.\\n\\nIf you do not have `git` available as a CLI command yet, you will need to [install Git](https://git-scm.com/downloads) for your platform. You will also need to [install Git LFS](https://git-lfs.github.com/), which will be used to handle large files such as images and model weights.\\n\\nTo be able to push your code to the Hub, you'll need to authenticate somehow. The easiest way to do this is by installing the [`huggingface_hub` CLI](https://huggingface.co/docs/huggingface_hub/index) and running the login command:\\n\\n```bash\\npython -m pip install huggingface_hub\\nhuggingface-cli login\\n```\\n\\n**The content in the Getting Started section of this document is also available as a video!**\\n\\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/rkCly_cbMBk\" title=\"Managing a repo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\\n\\n## Creating a repository\\n\\nUsing the Hub's web interface you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation.\\n\\n1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):</td>\n",
       "      <td>How do I access my new repository on the Hugging Face Hub?\\n</td>\n",
       "      <td>After creating a repository on the Hugging Face Hub, you can access it by visiting its page on the Hugging Face Hub.</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>&lt;!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spaces\\n    URL: https://paperswithcode.com/paper/designing-network-design-spaces\\nModels:\\n- Name: regnety_002\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 255754236\\n    Parameters: 3160000\\n    File Size: 12782926\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_002\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L409\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70.28%\\n      Top 5 Accuracy: 89.55%\\n- Name: regnety_004\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 515664568\\n    Parameters: 4340000\\n    File Size: 17542753\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic</td>\n",
       "      <td>What is the FLOPs of regnety_004?\\n</td>\n",
       "      <td>The FLOPs of regnety_004 is 515664568.</td>\n",
       "      <td>huggingface/pytorch-image-models/blob/main/docs/models/regnety.md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>- [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initialized with\\n  any convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models).\\n  Initializing with a MobileNet backbone for example can be done by setting the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`, and then initializing the model with that\\n  config.\\n- DETR resizes the input images such that the shortest side is at least a certain amount of pixels while the longest is\\n  at most 1333 pixels. At training time, scale augmentation is used such that the shortest side is randomly set to at\\n  least 480 and at most 800 pixels. At inference time, the shortest side is set to 800. One can use\\n  [`~transformers.DetrImageProcessor`] to prepare images (and optional annotations in COCO format) for the\\n  model. Due to this resizing, images in a batch can have different sizes. DETR solves this by padding images up to the\\n  largest size in a batch, and by creating a pixel mask that indicates which pixels are real/which are padding.\\n  Alternatively, one can also define a custom `collate_fn` in order to batch images together, using\\n  [`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`].\\n- The size of the images will determine the amount of memory being used, and will thus determine the `batch_size`.\\n  It is advised to use a batch size of 2 per GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150) for more info.</td>\n",
       "      <td>How to initialize a DETR model with a MobileNet backbone?\\n</td>\n",
       "      <td>To initialize a DETR model with a MobileNet backbone, one can set the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`. Then, one can initialize\\n  the model with that config.</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        context  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\\n\\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.\\n\\nThere are many ways to go from one precision to another, with many different \"translation\" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.\\n\\n\\n## What's next?\\nThe year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip download of urls in postprcess.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6289](https://github.com/gradio-app/gradio/pull/6289) [`5668036ee`](https://github.com/gradio-app/gradio/commit/5668036eef89051c1dbc5a74dc20988a3012ccbd) - Fix file upload on windows.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6290](https://github.com/gradio-app/gradio/pull/6290) [`e8216be94`](https://github.com/gradio-app/gradio/commit/e8216be948f76ce064595183d11e9148badf9421) - ensure `gr.Dataframe` updates as expected.  Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 4.1.0\\n\\n### Features   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 4.12.3                  | TensorFlow 2.5.1           | inference | GPU    | 3.7            |\\n| 4.12.3                  | PyTorch 1.9.1              | inference | CPU    | 3.8            |\\n| 4.12.3                  | TensorFlow 2.5.1           | inference | CPU    | 3.7            |\\n| 4.12.3                  | PyTorch 1.9.1              | inference | Inferentia    | 3.7            |\\n| 4.17.0                  | PyTorch 1.10.2              | inference | GPU    | 3.8            |\\n| 4.17.0                  | TensorFlow 2.6.3           | inference | GPU    | 3.8            |\\n| 4.17.0                  | PyTorch 1.10.2              | inference | CPU    | 3.8            |\\n| 4.17.0                  | TensorFlow 2.6.3           | inference | CPU    | 3.8            |\\n| 4.26.0                  | PyTorch 1.13.1              | inference | CPU    | 3.9            |\\n| 4.26.0                  | PyTorch 1.13.1              | inference | GPU    | 3.9            |   \n",
       "3                                                                                                                                                                                     ```py\\n>>> ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\\n\\n\\n>>> def preprocess_function(examples):\\n...     first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\\n...     question_headers = examples[\"sent2\"]\\n...     second_sentences = [\\n...         [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\\n...     ]\\n\\n...     first_sentences = sum(first_sentences, [])\\n...     second_sentences = sum(second_sentences, [])\\n\\n...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\\n...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\\n```\\n\\nTo apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\\n\\n```py\\ntokenized_swag = swag.map(preprocess_function, batched=True)\\n```\\n\\nü§ó Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\\n\\n`DataCollatorForMultipleChoice` flattens all the model inputs, applies padding, and then unflattens the results:\\n\\n<frameworkcontent>\\n<pt>\\n```py\\n>>> from dataclasses import dataclass\\n>>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\\n>>> from typing import Optional, Union\\n>>> import torch\\n\\n\\n>>> @dataclass\\n... class DataCollatorForMultipleChoice:\\n...     \"\"\"\\n...     Data collator that will dynamically pad the inputs for multiple choice received.\\n...     \"\"\"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     !--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# WavLM\\n\\n## Overview\\n\\nThe WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\\nMichael Zeng, Furu Wei.\\n\\nThe abstract from the paper is the following:   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \"Linked Models\" button should appear in the top right corner of the Space, as shown here: \\n\\n    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\\n    \\n    *Note*:  You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference).\\n\\n\\nYour Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ü§ó   \n",
       "185                                                                                                                                                                                                     Timeline of popular Transformer model releases:\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Transformer model timeline\" src=\"assets/52_bert_101/transformers-timeline.png\"></medium-zoom>\\n  <figcaption><a href=\"https://huggingface.co/course/chapter1/4\">Source</a></figcaption>\\n</figure>\\n\\n#### 2.4.1 How do Transformers work?\\n\\nTransformers work by leveraging attention, a powerful deep-learning algorithm, first seen in computer vision models.\\n\\n‚ÄîNot all that different from how we humans process information through attention. We are incredibly good at forgetting/ignoring mundane daily inputs that don‚Äôt pose a threat or require a response from us. For example, can you remember everything you saw and heard coming home last Tuesday? Of course not! Our brain‚Äôs memory is limited and valuable. Our recall is aided by our ability to forget trivial inputs. \\n\\nSimilarly, Machine Learning models need to learn how to pay attention only to the things that matter and not waste computational resources processing irrelevant information. Transformers create differential weights signaling which words in a sentence are the most critical to further process.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Encoder and Decoder\" src=\"assets/52_bert_101/encoder-and-decoder-transformers-blocks.png\"></medium-zoom>\\n</figure>\\n\\nA transformer does this by successively processing an input through a stack of transformer layers, usually called the encoder. If necessary, another stack of transformer layers - the decoder - can be used to predict a target output. ‚ÄîBERT however, doesn‚Äôt use a decoder. Transformers are uniquely suited for unsupervised learning because they can efficiently process millions of data points.   \n",
       "186                                                            Getting Started with Repositories\\n\\nThis beginner-friendly guide will help you get the basic skills you need to create and manage your repository on the Hub. Each section builds on the previous one, so feel free to choose where to start!\\n\\n## Requirements\\n\\nThis document shows how to handle repositories through the web interface as well as through the terminal. There are no requirements if working with the UI. If you want to work with the terminal, please follow these installation instructions.\\n\\nIf you do not have `git` available as a CLI command yet, you will need to [install Git](https://git-scm.com/downloads) for your platform. You will also need to [install Git LFS](https://git-lfs.github.com/), which will be used to handle large files such as images and model weights.\\n\\nTo be able to push your code to the Hub, you'll need to authenticate somehow. The easiest way to do this is by installing the [`huggingface_hub` CLI](https://huggingface.co/docs/huggingface_hub/index) and running the login command:\\n\\n```bash\\npython -m pip install huggingface_hub\\nhuggingface-cli login\\n```\\n\\n**The content in the Getting Started section of this document is also available as a video!**\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/rkCly_cbMBk\" title=\"Managing a repo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n\\n## Creating a repository\\n\\nUsing the Hub's web interface you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation.\\n\\n1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):   \n",
       "187  <!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spaces\\n    URL: https://paperswithcode.com/paper/designing-network-design-spaces\\nModels:\\n- Name: regnety_002\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 255754236\\n    Parameters: 3160000\\n    File Size: 12782926\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_002\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L409\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70.28%\\n      Top 5 Accuracy: 89.55%\\n- Name: regnety_004\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 515664568\\n    Parameters: 4340000\\n    File Size: 17542753\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic   \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        - [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initialized with\\n  any convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models).\\n  Initializing with a MobileNet backbone for example can be done by setting the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`, and then initializing the model with that\\n  config.\\n- DETR resizes the input images such that the shortest side is at least a certain amount of pixels while the longest is\\n  at most 1333 pixels. At training time, scale augmentation is used such that the shortest side is randomly set to at\\n  least 480 and at most 800 pixels. At inference time, the shortest side is set to 800. One can use\\n  [`~transformers.DetrImageProcessor`] to prepare images (and optional annotations in COCO format) for the\\n  model. Due to this resizing, images in a batch can have different sizes. DETR solves this by padding images up to the\\n  largest size in a batch, and by creating a pixel mask that indicates which pixels are real/which are padding.\\n  Alternatively, one can also define a custom `collate_fn` in order to batch images together, using\\n  [`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`].\\n- The size of the images will determine the amount of memory being used, and will thus determine the `batch_size`.\\n  It is advised to use a batch size of 2 per GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150) for more info.   \n",
       "\n",
       "                                                                                   question  \\\n",
       "0                               How much RAM does a 30B parameters model require in 4bit?\\n   \n",
       "1    Which user contributed to the fix of file upload on windows in gradio version 4.1.1?\\n   \n",
       "2        What is the version of TensorFlow used for inference on a CPU in version 4.12.3?\\n   \n",
       "3                 What is the name of the dataclass in the DataCollatorForMultipleChoice?\\n   \n",
       "4                                            What is the architecture of the WavLM model?\\n   \n",
       "..                                                                                      ...   \n",
       "184                                             How can I add a linked model to my Space?\\n   \n",
       "185                                   How does the Transformer model process information?\\n   \n",
       "186                            How do I access my new repository on the Hugging Face Hub?\\n   \n",
       "187                                                     What is the FLOPs of regnety_004?\\n   \n",
       "188                             How to initialize a DETR model with a MobileNet backbone?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                  answer  \\\n",
       "0                                                                                                                                                                                                                             A 30B parameters model requires around 16G of RAM in 4bit.   \n",
       "1                                                                                                                                                                                                                                                                        Freddy Aboulton   \n",
       "2                                                                                                                                                                                                                                                                       TensorFlow 2.5.1   \n",
       "3                                                                                                                                                                                                                                                          DataCollatorForMultipleChoice   \n",
       "4                                                                                                                                                                                                                       The WavLM model is built on top of the Transformer architecture.   \n",
       "..                                                                                                                                                                                                                                                                                   ...   \n",
       "184                You can add a linked model to your Space by updating them manually in the README metadata for the Space, as described in the Hugging Face documentation. Alternatively, Hugging Face will automatically detect and link a model to your Space as soon as it is built.   \n",
       "185  The Transformer model processes information by using attention, a deep Learning algorithm that helps it focus on the most critical parts of the input. It does this by creating differential weights signaling which words in a sentence are the most important to further process.   \n",
       "186                                                                                                                                                                 After creating a repository on the Hugging Face Hub, you can access it by visiting its page on the Hugging Face Hub.   \n",
       "187                                                                                                                                                                                                                                               The FLOPs of regnety_004 is 515664568.   \n",
       "188                                                               To initialize a DETR model with a MobileNet backbone, one can set the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`. Then, one can initialize\\n  the model with that config.   \n",
       "\n",
       "                                                                     source_doc  \\\n",
       "0                                    huggingface/blog/blob/main/2023-in-llms.md   \n",
       "1                               gradio-app/gradio/blob/main/gradio/CHANGELOG.md   \n",
       "2                    huggingface/hub-docs/blob/main/docs/sagemaker/reference.md   \n",
       "3    huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md   \n",
       "4          huggingface/transformers/blob/main/docs/source/en/model_doc/wavlm.md   \n",
       "..                                                                          ...   \n",
       "184              huggingface/hub-docs/blob/main/docs/hub/spaces-add-to-arxiv.md   \n",
       "185                                      huggingface/blog/blob/main/bert-101.md   \n",
       "186     huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md   \n",
       "187           huggingface/pytorch-image-models/blob/main/docs/models/regnety.md   \n",
       "188         huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md   \n",
       "\n",
       "     groundedness_score  \\\n",
       "0                   2.0   \n",
       "1                   5.0   \n",
       "2                   5.0   \n",
       "3                   1.0   \n",
       "4                   5.0   \n",
       "..                  ...   \n",
       "184                 5.0   \n",
       "185                 5.0   \n",
       "186                 NaN   \n",
       "187                 NaN   \n",
       "188                 NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            groundedness_eval  \\\n",
       "0                                                                                                                                                                                                 The context provides some information about the relationship between model precision and memory requirements, but it does not provide specific information about the memory requirements for a 30B parameters model in 4bit. The estimate provided is based on a general trend and may not be accurate.\\n\\n   \n",
       "1                                                                                                                                                                                         The context clearly states that there were fixes in gradio version 4.1.1, and one of the fixes was for file upload on windows. The context also clearly states that the user who contributed to this fix was @freddyaboulton. Therefore, the question is clearly and unambiguously answerable with the context.\\n\\n   \n",
       "2    The context provides a table with several rows, each row containing four pieces of information: a version number, a deep learning framework, a task, and a hardware type. The question asks for the version of TensorFlow used for inference on a CPU in version 4.12.3. The context contains two rows with version number 4.12.3, and one of them has TensorFlow as the deep learning framework and CPU as the hardware type. Therefore, the answer can be obtained unambiguously from the context.\\n\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                            The context does not provide the name of the dataclass in the DataCollatorForMultipleChoice.\\n\\n   \n",
       "4                                                                                                                                                                                                                   The context provides a clear and unambiguous answer to the question, as it directly states that the WavLM model is a large-scale self-supervised pre-training model for full stack speech processing. The context also provides a citation to the paper where the model was proposed.\\n\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "184                                                                                                                                                                                                                                                                                                                                 The context provides a clear and unambiguous answer to the question of how to add a linked model to a Space. The instructions are straightforward and easy to follow.\\n\\n   \n",
       "185                                                                                                                                                The context provides a timeline of popular Transformer model releases, including BERT, which doesn't use a decoder. The Transformer model's attention mechanism is explained, which is the core of how Transformers process information. The explanation is clear and concise, providing a good understanding of how Transformers process information.\\n\\n   \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "187                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "\n",
       "     relevance_score  \\\n",
       "0                4.0   \n",
       "1                1.0   \n",
       "2                NaN   \n",
       "3                3.0   \n",
       "4                5.0   \n",
       "..               ...   \n",
       "184              NaN   \n",
       "185              5.0   \n",
       "186              NaN   \n",
       "187              NaN   \n",
       "188              NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     relevance_eval  \\\n",
       "0                                                                                                                                                                                                                                             This question is specific to the Hugging Face ecosystem, as it asks about the memory requirements of a model with a specific number of parameters when using 4-bit quantization. It is also relevant to NLP developers who are working with large models and need to optimize their memory usage.\\n\\n   \n",
       "1                                                                                                                                                                                                  This question is not useful for machine learning developers building NLP applications with the Hugging Face Transformers library, as it is not related to the library or its usage. The question is about a specific contribution to the Gradio library, which is a separate library for building user interfaces for machine learning models.\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN   \n",
       "3                                                                                                                                              This question is asking about a specific class in the Hugging Face Transformers library, DataCollatorForMultipleChoice, and its dataclass. This information can be useful for developers who are working with this class and want to understand its structure and properties. However, the question is quite specific and may not be relevant to all developers working with NLP and Hugging Face.\\n   \n",
       "4                                                                                      The question is asking about the architecture of the WavLM model, which is a popular model for speech processing tasks. Knowing the architecture of a model can be very useful for developers who want to understand how the model works, how to fine-tune it, or how to implement it in their own applications. Therefore, I would rate this question as a 5, since it is extremely useful for machine learning developers working with NLP applications.\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NaN   \n",
       "185  \\nThis question is useful for understanding the fundamental principles of the Transformer model and how it differs from traditional RNNs. The answer provides a clear explanation of the self-attention mechanism and how it allows the model to capture long-range dependencies and relationships between words in a sentence. The answer also highlights the advantages of the self-attention mechanism over traditional RNNs, making it a valuable resource for those looking to understand the Transformer model and its applications.\\n\\n   \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NaN   \n",
       "187                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NaN   \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NaN   \n",
       "\n",
       "     standalone_score  \\\n",
       "0                 5.0   \n",
       "1                 2.0   \n",
       "2                 NaN   \n",
       "3                 5.0   \n",
       "4                 5.0   \n",
       "..                ...   \n",
       "184               NaN   \n",
       "185               NaN   \n",
       "186               NaN   \n",
       "187               NaN   \n",
       "188               NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        standalone_eval  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The question is context-independent, as it refers to a generic model with a specific number of parameters and a specific quantization format.\\n\\n  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The question is not context-independent, as it requires knowledge of the version history of Gradio and the specific changes made in that version.\\n\\n  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The question asks for the name of a specific dataclass, which is DataCollatorWithPadding. The question is context-independant, since the name of the dataclass is explicitly mentioned.\\n\\n  \n",
       "4    The question asks about the architecture of the WavLM model, which is a well-defined technical term in the field of machine learning and deep learning. The answer provides a clear and concise description of the architecture of the WavLM model, including its use of Transformers and convolutions, and its relationship to the wav2vec 2.0 model. The answer also provides a citation to the original paper where the model was introduced. Therefore, I would rate this question as a 5, since it is highly context-independent and the answer provides all the necessary information for an operator to understand the architecture of the WavLM model.\\n\\n  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...  \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN  \n",
       "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN  \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN  \n",
       "187                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN  \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN  \n",
       "\n",
       "[189 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(len(outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Evaluations saved to ./updated_initial_QAs_evals.json\n"
     ]
    }
   ],
   "source": [
    "# Save outputs to a JSON file\n",
    "output_file_path = './updated_initial_QAs_evals.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Generated QA Evaluations saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQv36Y_f9jVO"
   },
   "source": [
    "Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oBWuOu1b9jVO",
    "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much RAM does a 30B parameters model require in 4bit?\\n</td>\n",
       "      <td>A 30B parameters model requires around 16G of RAM in 4bit.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which user contributed to the fix of file upload on windows in gradio version 4.1.1?\\n</td>\n",
       "      <td>Freddy Aboulton</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the version of TensorFlow used for inference on a CPU in version 4.12.3?\\n</td>\n",
       "      <td>TensorFlow 2.5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the dataclass in the DataCollatorForMultipleChoice?\\n</td>\n",
       "      <td>DataCollatorForMultipleChoice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the architecture of the WavLM model?\\n</td>\n",
       "      <td>The WavLM model is built on top of the Transformer architecture.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>How can I add a linked model to my Space?\\n</td>\n",
       "      <td>You can add a linked model to your Space by updating them manually in the README metadata for the Space, as described in the Hugging Face documentation. Alternatively, Hugging Face will automatically detect and link a model to your Space as soon as it is built.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>How does the Transformer model process information?\\n</td>\n",
       "      <td>The Transformer model processes information by using attention, a deep Learning algorithm that helps it focus on the most critical parts of the input. It does this by creating differential weights signaling which words in a sentence are the most important to further process.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>How do I access my new repository on the Hugging Face Hub?\\n</td>\n",
       "      <td>After creating a repository on the Hugging Face Hub, you can access it by visiting its page on the Hugging Face Hub.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>What is the FLOPs of regnety_004?\\n</td>\n",
       "      <td>The FLOPs of regnety_004 is 515664568.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How to initialize a DETR model with a MobileNet backbone?\\n</td>\n",
       "      <td>To initialize a DETR model with a MobileNet backbone, one can set the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`. Then, one can initialize\\n  the model with that config.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   question  \\\n",
       "0                               How much RAM does a 30B parameters model require in 4bit?\\n   \n",
       "1    Which user contributed to the fix of file upload on windows in gradio version 4.1.1?\\n   \n",
       "2        What is the version of TensorFlow used for inference on a CPU in version 4.12.3?\\n   \n",
       "3                 What is the name of the dataclass in the DataCollatorForMultipleChoice?\\n   \n",
       "4                                            What is the architecture of the WavLM model?\\n   \n",
       "..                                                                                      ...   \n",
       "184                                             How can I add a linked model to my Space?\\n   \n",
       "185                                   How does the Transformer model process information?\\n   \n",
       "186                            How do I access my new repository on the Hugging Face Hub?\\n   \n",
       "187                                                     What is the FLOPs of regnety_004?\\n   \n",
       "188                             How to initialize a DETR model with a MobileNet backbone?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                  answer  \\\n",
       "0                                                                                                                                                                                                                             A 30B parameters model requires around 16G of RAM in 4bit.   \n",
       "1                                                                                                                                                                                                                                                                        Freddy Aboulton   \n",
       "2                                                                                                                                                                                                                                                                       TensorFlow 2.5.1   \n",
       "3                                                                                                                                                                                                                                                          DataCollatorForMultipleChoice   \n",
       "4                                                                                                                                                                                                                       The WavLM model is built on top of the Transformer architecture.   \n",
       "..                                                                                                                                                                                                                                                                                   ...   \n",
       "184                You can add a linked model to your Space by updating them manually in the README metadata for the Space, as described in the Hugging Face documentation. Alternatively, Hugging Face will automatically detect and link a model to your Space as soon as it is built.   \n",
       "185  The Transformer model processes information by using attention, a deep Learning algorithm that helps it focus on the most critical parts of the input. It does this by creating differential weights signaling which words in a sentence are the most important to further process.   \n",
       "186                                                                                                                                                                 After creating a repository on the Hugging Face Hub, you can access it by visiting its page on the Hugging Face Hub.   \n",
       "187                                                                                                                                                                                                                                               The FLOPs of regnety_004 is 515664568.   \n",
       "188                                                               To initialize a DETR model with a MobileNet backbone, one can set the `backbone` attribute of\\n  [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`. Then, one can initialize\\n  the model with that config.   \n",
       "\n",
       "     groundedness_score  relevance_score  standalone_score  \n",
       "0                   2.0              4.0               5.0  \n",
       "1                   5.0              1.0               2.0  \n",
       "2                   5.0              NaN               NaN  \n",
       "3                   1.0              3.0               5.0  \n",
       "4                   5.0              5.0               5.0  \n",
       "..                  ...              ...               ...  \n",
       "184                 5.0              NaN               NaN  \n",
       "185                 5.0              5.0               NaN  \n",
       "186                 NaN              NaN               NaN  \n",
       "187                 NaN              NaN               NaN  \n",
       "188                 NaN              NaN               NaN  \n",
       "\n",
       "[189 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the architecture of the WavLM model?\\n</td>\n",
       "      <td>The WavLM model is built on top of the Transformer architecture.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the role of positional embeddings in a LLM?\\n</td>\n",
       "      <td>Positional embeddings in a LLM help the model understand the positions of text inputs to each other by differentiating the distance between tokens. Without positional embeddings, the model would have difficulty differentiating between different sequences of the same tokens.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the name of the mechanism that can condition frozen language models to perform specific downstream tasks?\\n</td>\n",
       "      <td>Prompt tuning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How to install optimum with ONNX runtime?\\n</td>\n",
       "      <td>`pip install --upgrade-strategy eager install optimum[onnxruntime]`</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Is the JAX device type?\\n</td>\n",
       "      <td>The JAX device type is Tesla T4. You can verify this by running the following code in your `app.py` and checking the output in your Space logs:\\n\\n```Python\\nimport jax\\nprint(f\"JAX device type: {jax.devices()[0].device_kind()}\")\\n```</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                question  \\\n",
       "4                                                                         What is the architecture of the WavLM model?\\n   \n",
       "18                                                                 What is the role of positional embeddings in a LLM?\\n   \n",
       "22   What is the name of the mechanism that can condition frozen language models to perform specific downstream tasks?\\n   \n",
       "25                                                                           How to install optimum with ONNX runtime?\\n   \n",
       "172                                                                                            Is the JAX device type?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 answer  \\\n",
       "4                                                                                                                                                                                                                      The WavLM model is built on top of the Transformer architecture.   \n",
       "18   Positional embeddings in a LLM help the model understand the positions of text inputs to each other by differentiating the distance between tokens. Without positional embeddings, the model would have difficulty differentiating between different sequences of the same tokens.   \n",
       "22                                                                                                                                                                                                                                                                        Prompt tuning   \n",
       "25                                                                                                                                                                                                                  `pip install --upgrade-strategy eager install optimum[onnxruntime]`   \n",
       "172                                          The JAX device type is Tesla T4. You can verify this by running the following code in your `app.py` and checking the output in your Space logs:\\n\\n```Python\\nimport jax\\nprint(f\"JAX device type: {jax.devices()[0].device_kind()}\")\\n```   \n",
       "\n",
       "     groundedness_score  relevance_score  standalone_score  \n",
       "4                   5.0              5.0               5.0  \n",
       "18                  5.0              5.0               5.0  \n",
       "22                  5.0              5.0               4.0  \n",
       "25                  5.0              5.0               4.0  \n",
       "172                 5.0              5.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(len(generated_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaOMZyu69jVO"
   },
   "source": [
    "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "We have generated only a few QA couples here to reduce time and cost. But let's kick start the next part by loading a pre-generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Q3RRz4W79jVO"
   },
   "outputs": [],
   "source": [
    "#eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5s19uTd9jVO"
   },
   "source": [
    "# 2. Build our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-mET8Dy9jVO"
   },
   "source": [
    "### 2.1. Preprocessing documents to build our vector database\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    "\n",
    "Many options exist for text splitting:\n",
    "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    "- split after `n` words / character, but only on sentence boundaries\n",
    "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    "\n",
    "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "üí° _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "H4fhm55Q9jVO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:00<00:00, 60477.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "sz9Jw2_q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzBYfNG79jVO"
   },
   "source": [
    "### 2.2. Retriever - embeddings üóÇÔ∏è\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "üõ†Ô∏è __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "LqJlIDZR9jVO"
   },
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6y1mQJX9jVO"
   },
   "source": [
    "### 2.3. Reader - LLM üí¨\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "üõ†Ô∏è Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9PdpuWyP9jVP"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `HF_API_TOKEN = \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9SDqenld9jVP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19016/1783810473.py:7: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  READER_LLM = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QZ62CbcZ9jVP"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiygbqfT9jVP"
   },
   "source": [
    "# 3. Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    "\n",
    "To this end, __we setup a judge agent__. ‚öñÔ∏èü§ñ\n",
    "\n",
    "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
    "\n",
    "üí° _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
    "\n",
    "üí° _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "VrlMh_ZI9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Ae-3KWzK9jVP"
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `OPEN_AI_KEY = \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ia9Mvn859jVP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19016/3321759812.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXH-szLe9jVP"
   },
   "source": [
    "üöÄ Let's run the tests and evaluate answers!üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jW2nnvUT9jVQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19016/2918242049.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 30.54it/s]\n",
      "/tmp/ipykernel_19016/1514433945.py:37: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm(final_prompt)\n",
      " 20%|‚ñà‚ñà        | 1/5 [00:05<00:23,  5.93s/it]/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 33.83it/s]\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:10<00:15,  5.29s/it]/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.21it/s]\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:13<00:08,  4.29s/it]/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.86it/s]\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  4.60s/it]/home/tony/miniconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.50it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:22<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:24<00:00,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:18<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "                if rerank\n",
    "                else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tytXV5-h9jVT"
   },
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "D4YDSfmr9jVT"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "CdkXMNvS9jVT"
   },
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
    "    lambda x: int(x) if isinstance(x, str) else 1\n",
    ")\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "lgxBpid29jVT",
    "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json    0.903571\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json     0.953571\n",
       "Name: eval_score_GPT4, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['settings', 'eval_score_GPT4'], dtype='object')\n",
      "                                                                                             settings  \\\n",
      "0  ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json   \n",
      "1   ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json   \n",
      "\n",
      "   eval_score_GPT4  \n",
      "0         0.903571  \n",
      "1         0.953571  \n"
     ]
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[[\"eval_score_GPT4\"]].mean().reset_index()\n",
    "print(average_scores.columns)\n",
    "print(average_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Configuration=%{x}<br>Accuracy=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           0.9035714285714286,
           0.9535714285714286
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "./output/rag<br>Chunk Size: 200<br>Embeddings: thenlper/gte-small<br>Rerank: False<br>Reader Model: zephyr-7b-beta.json",
          "./output/rag<br>Chunk Size: 200<br>Embeddings: thenlper/gte-small<br>Rerank: True<br>Reader Model: zephyr-7b-beta.json"
         ],
         "xaxis": "x",
         "y": [
          0.9035714285714286,
          0.9535714285714286
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Accuracy"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(255,0,0)"
          ]
         ],
         "showscale": false
        },
        "font": {
         "size": 11
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Accuracy of Different RAG Configurations for 81 Filtered Q/A Pairs evaluated by gpt-4-1106-preview</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "RAG Settings"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"4b02c360-51fd-4599-8f3f-ddd1c3ade7b9\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4b02c360-51fd-4599-8f3f-ddd1c3ade7b9\")) {                    Plotly.newPlot(                        \"4b02c360-51fd-4599-8f3f-ddd1c3ade7b9\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Configuration=%{x}\\u003cbr\\u003eAccuracy=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0.9035714285714286,0.9535714285714286],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"outside\",\"x\":[\".\\u002foutput\\u002frag\\u003cbr\\u003eChunk Size: 200\\u003cbr\\u003eEmbeddings: thenlper\\u002fgte-small\\u003cbr\\u003eRerank: False\\u003cbr\\u003eReader Model: zephyr-7b-beta.json\",\".\\u002foutput\\u002frag\\u003cbr\\u003eChunk Size: 200\\u003cbr\\u003eEmbeddings: thenlper\\u002fgte-small\\u003cbr\\u003eRerank: True\\u003cbr\\u003eReader Model: zephyr-7b-beta.json\"],\"xaxis\":\"x\",\"y\":[0.9035714285714286,0.9535714285714286],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.1%}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RAG Settings\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"},\"range\":[0,1],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Accuracy\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"font\":{\"size\":11},\"width\":1000,\"height\":600,\"title\":{\"text\":\"\\u003cb\\u003eAccuracy of Different RAG Configurations for 81 Filtered Q\\u002fA Pairs evaluated by gpt-4-1106-preview\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4b02c360-51fd-4599-8f3f-ddd1c3ade7b9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Function to format the settings\n",
    "def format_settings(settings):\n",
    "    formatted = settings.replace(\"chunk:\", \"Chunk Size: \") \\\n",
    "                        .replace(\"embeddings:\", \"Embeddings: \") \\\n",
    "                        .replace(\"rerank:\", \"Rerank: \") \\\n",
    "                        .replace(\"reader-model:\", \"Reader Model: \") \\\n",
    "                        .replace(\"evaluator-model:\", \"Evaluator Model: \") \\\n",
    "                        .replace(\"~\", \"/\")  # Replace any special characters as needed\n",
    "    return formatted.replace(\"_\", \"<br>\")  # Replace underscores with line breaks for better formatting\n",
    "\n",
    "# Assuming 'average_scores' is a DataFrame that contains 'settings' and 'eval_score_GPT4'\n",
    "# Create a new column with formatted settings\n",
    "average_scores['formatted_settings'] = average_scores['settings'].apply(format_settings)\n",
    "\n",
    "# Now use 'formatted_settings' in the plot\n",
    "fig = px.bar(\n",
    "    average_scores,\n",
    "    x='formatted_settings',  # Use the formatted settings for x-axis\n",
    "    y='eval_score_GPT4',     # Use the evaluation scores for y-axis\n",
    "    labels={\n",
    "        \"eval_score_GPT4\": \"Accuracy\",  # Y-axis label\n",
    "        \"formatted_settings\": \"Configuration\",  # X-axis label\n",
    "    },\n",
    "    color='eval_score_GPT4',  # Color based on the evaluation score\n",
    "    color_continuous_scale=\"bluered\",  # Color scale\n",
    ")\n",
    "\n",
    "# Update layout settings\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 1],  # Adjusting range to [0, 1] since scores are normalized\n",
    "    title=\"<b>Accuracy of Different RAG Configurations for 81 Filtered Q/A Pairs evaluated by gpt-4-1106-preview</b>\",\n",
    "    xaxis_title=\"RAG Settings\",\n",
    "    font=dict(size=11),\n",
    ")\n",
    "\n",
    "# Add percentage suffix to the y-axis\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)  # Hide the color scale\n",
    "fig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"outside\")  # Display percentage on bars\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSPH9DYI9jVT"
   },
   "source": [
    "## Example results\n",
    "\n",
    "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
    "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
    "\n",
    "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
    "\n",
    "‚û°Ô∏è ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "RVOxatv99jVT"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "vqK0Dg2Q9jVT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "index=%{x}<br>Accuracy=%{y}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           74.6268656716418,
           86.94029850746269,
           88.4328,
           92.1642,
           94.7761
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "outside",
         "texttemplate": "%{y:.1f}",
         "type": "bar",
         "x": [
          "Baseline RAG",
          "+ Tune chunk size",
          "+ Better embeddings",
          "+ Rerank snippets",
          "+ Better reader LLM"
         ],
         "xaxis": "x",
         "y": [
          74.6268656716418,
          86.94029850746269,
          88.4328,
          92.1642,
          94.7761
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(255,0,0)"
          ]
         ],
         "showscale": false
        },
        "font": {
         "size": 15
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Accuracy of different RAG configurations</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "RAG settings"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          100
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"03852c0f-efec-4e7e-adde-84952c6a0197\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"03852c0f-efec-4e7e-adde-84952c6a0197\")) {                    Plotly.newPlot(                        \"03852c0f-efec-4e7e-adde-84952c6a0197\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"index=%{x}\\u003cbr\\u003eAccuracy=%{y}\\u003cbr\\u003ecolor=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"outside\",\"x\":[\"Baseline RAG\",\"+ Tune chunk size\",\"+ Better embeddings\",\"+ Rerank snippets\",\"+ Better reader LLM\"],\"xaxis\":\"x\",\"y\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.1f}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RAG settings\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"},\"range\":[0,100],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"font\":{\"size\":15},\"width\":1000,\"height\":600,\"title\":{\"text\":\"\\u003cb\\u003eAccuracy of different RAG configurations\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('03852c0f-efec-4e7e-adde-84952c6a0197');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPUOMWGk9jVT"
   },
   "source": [
    "\n",
    "\n",
    "As you can see, these had varying impact on performance. In particular, tuning the chunk size is both easy and very impactful.\n",
    "\n",
    "But this is our case: your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options! üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
