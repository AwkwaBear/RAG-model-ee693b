{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YErqpfH9jVI"
   },
   "source": [
    "# CLAUDE RAG Evaluation\n",
    "Modified Notebook Authored by:\n",
    "\n",
    "- [Anthony Gasbarro](https://github.com/AwkwaBear/RAG-model-ee693b)\n",
    "- Chris Aguilar\n",
    "- Maxwell Pauly\n",
    "- (_Original by: [Aymeric Roucher](https://huggingface.co/m-ric)_)\n",
    "\n",
    "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "\n",
    "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
    "So let's see how to evaluate our RAG system.\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    "\n",
    "For our evaluation pipeline, we will need:\n",
    "1. An evaluation dataset with question - answer couples (QA couples)\n",
    "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    "\n",
    "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
    "1. The evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
    "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ü§ñ will then perform the evaluation on this synthetic dataset.\n",
    "\n",
    "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Conda Environment Setup\n",
    "- In command line:\n",
    "    - `conda create -n <PICK SOME ENVIRONMENT NAME>`\n",
    "    - `conda install python=3.12 pytorch pytorch-cuda transformers accelerate sentence-transformers faiss-gpu openpyxl python-dotenv -c pytorch -c nvidia -c conda-forge -y`\n",
    "- Create open this notebook in VScode and set the jupyter interpreter to `<PICK SOME ENVIRONMENT NAME>`\n",
    "\n",
    "## Run the pip install below for the rest of the required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bCKBvOcp9jVK"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain langchain-anthropic sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille ipywidgets jupyter plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k_lJFbYm9jVL"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm.auto import tqdm # importing a library for progress bars\n",
    "import pandas as pd # pandas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "from huggingface_hub import notebook_login \n",
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from huggingface_hub import InferenceClient\n",
    "import random\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import glob\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment variables from .env file\n",
    "- Loads API keys etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will print true if the .env file is loaded, false if not\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "print(\"Will print true if the .env file is loaded, false if not\")\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# print(f\"Hugging Face API Token: {os.getenv(\"HF_API_TOKEN\")} \")\n",
    "# print(f\"OpenAI API Key: {os.getenv(\"OPENAI_API_KEY\")} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIlNZ1Mn9jVL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) # setting the maximum column width for pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Hugging Face Doc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./hf_docs_dataset/huggingface_doc_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def recreate_dataset():\n",
    "    # Log in to the Hugging Face Hub\n",
    "    hf_api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "    if not hf_api_token:\n",
    "        raise EnvironmentError(\"HF_API_TOKEN is not set in the environment.\")\n",
    "    login(hf_api_token)\n",
    "\n",
    "    # Load the dataset from Hugging Face\n",
    "    ds = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "    # Convert the dataset to a list of dictionaries\n",
    "    data_list = [doc for doc in ds]\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path_hf_doc_dataset), exist_ok=True)\n",
    "\n",
    "    # Write the dataset content to a JSON file\n",
    "    with open(path_hf_doc_dataset, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Dataset saved to {path_hf_doc_dataset}\")\n",
    "    return data_list\n",
    "\n",
    "# File path for the dataset\n",
    "path_hf_doc_dataset = \"./hf_docs_dataset/huggingface_doc_dataset.json\"\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(path_hf_doc_dataset) and os.path.getsize(path_hf_doc_dataset) > 0:\n",
    "    try:\n",
    "        with open(path_hf_doc_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "            data_list = json.load(f)\n",
    "    except (json.JSONDecodeError, OSError) as e:\n",
    "        print(f\"Error loading JSON file: {e}. Recreating dataset.\")\n",
    "        data_list = recreate_dataset()\n",
    "else:\n",
    "    data_list = recreate_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy9CKj0M9jVM"
   },
   "source": [
    "# 1. Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
    "\n",
    "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkoEgiDg9jVM"
   },
   "source": [
    "### 1.1. Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3gTOlRKO9jVM"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a17a51e9494c20b6eb067646c9d772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(data_list)\n",
    "]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section to save the processed documents into a file and load for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents saved to ./hf_docs_dataset/langchain_split_huggingface_doc_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Define output file paths\n",
    "langchain_processed_json_file_path = \"./hf_docs_dataset/langchain_split_huggingface_doc_dataset.json\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(path_hf_doc_dataset), exist_ok=True)\n",
    "\n",
    "if os.path.exists(langchain_processed_json_file_path):\n",
    "    with open(langchain_processed_json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "# Write the processed documents to a JSON file\n",
    "with open(langchain_processed_json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"text\": doc.page_content, \"source\": doc.metadata[\"source\"]} for doc in docs_processed], f)\n",
    "\n",
    "print(f\"Processed documents saved to {langchain_processed_json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjrNhcCh9jVN"
   },
   "source": [
    "### 1.2. Setup agents for question generation\n",
    "\n",
    "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GoRySj3Q9jVN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hIM_DJRo9jVN"
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVFc-lVy9jVN"
   },
   "source": [
    "Now let's generate our QA couples.\n",
    "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
    "\n",
    "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8fteqDDD9jVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 25 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [02:31<00:00,  6.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 25 # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(\n",
    "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    )\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving initial generated QA documents to a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA couples saved to ./initial_generated_qa_couples.json\n"
     ]
    }
   ],
   "source": [
    "# Save outputs to a JSON file\n",
    "output_file_path = './initial_generated_qa_couples.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Generated QA couples saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint section for Generated QA couples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=downloads&amp;search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n</td>\n",
       "      <td>The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>huggingface/blog/blob/main/sentiment-analysis-python.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "\n",
       "                                                                                          question  \\\n",
       "0  What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n   \n",
       "\n",
       "                                                                                              answer  \\\n",
       "0  The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "\n",
       "                                                source_doc  \n",
       "0  huggingface/blog/blob/main/sentiment-analysis-python.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the JSON file\n",
    "input_file_path = './initial_generated_qa_couples.json'\n",
    "\n",
    "# Load the outputs from the JSON file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    outputs = json.load(json_file)\n",
    "\n",
    "# Convert the loaded data into a pandas DataFrame\n",
    "qa_df = pd.DataFrame(outputs)\n",
    "\n",
    "# Display the first entry of the DataFrame to check if it loaded correctly\n",
    "display(qa_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aUlOUDv59jVN",
    "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=downloads&amp;search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n</td>\n",
       "      <td>The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>huggingface/blog/blob/main/sentiment-analysis-python.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "\n",
       "                                                                                          question  \\\n",
       "0  What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n   \n",
       "\n",
       "                                                                                              answer  \\\n",
       "0  The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "\n",
       "                                                source_doc  \n",
       "0  huggingface/blog/blob/main/sentiment-analysis-python.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KG4dNtg9jVN"
   },
   "source": [
    "### 1.3. Setup critique agents\n",
    "\n",
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
    "\n",
    "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "- **Groundedness:** can the question be answered from the given context?\n",
    "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
    "\n",
    "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
    "We also build a critique agent for this criteria:\n",
    "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
    "\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    "\n",
    "üí° ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
    "\n",
    "We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "05aSgTGs9jVO"
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a context and a question.\n",
      "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here are the question and context.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer::: \n",
      "\n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer::: \n",
      "\n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
      "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
      "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
      "\n",
      "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer::: \n"
     ]
    }
   ],
   "source": [
    "print(question_groundedness_critique_prompt)\n",
    "print(question_relevance_critique_prompt)\n",
    "print(question_standalone_critique_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "b9tbk7ME9jVO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 4/18 [00:19<01:16,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'relevance'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What is the number of parameters in tf_efficientnet_es?\n",
      "\n",
      "\n",
      "Answer::: \n",
      "The number of parameters in tf_efficientnet_es depends on the specific model size you choose. For example, 'tf_efficientnet_es_b0' has approximately 5.3 million parameters, while 'tf_efficientnet_es_b7' has approximately 66 million parameters.\n",
      "\n",
      "Evaluation: This question is useful for machine learning developers who are working with the tf_efficientnet_es model and need to understand the model's complexity in terms of the number of parameters. The answer provides a clear and concise explanation of how the number of parameters depends on the specific model size.\n",
      "\n",
      "Total rating: 4.5\n",
      "\n",
      "Confidence: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 7/18 [01:02<02:32, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'relevance'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What other library can I use to replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM?\n",
      "\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The question is useful for developers who want to use large models from Hugging Face, but are facing limitations with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find alternative libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are looking for alternatives to the checkpoint system in Hugging Face. The answer can help them find other libraries that support large model loading. Total rating: 4\n",
      "\n",
      "\n",
      "Answer:::\n",
      "Evaluation: The question is useful for developers who are working with large models in Hugging Face and are facing issues with the checkpoint system. The answer can help them find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 9/18 [02:38<04:27, 29.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'groundedness'. Evaluation content: \n",
      "You will be given a context and a question.\n",
      "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here are the question and context.\n",
      "\n",
      "Question: (your factoid question)\n",
      "\n",
      "\n",
      "Context: The dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \n",
      "at the end of the robust speech challenge.\n",
      "\n",
      "If there is no real audio data for your language the final score will be \n",
      "computed solely based on the Common Voice 7 test dataset. If there is also\n",
      "no Common Voice 7 test dataset for your language, we will see together how to \n",
      "score your model - if this is the case, please don't be discouraged. We are \n",
      "especially excited about speech recognition systems of such low-resource \n",
      "languages and will make sure that we'll decide on a good approach to evaluating \n",
      "your model.\n",
      "\n",
      "## Prizes\n",
      "\n",
      "TODO(Patrick, Omar, ...)\n",
      "\n",
      "## Communication and Problems\n",
      "\n",
      "If you encounter any problems or have any questions, you should use one of the following platforms\n",
      "depending on your type of problem. Hugging Face is an \"open-source-first\" organization meaning \n",
      "that we'll try to solve all problems in the most public and most transparent way possible so that everybody\n",
      "in the community profits.\n",
      "\n",
      "The following table summarizes what platform to use for which problem.\n",
      "\n",
      "Answer::: \n",
      "\n",
      "| Problem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 11/18 [03:09<02:44, 23.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'groundedness'. Evaluation content: \n",
      "You will be given a context and a question.\n",
      "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here are the question and context.\n",
      "\n",
      "Question: What is the default value for HF_HUB_\n",
      "\n",
      "Context: For more details about authentication, check out [this section](../quick-start#authentication).\n",
      "\n",
      "### HF_HUB_VERBOSITY\n",
      "\n",
      "Set the verbosity level of the `huggingface_hub`'s logger. Must be one of\n",
      "`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\n",
      "\n",
      "Defaults to `\"warning\"`.\n",
      "\n",
      "For more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\n",
      "\n",
      "### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\n",
      "\n",
      "Integer value to define under which size a file is considered as \"small\". When downloading files to a local directory,\n",
      "small files will be duplicated to ease user experience while bigger files are symlinked to save disk usage.\n",
      "\n",
      "For more details, see the [download guide](../guides/download#download-files-to-local-folder).\n",
      "\n",
      "### HF_HUB_ETAG_TIMEOUT\n",
      "\n",
      "Integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s.\n",
      "\n",
      "### HF_HUB_DOWNLOAD_TIMEOUT\n",
      "\n",
      "Integer value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s.\n",
      "\n",
      "## Boolean values\n",
      "\n",
      "The following environment variables expect a boolean value. The variable will be considered\n",
      "as `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\n",
      "(or undefined) will be considered as `False`.\n",
      "\n",
      "### HF_HUB_OFFLINE\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_USE_AUTH_TOKEN\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_USE_AUTH_TOKEN.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_USE_PROXY\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_USE_PROXY.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_PYTORCH_PREDICT_CONVERT_GRAPH\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_PYTORCH_PREDICT_CONVERT_GRAPH.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE_AND_LOAD\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE_AND_LOAD.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE_AND_LOAD_AND_SERVE\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The context does not provide a default value for HF_HUB_DISABLE_TORCH_SERVE_CONVERT_GRAPH_FOR_MODEL_MAPPING_WITH_PREDICT_MODULE_AND_EXPORT_AND_SAVE_AND_LOAD_AND_SERVE.\n",
      "Total rating: 1\n",
      "\n",
      "### HF_HUB_\n",
      "Error parsing score or evaluation for criterion 'relevance'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What is the default value for HF_HUB_\n",
      "\n",
      "Answer::: \n",
      "The default value for HF_HUB_TOKEN is None.\n",
      "\n",
      "Evaluation: This question is related to the Hugging Face ecosystem, and specifically to the handling of the HF_HUB_TOKEN environment variable. The default value for this variable is None, which is useful information for developers who are working with the Hugging Face Model Hub and want to know the default behavior of the library.\n",
      "\n",
      "Total rating: 4\n",
      "\n",
      "Note: I am a helpful, respectful, and honest community member. I am not a bot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 15/18 [04:40<01:12, 24.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'relevance'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What is the format of the annotations expected by the image_processor?\n",
      "\n",
      "\n",
      "Answer::: \n",
      "Evaluation: The question is asking about the format of the annotations expected by the `image_processor` class from the `transformers` library, which is a crucial component for handling image data in NLP applications using the Hugging Face ecosystem. Providing a clear understanding of the expected format of the annotations is essential for developers to correctly preprocess their image data and use it in conjunction with text data in their NLP models.\n",
      "Total rating: 5\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "Error parsing score or evaluation for criterion 'standalone'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
      "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
      "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
      "\n",
      "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What is the format of the annotations expected by the image_processor?\n",
      "\n",
      "\n",
      "Answer::: \n",
      "The format of the annotations expected by the image_processor is a JSON file.\n",
      "\n",
      "Evaluation: The question is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "Total rating: 5\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the image_processor, which is a technical term that refers to a specific function in the Gradio library.\n",
      "\n",
      "1. The format of the annotations expected by the image_processor is a JSON file, which is a common format for data serialization.\n",
      "\n",
      "The question is clear and self-contained, and it is asking about the format of the annotations expected by the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 16/18 [04:57<00:44, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing score or evaluation for criterion 'relevance'. Evaluation content: \n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
      "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (your rationale for the rating, as a text)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: What is the architecture of gluon\\_resnext50\\_32x4d?\n",
      "\n",
      "\n",
      "Answer::: \n",
      "The architecture of gluon\\_resnext50\\_32x4d is a type of ResNeXt model, which is a variant of the ResNet model. The ResNeXt model modifies the ResNet model by replacing the original residual block with a bottleneck block that contains a 3x3 convolution layer, a 1x1 convolution layer, and a shortcut connection. The ResNeXt model introduces a cardinality hyperparameter, which represents the number of groups for the 3x3 convolution layer in the bottleneck block. The cardinality hyperparameter allows the model to learn more diverse features and improve the model's performance.\n",
      "\n",
      "The gluon\\_resnext50\\_32x4d model has 50 layers, a width factor of 32, and a bottleneck width of 4d. The width factor determines the number of channels in the 1x1 convolution layer in the bottleneck block. The bottleneck width of 4d means that the 3x3 convolution layer has an output channel size of 4d.\n",
      "\n",
      "Evaluation:\n",
      "The question is useful for machine learning developers who want to understand the architecture of the gluon\\_resnext50\\_32x4d model and how it differs from other ResNet and ResNeXt models. The question also helps developers understand the ResNeXt model's improvements over the original ResNet model.\n",
      "\n",
      "Total rating: 4\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [05:21<00:00, 17.87s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    try:\n",
    "        evaluations = {\n",
    "            \"groundedness\": call_llm(\n",
    "                llm_client,\n",
    "                question_groundedness_critique_prompt.format(\n",
    "                    context=output[\"context\"], question=output[\"question\"]\n",
    "                ),\n",
    "            ),\n",
    "            \"relevance\": call_llm(\n",
    "                llm_client,\n",
    "                question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "            ),\n",
    "            \"standalone\": call_llm(\n",
    "                llm_client,\n",
    "                question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "            ),\n",
    "        }\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            try:\n",
    "                score, eval = (\n",
    "                    int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                    evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "                )\n",
    "                output.update(\n",
    "                    {\n",
    "                        f\"{criterion}_score\": score,\n",
    "                        f\"{criterion}_eval\": eval,\n",
    "                    }\n",
    "                )\n",
    "            except (IndexError, ValueError):\n",
    "                print(\n",
    "                    f\"Error parsing score or evaluation for criterion '{criterion}'. \"\n",
    "                    f\"Evaluation content: {evaluation}\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for output {output}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect and save evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>groundedness_eval</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_eval</th>\n",
       "      <th>standalone_score</th>\n",
       "      <th>standalone_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=downloads&amp;search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n</td>\n",
       "      <td>The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>huggingface/blog/blob/main/sentiment-analysis-python.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context clearly states that the IMDB dataset is used in the tutorial to fine-tune a DistilBERT model for sentiment analysis. The question asks for the dataset used in the tutorial for sentiment analysis, so the context directly answers the question.\\n\\n</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This question is useful for machine learning developers who are working on NLP applications using the Hugging Face ecosystem, as it helps them understand the dataset used in the tutorial and potentially reuse it for their own projects.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about a specific dataset used in a tutorial, which is a clear and specific request. The tutorial in question is the Hugging Face Space tutorial on fine-tuning a DistilBERT model for sentiment analysis, so the context is clear.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;figure class=\"image\"&gt;\\n  &lt;img alt=\"Batch scaling experiment for PyTorch and Tensorflow\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/batch_scaling_exp_throughput.svg\" style=\"width:100%\"/&gt;\\n&lt;figcaption&gt;Figure 10. Sum throughput with respect to number of instances for a total batch size of 8&lt;/figcaption&gt;\\n&lt;/figure&gt;\\n&lt;br&gt;\\n\\n## 9. Conclusion\\n\\nThrough this blog post, we covered out-of-box BERT inference performance one can expect for PyTorch and TensorFlow, \\nfrom a simple PyPi install and without further tuning.  \\nIt is important to highlight results provided here reflects out-of-the-box framework setup hence, they might not provide the absolute best performances.  \\nWe decided to not include optimizations as part of this blog post to focus on hardware and efficiency. \\nOptimizations will be discussed in the second part! üöÄ\\n\\nThen, we covered and detailed the impact, and the importance of setting the thread affinity along with the trade-off between the target problem size, and the number of cores required for achieving the task.  \\nAlso, it is important to define **which criteria**  _(i.e. latency vs throughput)_ to use when optimizing your deployment as the resulting setups might be totally different.\\n\\nOn a more general note, small problem sizes (_short sequences and/or small batches_) might require much fewer cores to achieve the best possible latency than big problems (_very long sequences and/or big batches_).\\n\\nIt is interesting to cover all these aspects when thinking about the final deployment platform as it might cut the cost of the infrastructure drastically.  \\nFor instance, our 48 cores machine charges **4.848\\$/h** whereas a smaller instances with only 8 cores lowers the cost to **0.808\\$/h**, leading to a **6x cost reduction**.</td>\n",
       "      <td>How much does a 48 cores machine cost per hour?\\n</td>\n",
       "      <td>4.848$/h</td>\n",
       "      <td>huggingface/blog/blob/main/bert-cpu-scaling-part-1.md</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The context does not provide any information about the cost of a 48 cores machine per hour. It only mentions that the cost of a 48 cores machine is 4.848$/h, but it does not specify whether this cost is for a hour of usage or for the machine itself.\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not useful at all for machine learning developers building NLP applications with the Hugging Face ecosystem.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is context-independant, as it is clear what the question is about.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|      |      |[huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b)                                                                                |12186       |117     |llama-license            |https://huggingface.co/huggyllama/llama-13b/blob/main/LICENSE                      |[LICENSE](https://huggingface.co/huggyllama/llama-13b/blob/main/LICENSE)                                                |                                                                                                    |             |\\n|      |      |[TheBloke/Llama-2-7B-fp16](https://huggingface.co/TheBloke/Llama-2-7B-fp16)                                                                        |11108       |32      | llama2 |                                                 |[LICENSE](https://huggingface.co/TheBloke/Llama-2-7B-fp16/blob/main/LICENSE)                                            |                                                                                                    |             |\\n|      |      |[beomi/llama-2-ko-7b](https://huggingface.co/beomi/llama-2-ko-7b)                                                                                  |10623       |77      | llama2 |                                                 |[LICENSE](https://huggingface.co/beomi/llama-2-ko-7b/blob/main/LICENSE)                                                 |                                                                                                    |             |</td>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>The model [huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b) is released under the llama-license.</td>\n",
       "      <td>huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The context does not provide any information about the question.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_em\\n    Crop Pct: '0.882'\\n    Image Size: '240'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1541\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_em-e78cfe58.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.71%\\n      Top 5 Accuracy: 94.33%\\n- Name: tf_efficientnet_es\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 2057577472\\n    Parameters: 5440000\\n    File Size: 22008479\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_es\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1531\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.28%\\n      Top 5 Accuracy: 93.6%\\n- Name: tf_efficientnet_l2_ns_475\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 217795669644\\n    Parameters: 480310000\\n    File Size: 1925950424\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout</td>\n",
       "      <td>What is the number of parameters in tf_efficientnet_es?\\n</td>\n",
       "      <td>5440000</td>\n",
       "      <td>huggingface/pytorch-image-models/blob/main/docs/models/tf-efficientnet.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context clearly states that the number of parameters in tf_efficientnet_es is 5440000.\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the number of parameters in a specific model, tf_efficientnet_es, which is a pre-trained model from TensorFlow. The question does not depend on any specific context or additional information, so it is context-independent.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# CANINE\\n\\n## Overview\\n\\nThe CANINE model was proposed in [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\\nRepresentation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. It's\\namong the first papers that trains a Transformer without using an explicit tokenization step (such as Byte Pair\\nEncoding (BPE), WordPiece or SentencePiece). Instead, the model is trained directly at a Unicode character-level.\\nTraining at a character-level inevitably comes with a longer sequence length, which CANINE solves with an efficient\\ndownsampling strategy, before applying a deep Transformer encoder.\\n\\nThe abstract from the paper is the following:</td>\n",
       "      <td>What corpus is CANINE pre-trained on?\\n</td>\n",
       "      <td>CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/canine.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context clearly states that the CANINE model is pre-trained on the text8 corpus.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about the corpus used for pre-training a specific model, CANINE. This information is useful for developers who want to understand the performance and applicability of the model in their NLP tasks. The C4 corpus is a widely used dataset in NLP, and knowing that CANINE is pre-trained on it can help developers compare the model with other models pre-trained on the same corpus.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about the corpus used to pre-train the CANINE model. The answer is not dependent on any specific context, as CANINE is a well-known model and its pre-training corpus is a fixed property of the model.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Very quickly we see that it's a much more coordinated approach for searching through the API, with no added headache for you!\\n\\n## What is the magic?\\n\\nVery briefly we'll talk about the underlying magic at play that gives us this enum-dictionary-like datatype, the `AttributeDictionary`.\\n\\nHeavily inspired by the `AttrDict` class from the [fastcore](https://fastcore.fast.ai/basics.html#AttrDict) library, the general idea is we take a normal dictionary and supercharge it for *exploratory programming* by providing tab-completion for every key in the dictionary. \\n\\nAs we saw earlier, this gets even stronger when we have nested dictionaries we can explore through, such as `model_args.dataset.glue`!\\n\\n&gt; For those familiar with JavaScript, we mimic how the `object` class is working.\\n\\nThis simple utility class can provide a much more user-focused experience when exploring nested datatypes and trying to understand what is there, such as the return of an API request!\\n\\nAs mentioned before, we expand on the `AttrDict` in a few key ways:\\n- You can delete keys with `del model_args[key]` *or* with `del model_args.key`\\n- That clean `__repr__` we saw earlier \\n\\nOne very important concept to note though, is that if a key contains a number or special character it **must** be indexed as a dictionary, and *not* as an object.\\n\\n```python\\n&gt;&gt;&gt; from huggingface_hub.utils.endpoint_helpers import AttributeDictionary\\n```\\n\\nA very brief example of this is if we have an `AttributeDictionary` with a key of `3_c`:\\n\\n\\n```python\\n&gt;&gt;&gt; d = {\"a\":2, \"b\":3, \"3_c\":4}\\n&gt;&gt;&gt; ad = AttributeDictionary(d)\\n```\\n\\n\\n```python\\n&gt;&gt;&gt; # As an attribute\\n&gt;&gt;&gt; ad.3_c\\n```\\n     File \"&lt;ipython-input-6-c0fe109cf75d&gt;\", line 2\\n        ad.3_c\\n            ^\\n    SyntaxError: invalid token\\n\\n```python\\n&gt;&gt;&gt; # As a dictionary key\\n&gt;&gt;&gt; ad[\"3_c\"]\\n```\\n    4\\n\\n## Concluding thoughts</td>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>A dictionary key must be used to index a key containing a number or special character in an AttributeDictionary.</td>\n",
       "      <td>huggingface/blog/blob/main/searching-the-hub.md</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The context provides information about the `AttributeDictionary` class, which is a dictionary-like datatype that provides tab-completion for every key in the dictionary. However, the context does not provide any information about how this class is used to search through an API. Therefore, it is not possible to answer the question without additional context.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You can easily run `pipeline` on large models using ü§ó `accelerate`! First make sure you have installed `accelerate` with `pip install accelerate`. \\n\\nFirst load your model using `device_map=\"auto\"`! We will use `facebook/opt-1.3b` for our example.\\n\\n```py\\n# pip install accelerate\\nimport torch\\nfrom transformers import pipeline\\n\\npipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\\n```\\n\\nYou can also pass 8-bit loaded models if you install `bitsandbytes` and add the argument `load_in_8bit=True`\\n\\n```py\\n# pip install accelerate bitsandbytes\\nimport torch\\nfrom transformers import pipeline\\n\\npipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\\n```\\n\\nNote that you can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM!</td>\n",
       "      <td>What other library can I use to replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM?\\n</td>\n",
       "      <td>You can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM by specifying it in the `pipeline` function, like so: `pipeline(model=\"your_model_name\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})`.</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a clear example of how to load a large model using the `device_map=\"auto\"` argument and the `pipeline` function from the `transformers` library. It also mentions that you can replace the checkpoint with any of the Hugging Face models that support large model loading, such as BLOOM. Therefore, the question is clearly answerable with the context.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is clear and concise, and it is not dependent on any specific context or document. The question is asking about replacing the checkpoint with a Hugging Face model that supports large model loading, such as BLOOM, which is a well-known model in the field of natural language processing.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Pawe≈Ç Krzysztof Nowak, Thomas M√ºller, Francesco Piccinno and Julian Martin Eisenschlos.\\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).\\n1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\\n1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</td>\n",
       "      <td>What is the name of the model from Microsoft Research that is used for table extraction from unstructured documents?\\n</td>\n",
       "      <td>Table Transformer</td>\n",
       "      <td>huggingface/transformers/blob/main/README_pt-br.md</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The context provides a list of models from Microsoft Research, Google AI, HuggingFace, Facebook, the University of California at Berkeley, Google/CMU, and their respective papers. However, the context does not provide the name of the model from Microsoft Research that is used for table extraction from unstructured documents.\\n</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The question is specific and clear, but it asks for a proper name, which is not provided in the context. The question is still relevant to NLP and the Hugging Face ecosystem, as it relates to a model used for a specific NLP task.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is about a specific model used for a specific task, and it is clear that the model is from Microsoft Research. The name of the model is not a common name, but it is a name that is specific to the model, and it is not a name that is used for other models. The question is clear and does not depend on any additional information.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \\nat the end of the robust speech challenge.\\n\\nIf there is no real audio data for your language the final score will be \\ncomputed solely based on the Common Voice 7 test dataset. If there is also\\nno Common Voice 7 test dataset for your language, we will see together how to \\nscore your model - if this is the case, please don't be discouraged. We are \\nespecially excited about speech recognition systems of such low-resource \\nlanguages and will make sure that we'll decide on a good approach to evaluating \\nyour model.\\n\\n## Prizes\\n\\nTODO(Patrick, Omar, ...)\\n\\n## Communication and Problems\\n\\nIf you encounter any problems or have any questions, you should use one of the following platforms\\ndepending on your type of problem. Hugging Face is an \"open-source-first\" organization meaning \\nthat we'll try to solve all problems in the most public and most transparent way possible so that everybody\\nin the community profits.\\n\\nThe following table summarizes what platform to use for which problem.</td>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>If you encounter any problems or have any questions about the Hugging Face Robust Speech Challenge, you should use the Hugging Face Discord server.</td>\n",
       "      <td>huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very personal and probably biased view:\\n\\n| Format                  | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\\n| pickle (PyTorch)        | ‚úó | ‚úó | ‚úó | üó∏ | ‚úó | üó∏ | üó∏ |\\n| H5 (Tensorflow)         | üó∏ | ‚úó | üó∏ | üó∏ | ~ | ~ | ‚úó |\\n| SavedModel (Tensorflow) | üó∏ | ‚úó | ‚úó | üó∏ | üó∏ | ‚úó | üó∏ |\\n| MsgPack (flax)          | üó∏ | üó∏ | ‚úó | üó∏ | ‚úó | ‚úó | üó∏ |\\n| Protobuf (ONNX)         | üó∏ | ‚úó | ‚úó | ‚úó | ‚úó | ‚úó | üó∏ |\\n| Cap'n'Proto             | üó∏ | üó∏ | ~ | üó∏ | üó∏ | ~ | ‚úó |\\n| Arrow                   | ? | ? | ? | ? | ? | ? | ‚úó |\\n| Numpy (npy,npz)         | üó∏ | ? | ? | ‚úó | üó∏ | ‚úó | ‚úó |\\n| pdparams (Paddle)       | ‚úó | ‚úó | ‚úó | üó∏ | ‚úó | üó∏ | üó∏ |\\n| SafeTensors             | üó∏ | üó∏ | üó∏ | üó∏ | üó∏ | ‚úó | üó∏ |\\n\\n- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\\n- Zero-copy: Does reading the file require more memory than the original file ?\\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\\nsome tensors in it without scanning the whole file (distributed setting) ?\\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -&gt; RAM copies). Controlling the layout to keep fast access to single tensors is important.\\n- No file size limit: Is there a limit to the file size ?\\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)\\n- Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are\\nnecessary)? This is becoming increasingly important in the ML world.\\n\\n\\n### Main oppositions</td>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.</td>\n",
       "      <td>huggingface/safetensors/blob/main/safetensors/README.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\\nThe question asks for the main oppositions between the different formats. The context provides a table that highlights the differences between the formats, and the answer identifies the main oppositions as being between formats that support lazy loading and layout control, and those that do not. The answer is directly responsive to the question and provides a clear and unambiguous answer.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>For more details about authentication, check out [this section](../quick-start#authentication).\\n\\n### HF_HUB_VERBOSITY\\n\\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of\\n`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\\n\\nDefaults to `\"warning\"`.\\n\\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\\n\\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\\n\\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,\\nsmall files will be duplicated to ease user experience while bigger files are symlinked to save disk usage.\\n\\nFor more details, see the [download guide](../guides/download#download-files-to-local-folder).\\n\\n### HF_HUB_ETAG_TIMEOUT\\n\\nInteger value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s.\\n\\n### HF_HUB_DOWNLOAD_TIMEOUT\\n\\nInteger value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s.\\n\\n## Boolean values\\n\\nThe following environment variables expect a boolean value. The variable will be considered\\nas `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\\n(or undefined) will be considered as `False`.\\n\\n### HF_HUB_OFFLINE</td>\n",
       "      <td>What is the default value for HF_HUB_</td>\n",
       "      <td>The default value for HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR is False.\\n\\n### HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR2\\n\\nOutput:::\\nFactoid question: What is the default value for HF_HUB_</td>\n",
       "      <td>huggingface/huggingface_hub/blob/main/docs/source/en/package_reference/environment_variables.md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the default value of a variable, which is a self-contained question.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from enum import Enum\\n```\\n\\n## Prepare Model and Tokenizer\\n\\nNow, we will be adding 27 new tokens as well as replace the existing pad, bos and eos tokens of the model.\\n\\n\\n```python\\nclass SpecialTokens(str, Enum):\\n    begin_target = \"&lt;|begintarget|&gt;\"\\n    end_target = \"&lt;|endtarget|&gt;\"\\n    begin_context = \"&lt;|begincontext|&gt;\"\\n    end_context = \"&lt;|endcontext|&gt;\"\\n    system = \"&lt;|system|&gt;\"\\n    user = \"&lt;|user|&gt;\"\\n    begin_last_user_utterance = \"&lt;|beginlastuserutterance|&gt;\"\\n    end_last_user_utterance = \"&lt;|endlastuserutterance|&gt;\"\\n    begin_dsts = \"&lt;|begindsts|&gt;\"\\n    end_dsts = \"&lt;|enddsts|&gt;\"\\n    begin_dst = \"&lt;|begindst|&gt;\"\\n    end_dst = \"&lt;|enddst|&gt;\"\\n    begin_belief = \"&lt;|beginbelief|&gt;\"\\n    end_belief = \"&lt;|endbelief|&gt;\"\\n    begin_response = \"&lt;|beginresponse|&gt;\"\\n    end_response = \"&lt;|endresponse|&gt;\"\\n    begin_action = \"&lt;|beginaction|&gt;\"\\n    end_action = \"&lt;|endaction|&gt;\"\\n    begin_user_action = \"&lt;|beginuseraction|&gt;\"\\n    end_user_action = \"&lt;|enduseraction|&gt;\"\\n    sys_actions = \"&lt;|sysactions|&gt;\"\\n    begin_intent = \"&lt;|beginintent|&gt;\"\\n    end_intent = \"&lt;|endintent|&gt;\"\\n    begin_requested_slots = \"&lt;|beginrequestedslots|&gt;\"\\n    end_requested_slots = \"&lt;|endrequestedslots|&gt;\"\\n    pad_token = \"&lt;|pad|&gt;\"\\n    bos_token = \"&lt;|startoftext|&gt;\"\\n\\n    @classmethod\\n    def list(cls):\\n        return [c.value for c in cls]\\n```\\n\\nWe will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens followed by loading the base model and resizzing the embedding layers to accomodate the newly added tokens.\\n\\n\\n```python\\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\\ntokenizer = AutoTokenizer.from_pretrained(\\n    model_name,\\n    pad_token=SpecialTokens.pad_token.value,\\n    bos_token=SpecialTokens.bos_token.value,\\n    eos_token=SpecialTokens.end_target.value,\\n    additional_special_tokens=SpecialTokens.list(),\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    low_cpu_mem_usage=True\\n    # use_flash_attention_2=True, # leading to an error\\n)\\nmodel.resize_token_embeddings(len(tokenizer))\\n```</td>\n",
       "      <td>How many new tokens are added to the model?\\n</td>\n",
       "      <td>27 new tokens are added to the model.\\n\\n\\n```python\\n\\n```</td>\n",
       "      <td>huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The context provides information about the addition of 27 new tokens and replacement of existing pad, bos and eos tokens of the model. However, it does not provide information about the number of new tokens added to the model.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is highly relevant to NLP development using the Hugging Face ecosystem, as it pertains to the tokenization process, which is a crucial step in preparing text data for input into a model. The question also implies an understanding of the tokenization process and the ability to count the number of new tokens added to a model, which suggests a high level of expertise in NLP development using the Hugging Face ecosystem.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is clear and concise, and it does not depend on any additional information to be understood. It is asking about the number of new tokens added to a model, which is a technical concept that is well-defined and easy to understand.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>- [DistilBERT ‡∞§‡±ã ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&amp;context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\\n- [T5 ‡∞§‡±ã ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)</td>\n",
       "      <td>How many square kilometers of the Amazon rainforest are covered by the basin?\\n</td>\n",
       "      <td>5,500,000 square kilometers of the Amazon rainforest are covered by the basin.</td>\n",
       "      <td>huggingface/transformers/blob/main/README_te.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a clear and unambiguous answer to the question, as it specifies the exact area covered by the Amazon rainforest in the Amazon basin.\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This question is not related to machine learning, natural language processing, or the Hugging Face ecosystem. It is a geographical question about the Amazon rainforest.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about the area of the Amazon rainforest that is covered by the basin. It is a context-independent question, as it is clear what the Amazon rainforest and the Amazon basin are referring to.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Software also plays a vital role in unlocking the IPU‚Äôs capabilities, so naturally Optimum offers a plug-and-play experience with Graphcore‚Äôs easy-to-use Poplar SDK ‚Äî which itself has received a major 2.5 update. Poplar makes it easy to train state-of-the-art models on state-of-the-art hardware, thanks to its full integration with standard machine learning frameworks, including PyTorch, PyTorch Lightning, and TensorFlow‚Äîas well as orchestration and deployment tools such as Docker and Kubernetes. Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU‚Äôs advanced AI capabilities.\\n\\n## Get started with Hugging Face‚Äôs Optimum Graphcore models\\n\\nIf you‚Äôre interested in combining the benefits of IPU technology with the strengths of transformer models, you can download the latest range of Optimum Graphcore models from the [Graphcore organization on the Hub](https://huggingface.co/Graphcore), or access the code from the [Optimum GitHub repo](https://github.com/huggingface/optimum-graphcore). Our [Getting Started blog post](https://huggingface.co/blog/graphcore-getting-started) will guide you through each step to start experimenting with IPUs.\\n\\nAdditionally, Graphcore has built an extensive page of [developer resources](https://www.graphcore.ai/developer), where you can find the IPU Model Garden‚Äîa repository of deployment-ready ML applications including computer vision, NLP, graph networks and more‚Äîalongside an array of documentation, tutorials, how-to-videos, webinars, and more. You can also access [Graphcore‚Äôs GitHub repo](https://github.com/graphcore) for more code references and tutorials.\\n\\nTo learn more about using Hugging Face on Graphcore, head over to our [partner page](https://huggingface.co/hardware/graphcore)!</td>\n",
       "      <td>What is the name of the SDK that Graphcore offers for easy use with IPU?\\n</td>\n",
       "      <td>Poplar</td>\n",
       "      <td>huggingface/blog/blob/main/graphcore-update.md</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The context provides a lot of information about Graphcore's Poplar SDK, but it does not explicitly mention the name of the SDK that Graphcore offers for easy use with IPU. Therefore, I would give a rating of 3, which indicates that the question is somewhat answerable with the context.\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This question is not directly related to the Hugging Face ecosystem, which includes libraries for natural language processing and computer vision tasks. The Poplar SDK is a software development kit for Graphcore's Intelligence Processing Unit (IPU) hardware, which is a type of processor designed for machine learning workloads. While it's good to know about different hardware and software options for machine learning, this question is not particularly useful for developers building NLP applications with the Hugging Face ecosystem.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking about the name of the SDK that Graphcore offers for easy use with IPU. This question is context-independant, as it is clear to an operator with documentation what the question is asking about.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>```py\\n&gt;&gt;&gt; import albumentations\\n&gt;&gt;&gt; import numpy as np\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; transform = albumentations.Compose(\\n...     [\\n...         albumentations.Resize(480, 480),\\n...         albumentations.HorizontalFlip(p=1.0),\\n...         albumentations.RandomBrightnessContrast(p=1.0),\\n...     ],\\n...     bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\\n... )\\n```\\n\\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': List[Dict]}`,\\n where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:\\n\\n```py\\n&gt;&gt;&gt; def formatted_anns(image_id, category, area, bbox):\\n...     annotations = []\\n...     for i in range(0, len(category)):\\n...         new_ann = {\\n...             \"image_id\": image_id,\\n...             \"category_id\": category[i],\\n...             \"isCrowd\": 0,\\n...             \"area\": area[i],\\n...             \"bbox\": list(bbox[i]),\\n...         }\\n...         annotations.append(new_ann)\\n\\n...     return annotations\\n```\\n\\nNow you can combine the image and annotation transformations to use on a batch of examples:\\n\\n```py\\n&gt;&gt;&gt; # transforming a batch\\n&gt;&gt;&gt; def transform_aug_ann(examples):\\n...     image_ids = examples[\"image_id\"]\\n...     images, bboxes, area, categories = [], [], [], []\\n...     for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\\n...         image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\\n...         out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\\n\\n...         area.append(objects[\"area\"])\\n...         images.append(out[\"image\"])\\n...         bboxes.append(out[\"bboxes\"])\\n...         categories.append(out[\"category\"])\\n\\n...     targets = [\\n...         {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\\n...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\\n...     ]</td>\n",
       "      <td>What is the format of the annotations expected by the image_processor?\\n</td>\n",
       "      <td>The annotations are expected in the format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation.</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The context provides a function `formatted_anns` that reformats annotations for a single example into the format expected by the `image_processor`. The question asks for this format. The context also provides the `transform` that is used to transform a batch of examples. The question asks for the format of the annotations expected by this `transform`. However, the question does not specify whether the annotations should be in the format expected by the `image_processor` before or after being transformed by the `transform`. Therefore, the rating is 3, because the question is answerable with the context, but the answer is ambiguous.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L201\\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_64x4d-f9a8e184.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.63%\\n      Top 5 Accuracy: 95.0%\\n- Name: gluon_resnext50_32x4d\\n  In Collection: Gloun ResNeXt\\n  Metadata:\\n    FLOPs: 5472648192\\n    Parameters: 25030000\\n    File Size: 100441719\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resnext50_32x4d\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L185\\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext50_32x4d-e6a097c1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.35%\\n      Top 5 Accuracy: 94.42%\\n--&gt;</td>\n",
       "      <td>What is the architecture of gluon\\_resnext50\\_32x4d?\\n</td>\n",
       "      <td>The architecture of gluon\\_resnext50\\_32x4d includes 1x1 Convolution, Batch Normalization, Convolution, Global Average Pooling, Grouped Convolution, Max Pooling, ReLU, ResNeXt Block, Residual Connection, and Softmax.</td>\n",
       "      <td>huggingface/pytorch-image-models/blob/main/docs/models/gloun-resnext.md</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The context provides the architecture of the gluon\\_resnext50\\_32x4d model, which is a type of ResNeXt model. However, it does not provide a direct answer to the question about the architecture of gluon\\_resnext50\\_32x4d. The context only provides the architecture of the model, but not the exact model requested in the question. Therefore, I would rate the total rating a 3 out of 5.\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the architecture of a specific model, gluon\\_resnext50\\_32x4d, and provides enough context for me to understand that it is a convolutional neural network architecture.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Models\\n\\n[[autodoc]] timm.create_model\\n\\n[[autodoc]] timm.list_models</td>\n",
       "      <td>What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n</td>\n",
       "      <td>1,024,116,960</td>\n",
       "      <td>huggingface/pytorch-image-models/blob/main/hfdocs/source/reference/models.mdx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides a function to create a model with timm.create\\_model, and it is possible to specify the vit\\_giant\\_patch16\\_384 model and check the number of parameters. The function returns a model with a specific architecture, and it is possible to compare it with the vit\\_giant\\_patch16\\_384 model and find the number of parameters. Therefore, it is completely straightforward to answer the question with the given context.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This answer is direct and provides the requested information. It is useful for developers who want to use this model and need to know its size.\\n\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the number of parameters in a specific model, vit\\_giant\\_patch16\\_384. The answer is a number, not a range or a list of models, so it is clear what the question is asking for.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>```python\\naccelerate launch scripts/codeparrot_training.py \\\\n    --model_ckpt codeparrot/codeparrot-small \\\\n    --dataset_name_train codeparrot/github-jupyter-text-to-code \\\\n    --dataset_name_valid codeparrot/github-jupyter-text-to-code \\\\n    --train_batch_size 12 \\\\n    --valid_batch_size 12 \\\\n    --learning_rate 5e-4 \\\\n    --num_warmup_steps 100 \\\\n    --gradient_accumulation 1 \\\\n    --gradient_checkpointing False \\\\n    --max_train_steps 3000 \\\\n    --save_checkpoint_steps 200 \\\\n    --save_dir jupyter-text-to-python\\n```\\n\\n## Code explanation: python to text\\nIn this task we want to train a model to explain python code. We finetuned Codeparrot-small on [github-jupyter-code-to-text](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text), a dataset where the samples are a succession of Python code and its explanation as a docstring, we just inverted the order of text and code pairs in github-jupyter-code-to-text dataset and added the delimiters \"Explanation:\" and \"End of explanation\" inside the doctrings.\\n\\nTo fine-tune a model on this dataset we use the same [script](https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/codeparrot_training.py) as the pretraining of codeparrot:\\n\\n```python\\naccelerate launch scripts/codeparrot_training.py \\\\n    --model_ckpt codeparrot/codeparrot-small \\\\n    --dataset_name_train codeparrot/github-jupyter-code-to-text \\\\n    --dataset_name_valid codeparrot/github-jupyter-code-to-text \\\\n    --train_batch_size 12 \\\\n    --valid_batch_size 12 \\\\n    --learning_rate 5e-4 \\\\n    --num_warmup_steps 100 \\\\n    --gradient_accumulation 1 \\\\n    --gradient_checkpointing False \\\\n    --max_train_steps 3000 \\\\n    --save_checkpoint_steps 200 \\\\n    --save_dir jupyter-python-to-text\\n```</td>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>codeparrot/codeparrot-small</td>\n",
       "      <td>huggingface/transformers/blob/main/examples/research_projects/codeparrot/examples/README.md</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context provides a lot of information about the training process of a model, but it does not directly answer the question. The question asks for the number of layers in the model, while the context only mentions the model is Codeparrot-small, which is a pretrained model from Hugging Face. The number of layers is not specified in the model card. Therefore, I would rate this a 2, as the question is not directly answerable with the context.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                               Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) and filter at the left according to the language of your interest.\\n\\n## 3. Building Your Own Sentiment Analysis Model\\n\\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\\n\\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [ü§óTransformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\\n\\nLet's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "1                                                                                                                                                                                                                                                                                            <figure class=\"image\">\\n  <img alt=\"Batch scaling experiment for PyTorch and Tensorflow\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/batch_scaling_exp_throughput.svg\" style=\"width:100%\"/>\\n<figcaption>Figure 10. Sum throughput with respect to number of instances for a total batch size of 8</figcaption>\\n</figure>\\n<br>\\n\\n## 9. Conclusion\\n\\nThrough this blog post, we covered out-of-box BERT inference performance one can expect for PyTorch and TensorFlow, \\nfrom a simple PyPi install and without further tuning.  \\nIt is important to highlight results provided here reflects out-of-the-box framework setup hence, they might not provide the absolute best performances.  \\nWe decided to not include optimizations as part of this blog post to focus on hardware and efficiency. \\nOptimizations will be discussed in the second part! üöÄ\\n\\nThen, we covered and detailed the impact, and the importance of setting the thread affinity along with the trade-off between the target problem size, and the number of cores required for achieving the task.  \\nAlso, it is important to define **which criteria**  _(i.e. latency vs throughput)_ to use when optimizing your deployment as the resulting setups might be totally different.\\n\\nOn a more general note, small problem sizes (_short sequences and/or small batches_) might require much fewer cores to achieve the best possible latency than big problems (_very long sequences and/or big batches_).\\n\\nIt is interesting to cover all these aspects when thinking about the final deployment platform as it might cut the cost of the infrastructure drastically.  \\nFor instance, our 48 cores machine charges **4.848\\$/h** whereas a smaller instances with only 8 cores lowers the cost to **0.808\\$/h**, leading to a **6x cost reduction**.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |      |      |[huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b)                                                                                |12186       |117     |llama-license            |https://huggingface.co/huggyllama/llama-13b/blob/main/LICENSE                      |[LICENSE](https://huggingface.co/huggyllama/llama-13b/blob/main/LICENSE)                                                |                                                                                                    |             |\\n|      |      |[TheBloke/Llama-2-7B-fp16](https://huggingface.co/TheBloke/Llama-2-7B-fp16)                                                                        |11108       |32      | llama2 |                                                 |[LICENSE](https://huggingface.co/TheBloke/Llama-2-7B-fp16/blob/main/LICENSE)                                            |                                                                                                    |             |\\n|      |      |[beomi/llama-2-ko-7b](https://huggingface.co/beomi/llama-2-ko-7b)                                                                                  |10623       |77      | llama2 |                                                 |[LICENSE](https://huggingface.co/beomi/llama-2-ko-7b/blob/main/LICENSE)                                                 |                                                                                                    |             |   \n",
       "3                  - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_em\\n    Crop Pct: '0.882'\\n    Image Size: '240'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1541\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_em-e78cfe58.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.71%\\n      Top 5 Accuracy: 94.33%\\n- Name: tf_efficientnet_es\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 2057577472\\n    Parameters: 5440000\\n    File Size: 22008479\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_es\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1531\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.28%\\n      Top 5 Accuracy: 93.6%\\n- Name: tf_efficientnet_l2_ns_475\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 217795669644\\n    Parameters: 480310000\\n    File Size: 1925950424\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 !--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# CANINE\\n\\n## Overview\\n\\nThe CANINE model was proposed in [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\\nRepresentation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. It's\\namong the first papers that trains a Transformer without using an explicit tokenization step (such as Byte Pair\\nEncoding (BPE), WordPiece or SentencePiece). Instead, the model is trained directly at a Unicode character-level.\\nTraining at a character-level inevitably comes with a longer sequence length, which CANINE solves with an efficient\\ndownsampling strategy, before applying a deep Transformer encoder.\\n\\nThe abstract from the paper is the following:   \n",
       "5                                                                                                                                                                                         Very quickly we see that it's a much more coordinated approach for searching through the API, with no added headache for you!\\n\\n## What is the magic?\\n\\nVery briefly we'll talk about the underlying magic at play that gives us this enum-dictionary-like datatype, the `AttributeDictionary`.\\n\\nHeavily inspired by the `AttrDict` class from the [fastcore](https://fastcore.fast.ai/basics.html#AttrDict) library, the general idea is we take a normal dictionary and supercharge it for *exploratory programming* by providing tab-completion for every key in the dictionary. \\n\\nAs we saw earlier, this gets even stronger when we have nested dictionaries we can explore through, such as `model_args.dataset.glue`!\\n\\n> For those familiar with JavaScript, we mimic how the `object` class is working.\\n\\nThis simple utility class can provide a much more user-focused experience when exploring nested datatypes and trying to understand what is there, such as the return of an API request!\\n\\nAs mentioned before, we expand on the `AttrDict` in a few key ways:\\n- You can delete keys with `del model_args[key]` *or* with `del model_args.key`\\n- That clean `__repr__` we saw earlier \\n\\nOne very important concept to note though, is that if a key contains a number or special character it **must** be indexed as a dictionary, and *not* as an object.\\n\\n```python\\n>>> from huggingface_hub.utils.endpoint_helpers import AttributeDictionary\\n```\\n\\nA very brief example of this is if we have an `AttributeDictionary` with a key of `3_c`:\\n\\n\\n```python\\n>>> d = {\"a\":2, \"b\":3, \"3_c\":4}\\n>>> ad = AttributeDictionary(d)\\n```\\n\\n\\n```python\\n>>> # As an attribute\\n>>> ad.3_c\\n```\\n     File \"<ipython-input-6-c0fe109cf75d>\", line 2\\n        ad.3_c\\n            ^\\n    SyntaxError: invalid token\\n\\n```python\\n>>> # As a dictionary key\\n>>> ad[\"3_c\"]\\n```\\n    4\\n\\n## Concluding thoughts   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   You can easily run `pipeline` on large models using ü§ó `accelerate`! First make sure you have installed `accelerate` with `pip install accelerate`. \\n\\nFirst load your model using `device_map=\"auto\"`! We will use `facebook/opt-1.3b` for our example.\\n\\n```py\\n# pip install accelerate\\nimport torch\\nfrom transformers import pipeline\\n\\npipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\\n```\\n\\nYou can also pass 8-bit loaded models if you install `bitsandbytes` and add the argument `load_in_8bit=True`\\n\\n```py\\n# pip install accelerate bitsandbytes\\nimport torch\\nfrom transformers import pipeline\\n\\npipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\\n```\\n\\nNote that you can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM!   \n",
       "7                                                        1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Pawe≈Ç Krzysztof Nowak, Thomas M√ºller, Francesco Piccinno and Julian Martin Eisenschlos.\\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).\\n1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\\n1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \\nat the end of the robust speech challenge.\\n\\nIf there is no real audio data for your language the final score will be \\ncomputed solely based on the Common Voice 7 test dataset. If there is also\\nno Common Voice 7 test dataset for your language, we will see together how to \\nscore your model - if this is the case, please don't be discouraged. We are \\nespecially excited about speech recognition systems of such low-resource \\nlanguages and will make sure that we'll decide on a good approach to evaluating \\nyour model.\\n\\n## Prizes\\n\\nTODO(Patrick, Omar, ...)\\n\\n## Communication and Problems\\n\\nIf you encounter any problems or have any questions, you should use one of the following platforms\\ndepending on your type of problem. Hugging Face is an \"open-source-first\" organization meaning \\nthat we'll try to solve all problems in the most public and most transparent way possible so that everybody\\nin the community profits.\\n\\nThe following table summarizes what platform to use for which problem.   \n",
       "9                                               Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very personal and probably biased view:\\n\\n| Format                  | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\\n| pickle (PyTorch)        | ‚úó | ‚úó | ‚úó | üó∏ | ‚úó | üó∏ | üó∏ |\\n| H5 (Tensorflow)         | üó∏ | ‚úó | üó∏ | üó∏ | ~ | ~ | ‚úó |\\n| SavedModel (Tensorflow) | üó∏ | ‚úó | ‚úó | üó∏ | üó∏ | ‚úó | üó∏ |\\n| MsgPack (flax)          | üó∏ | üó∏ | ‚úó | üó∏ | ‚úó | ‚úó | üó∏ |\\n| Protobuf (ONNX)         | üó∏ | ‚úó | ‚úó | ‚úó | ‚úó | ‚úó | üó∏ |\\n| Cap'n'Proto             | üó∏ | üó∏ | ~ | üó∏ | üó∏ | ~ | ‚úó |\\n| Arrow                   | ? | ? | ? | ? | ? | ? | ‚úó |\\n| Numpy (npy,npz)         | üó∏ | ? | ? | ‚úó | üó∏ | ‚úó | ‚úó |\\n| pdparams (Paddle)       | ‚úó | ‚úó | ‚úó | üó∏ | ‚úó | üó∏ | üó∏ |\\n| SafeTensors             | üó∏ | üó∏ | üó∏ | üó∏ | üó∏ | ‚úó | üó∏ |\\n\\n- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\\n- Zero-copy: Does reading the file require more memory than the original file ?\\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\\nsome tensors in it without scanning the whole file (distributed setting) ?\\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\\n- No file size limit: Is there a limit to the file size ?\\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)\\n- Bfloat16/Fp8: Does the format support native bfloat16/fp8 (meaning no weird workarounds are\\nnecessary)? This is becoming increasingly important in the ML world.\\n\\n\\n### Main oppositions   \n",
       "10                                                                                                                                                                                  For more details about authentication, check out [this section](../quick-start#authentication).\\n\\n### HF_HUB_VERBOSITY\\n\\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of\\n`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\\n\\nDefaults to `\"warning\"`.\\n\\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\\n\\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\\n\\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,\\nsmall files will be duplicated to ease user experience while bigger files are symlinked to save disk usage.\\n\\nFor more details, see the [download guide](../guides/download#download-files-to-local-folder).\\n\\n### HF_HUB_ETAG_TIMEOUT\\n\\nInteger value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s.\\n\\n### HF_HUB_DOWNLOAD_TIMEOUT\\n\\nInteger value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s.\\n\\n## Boolean values\\n\\nThe following environment variables expect a boolean value. The variable will be considered\\nas `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\\n(or undefined) will be considered as `False`.\\n\\n### HF_HUB_OFFLINE   \n",
       "11  from enum import Enum\\n```\\n\\n## Prepare Model and Tokenizer\\n\\nNow, we will be adding 27 new tokens as well as replace the existing pad, bos and eos tokens of the model.\\n\\n\\n```python\\nclass SpecialTokens(str, Enum):\\n    begin_target = \"<|begintarget|>\"\\n    end_target = \"<|endtarget|>\"\\n    begin_context = \"<|begincontext|>\"\\n    end_context = \"<|endcontext|>\"\\n    system = \"<|system|>\"\\n    user = \"<|user|>\"\\n    begin_last_user_utterance = \"<|beginlastuserutterance|>\"\\n    end_last_user_utterance = \"<|endlastuserutterance|>\"\\n    begin_dsts = \"<|begindsts|>\"\\n    end_dsts = \"<|enddsts|>\"\\n    begin_dst = \"<|begindst|>\"\\n    end_dst = \"<|enddst|>\"\\n    begin_belief = \"<|beginbelief|>\"\\n    end_belief = \"<|endbelief|>\"\\n    begin_response = \"<|beginresponse|>\"\\n    end_response = \"<|endresponse|>\"\\n    begin_action = \"<|beginaction|>\"\\n    end_action = \"<|endaction|>\"\\n    begin_user_action = \"<|beginuseraction|>\"\\n    end_user_action = \"<|enduseraction|>\"\\n    sys_actions = \"<|sysactions|>\"\\n    begin_intent = \"<|beginintent|>\"\\n    end_intent = \"<|endintent|>\"\\n    begin_requested_slots = \"<|beginrequestedslots|>\"\\n    end_requested_slots = \"<|endrequestedslots|>\"\\n    pad_token = \"<|pad|>\"\\n    bos_token = \"<|startoftext|>\"\\n\\n    @classmethod\\n    def list(cls):\\n        return [c.value for c in cls]\\n```\\n\\nWe will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens followed by loading the base model and resizzing the embedding layers to accomodate the newly added tokens.\\n\\n\\n```python\\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\\ntokenizer = AutoTokenizer.from_pretrained(\\n    model_name,\\n    pad_token=SpecialTokens.pad_token.value,\\n    bos_token=SpecialTokens.bos_token.value,\\n    eos_token=SpecialTokens.end_target.value,\\n    additional_special_tokens=SpecialTokens.list(),\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    low_cpu_mem_usage=True\\n    # use_flash_attention_2=True, # leading to an error\\n)\\nmodel.resize_token_embeddings(len(tokenizer))\\n```   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    - [DistilBERT ‡∞§‡±ã ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\\n- [T5 ‡∞§‡±ã ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)   \n",
       "13                                                                                                                                                                                 Software also plays a vital role in unlocking the IPU‚Äôs capabilities, so naturally Optimum offers a plug-and-play experience with Graphcore‚Äôs easy-to-use Poplar SDK ‚Äî which itself has received a major 2.5 update. Poplar makes it easy to train state-of-the-art models on state-of-the-art hardware, thanks to its full integration with standard machine learning frameworks, including PyTorch, PyTorch Lightning, and TensorFlow‚Äîas well as orchestration and deployment tools such as Docker and Kubernetes. Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU‚Äôs advanced AI capabilities.\\n\\n## Get started with Hugging Face‚Äôs Optimum Graphcore models\\n\\nIf you‚Äôre interested in combining the benefits of IPU technology with the strengths of transformer models, you can download the latest range of Optimum Graphcore models from the [Graphcore organization on the Hub](https://huggingface.co/Graphcore), or access the code from the [Optimum GitHub repo](https://github.com/huggingface/optimum-graphcore). Our [Getting Started blog post](https://huggingface.co/blog/graphcore-getting-started) will guide you through each step to start experimenting with IPUs.\\n\\nAdditionally, Graphcore has built an extensive page of [developer resources](https://www.graphcore.ai/developer), where you can find the IPU Model Garden‚Äîa repository of deployment-ready ML applications including computer vision, NLP, graph networks and more‚Äîalongside an array of documentation, tutorials, how-to-videos, webinars, and more. You can also access [Graphcore‚Äôs GitHub repo](https://github.com/graphcore) for more code references and tutorials.\\n\\nTo learn more about using Hugging Face on Graphcore, head over to our [partner page](https://huggingface.co/hardware/graphcore)!   \n",
       "14                                                                                      ```py\\n>>> import albumentations\\n>>> import numpy as np\\n>>> import torch\\n\\n>>> transform = albumentations.Compose(\\n...     [\\n...         albumentations.Resize(480, 480),\\n...         albumentations.HorizontalFlip(p=1.0),\\n...         albumentations.RandomBrightnessContrast(p=1.0),\\n...     ],\\n...     bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\\n... )\\n```\\n\\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': List[Dict]}`,\\n where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:\\n\\n```py\\n>>> def formatted_anns(image_id, category, area, bbox):\\n...     annotations = []\\n...     for i in range(0, len(category)):\\n...         new_ann = {\\n...             \"image_id\": image_id,\\n...             \"category_id\": category[i],\\n...             \"isCrowd\": 0,\\n...             \"area\": area[i],\\n...             \"bbox\": list(bbox[i]),\\n...         }\\n...         annotations.append(new_ann)\\n\\n...     return annotations\\n```\\n\\nNow you can combine the image and annotation transformations to use on a batch of examples:\\n\\n```py\\n>>> # transforming a batch\\n>>> def transform_aug_ann(examples):\\n...     image_ids = examples[\"image_id\"]\\n...     images, bboxes, area, categories = [], [], [], []\\n...     for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\\n...         image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\\n...         out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\\n\\n...         area.append(objects[\"area\"])\\n...         images.append(out[\"image\"])\\n...         bboxes.append(out[\"bboxes\"])\\n...         categories.append(out[\"category\"])\\n\\n...     targets = [\\n...         {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\\n...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\\n...     ]   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L201\\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_64x4d-f9a8e184.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.63%\\n      Top 5 Accuracy: 95.0%\\n- Name: gluon_resnext50_32x4d\\n  In Collection: Gloun ResNeXt\\n  Metadata:\\n    FLOPs: 5472648192\\n    Parameters: 25030000\\n    File Size: 100441719\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resnext50_32x4d\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L185\\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext50_32x4d-e6a097c1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.35%\\n      Top 5 Accuracy: 94.42%\\n-->   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Models\\n\\n[[autodoc]] timm.create_model\\n\\n[[autodoc]] timm.list_models   \n",
       "17                                                                                                                                                                                                                                                 ```python\\naccelerate launch scripts/codeparrot_training.py \\\\n    --model_ckpt codeparrot/codeparrot-small \\\\n    --dataset_name_train codeparrot/github-jupyter-text-to-code \\\\n    --dataset_name_valid codeparrot/github-jupyter-text-to-code \\\\n    --train_batch_size 12 \\\\n    --valid_batch_size 12 \\\\n    --learning_rate 5e-4 \\\\n    --num_warmup_steps 100 \\\\n    --gradient_accumulation 1 \\\\n    --gradient_checkpointing False \\\\n    --max_train_steps 3000 \\\\n    --save_checkpoint_steps 200 \\\\n    --save_dir jupyter-text-to-python\\n```\\n\\n## Code explanation: python to text\\nIn this task we want to train a model to explain python code. We finetuned Codeparrot-small on [github-jupyter-code-to-text](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text), a dataset where the samples are a succession of Python code and its explanation as a docstring, we just inverted the order of text and code pairs in github-jupyter-code-to-text dataset and added the delimiters \"Explanation:\" and \"End of explanation\" inside the doctrings.\\n\\nTo fine-tune a model on this dataset we use the same [script](https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/codeparrot_training.py) as the pretraining of codeparrot:\\n\\n```python\\naccelerate launch scripts/codeparrot_training.py \\\\n    --model_ckpt codeparrot/codeparrot-small \\\\n    --dataset_name_train codeparrot/github-jupyter-code-to-text \\\\n    --dataset_name_valid codeparrot/github-jupyter-code-to-text \\\\n    --train_batch_size 12 \\\\n    --valid_batch_size 12 \\\\n    --learning_rate 5e-4 \\\\n    --num_warmup_steps 100 \\\\n    --gradient_accumulation 1 \\\\n    --gradient_checkpointing False \\\\n    --max_train_steps 3000 \\\\n    --save_checkpoint_steps 200 \\\\n    --save_dir jupyter-python-to-text\\n```   \n",
       "\n",
       "                                                                                                                                        question  \\\n",
       "0                                                What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n   \n",
       "1                                                                                              How much does a 48 cores machine cost per hour?\\n   \n",
       "2                                                                                                                      (your factoid question)\\n   \n",
       "3                                                                                      What is the number of parameters in tf_efficientnet_es?\\n   \n",
       "4                                                                                                        What corpus is CANINE pre-trained on?\\n   \n",
       "5                                                                                                                      (your factoid question)\\n   \n",
       "6   What other library can I use to replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM?\\n   \n",
       "7                         What is the name of the model from Microsoft Research that is used for table extraction from unstructured documents?\\n   \n",
       "8                                                                                                                      (your factoid question)\\n   \n",
       "9                                                                                                                      (your factoid question)\\n   \n",
       "10                                                                                                         What is the default value for HF_HUB_   \n",
       "11                                                                                                 How many new tokens are added to the model?\\n   \n",
       "12                                                               How many square kilometers of the Amazon rainforest are covered by the basin?\\n   \n",
       "13                                                                    What is the name of the SDK that Graphcore offers for easy use with IPU?\\n   \n",
       "14                                                                      What is the format of the annotations expected by the image_processor?\\n   \n",
       "15                                                                                        What is the architecture of gluon\\_resnext50\\_32x4d?\\n   \n",
       "16                                                                     What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n   \n",
       "17                                                                                                                     (your factoid question)\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                  answer  \\\n",
       "0                                                                                                                                                                      The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "1                                                                                                                                                                                                                                                               4.848$/h   \n",
       "2                                                                                                                                                     The model [huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b) is released under the llama-license.   \n",
       "3                                                                                                                                                                                                                                                                5440000   \n",
       "4                                                                                     CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with   \n",
       "5                                                                                                                                                       A dictionary key must be used to index a key containing a number or special character in an AttributeDictionary.   \n",
       "6   You can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM by specifying it in the `pipeline` function, like so: `pipeline(model=\"your_model_name\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})`.   \n",
       "7                                                                                                                                                                                                                                                      Table Transformer   \n",
       "8                                                                                                                    If you encounter any problems or have any questions about the Hugging Face Robust Speech Challenge, you should use the Hugging Face Discord server.   \n",
       "9                                                                                                            The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.   \n",
       "10                                                                               The default value for HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR is False.\\n\\n### HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR2\\n\\nOutput:::\\nFactoid question: What is the default value for HF_HUB_   \n",
       "11                                                                                                                                                                                                           27 new tokens are added to the model.\\n\\n\\n```python\\n\\n```   \n",
       "12                                                                                                                                                                                        5,500,000 square kilometers of the Amazon rainforest are covered by the basin.   \n",
       "13                                                                                                                                                                                                                                                                Poplar   \n",
       "14                                                                                                                          The annotations are expected in the format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation.   \n",
       "15                                              The architecture of gluon\\_resnext50\\_32x4d includes 1x1 Convolution, Batch Normalization, Convolution, Global Average Pooling, Grouped Convolution, Max Pooling, ReLU, ResNeXt Block, Residual Connection, and Softmax.   \n",
       "16                                                                                                                                                                                                                                                         1,024,116,960   \n",
       "17                                                                                                                                                                                                                                           codeparrot/codeparrot-small   \n",
       "\n",
       "                                                                                                 source_doc  \\\n",
       "0                                                   huggingface/blog/blob/main/sentiment-analysis-python.md   \n",
       "1                                                     huggingface/blog/blob/main/bert-cpu-scaling-part-1.md   \n",
       "2                               huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md   \n",
       "3                                 huggingface/pytorch-image-models/blob/main/docs/models/tf-efficientnet.md   \n",
       "4                                     huggingface/transformers/blob/main/docs/source/en/model_doc/canine.md   \n",
       "5                                                           huggingface/blog/blob/main/searching-the-hub.md   \n",
       "6                                    huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md   \n",
       "7                                                        huggingface/transformers/blob/main/README_pt-br.md   \n",
       "8               huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md   \n",
       "9                                                   huggingface/safetensors/blob/main/safetensors/README.md   \n",
       "10          huggingface/huggingface_hub/blob/main/docs/source/en/package_reference/environment_variables.md   \n",
       "11  huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb   \n",
       "12                                                          huggingface/transformers/blob/main/README_te.md   \n",
       "13                                                           huggingface/blog/blob/main/graphcore-update.md   \n",
       "14                              huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md   \n",
       "15                                  huggingface/pytorch-image-models/blob/main/docs/models/gloun-resnext.md   \n",
       "16                            huggingface/pytorch-image-models/blob/main/hfdocs/source/reference/models.mdx   \n",
       "17              huggingface/transformers/blob/main/examples/research_projects/codeparrot/examples/README.md   \n",
       "\n",
       "    groundedness_score  \\\n",
       "0                  5.0   \n",
       "1                  1.0   \n",
       "2                  1.0   \n",
       "3                  5.0   \n",
       "4                  5.0   \n",
       "5                  1.0   \n",
       "6                  5.0   \n",
       "7                  1.0   \n",
       "8                  NaN   \n",
       "9                  5.0   \n",
       "10                 NaN   \n",
       "11                 3.0   \n",
       "12                 5.0   \n",
       "13                 3.0   \n",
       "14                 3.0   \n",
       "15                 3.0   \n",
       "16                 5.0   \n",
       "17                 2.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     groundedness_eval  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                    The context clearly states that the IMDB dataset is used in the tutorial to fine-tune a DistilBERT model for sentiment analysis. The question asks for the dataset used in the tutorial for sentiment analysis, so the context directly answers the question.\\n\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                        The context does not provide any information about the cost of a 48 cores machine per hour. It only mentions that the cost of a 48 cores machine is 4.848$/h, but it does not specify whether this cost is for a hour of usage or for the machine itself.\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The context does not provide any information about the question.\\n\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The context clearly states that the number of parameters in tf_efficientnet_es is 5440000.\\n   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The context clearly states that the CANINE model is pre-trained on the text8 corpus.\\n\\n   \n",
       "5                                                                                                                                                                                                                                                                                          The context provides information about the `AttributeDictionary` class, which is a dictionary-like datatype that provides tab-completion for every key in the dictionary. However, the context does not provide any information about how this class is used to search through an API. Therefore, it is not possible to answer the question without additional context.\\n\\n   \n",
       "6                                                                                                                                                                                                                                                                                  The context provides a clear example of how to load a large model using the `device_map=\"auto\"` argument and the `pipeline` function from the `transformers` library. It also mentions that you can replace the checkpoint with any of the Hugging Face models that support large model loading, such as BLOOM. Therefore, the question is clearly answerable with the context.\\n\\n   \n",
       "7                                                                                                                                                                                                                                                                                                                             The context provides a list of models from Microsoft Research, Google AI, HuggingFace, Facebook, the University of California at Berkeley, Google/CMU, and their respective papers. However, the context does not provide the name of the model from Microsoft Research that is used for table extraction from unstructured documents.\\n   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NaN   \n",
       "9                                                                                                                                                                                                                                                        \\nThe question asks for the main oppositions between the different formats. The context provides a table that highlights the differences between the formats, and the answer identifies the main oppositions as being between formats that support lazy loading and layout control, and those that do not. The answer is directly responsive to the question and provides a clear and unambiguous answer.\\n\\n   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                              The context provides information about the addition of 27 new tokens and replacement of existing pad, bos and eos tokens of the model. However, it does not provide information about the number of new tokens added to the model.\\n\\n   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The context provides a clear and unambiguous answer to the question, as it specifies the exact area covered by the Amazon rainforest in the Amazon basin.\\n\\n   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                   The context provides a lot of information about Graphcore's Poplar SDK, but it does not explicitly mention the name of the SDK that Graphcore offers for easy use with IPU. Therefore, I would give a rating of 3, which indicates that the question is somewhat answerable with the context.\\n\\n   \n",
       "14  The context provides a function `formatted_anns` that reformats annotations for a single example into the format expected by the `image_processor`. The question asks for this format. The context also provides the `transform` that is used to transform a batch of examples. The question asks for the format of the annotations expected by this `transform`. However, the question does not specify whether the annotations should be in the format expected by the `image_processor` before or after being transformed by the `transform`. Therefore, the rating is 3, because the question is answerable with the context, but the answer is ambiguous.\\n\\n   \n",
       "15                                                                                                                                                                                                                                                                The context provides the architecture of the gluon\\_resnext50\\_32x4d model, which is a type of ResNeXt model. However, it does not provide a direct answer to the question about the architecture of gluon\\_resnext50\\_32x4d. The context only provides the architecture of the model, but not the exact model requested in the question. Therefore, I would rate the total rating a 3 out of 5.\\n\\n   \n",
       "16                                                                                                                                                                                                               The context provides a function to create a model with timm.create\\_model, and it is possible to specify the vit\\_giant\\_patch16\\_384 model and check the number of parameters. The function returns a model with a specific architecture, and it is possible to compare it with the vit\\_giant\\_patch16\\_384 model and find the number of parameters. Therefore, it is completely straightforward to answer the question with the given context.\\n\\n   \n",
       "17                                                                                                                                                                                                     The context provides a lot of information about the training process of a model, but it does not directly answer the question. The question asks for the number of layers in the model, while the context only mentions the model is Codeparrot-small, which is a pretrained model from Hugging Face. The number of layers is not specified in the model card. Therefore, I would rate this a 2, as the question is not directly answerable with the context.\\n   \n",
       "\n",
       "    relevance_score  \\\n",
       "0               4.0   \n",
       "1               1.0   \n",
       "2               5.0   \n",
       "3               NaN   \n",
       "4               5.0   \n",
       "5               5.0   \n",
       "6               NaN   \n",
       "7               2.0   \n",
       "8               5.0   \n",
       "9               5.0   \n",
       "10              NaN   \n",
       "11              5.0   \n",
       "12              1.0   \n",
       "13              1.0   \n",
       "14              NaN   \n",
       "15              NaN   \n",
       "16              5.0   \n",
       "17              5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 relevance_eval  \\\n",
       "0                                                                                                                                                                                                                                                                                                               This question is useful for machine learning developers who are working on NLP applications using the Hugging Face ecosystem, as it helps them understand the dataset used in the tutorial and potentially reuse it for their own projects.\\n\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                Not useful at all for machine learning developers building NLP applications with the Hugging Face ecosystem.\\n   \n",
       "2                                                                                             This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
       "4                                                                                                                                           The question is asking about the corpus used for pre-training a specific model, CANINE. This information is useful for developers who want to understand the performance and applicability of the model in their NLP tasks. The C4 corpus is a widely used dataset in NLP, and knowing that CANINE is pre-trained on it can help developers compare the model with other models pre-trained on the same corpus.\\n\\n   \n",
       "5                                                                                             This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
       "7                                                                                                                                                                                                                                                                                                                     The question is specific and clear, but it asks for a proper name, which is not provided in the context. The question is still relevant to NLP and the Hugging Face ecosystem, as it relates to a model used for a specific NLP task.\\n\\n   \n",
       "8                                                                                             This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n   \n",
       "9                                                                                             This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "11                                                                                                          This question is highly relevant to NLP development using the Hugging Face ecosystem, as it pertains to the tokenization process, which is a crucial step in preparing text data for input into a model. The question also implies an understanding of the tokenization process and the ability to count the number of new tokens added to a model, which suggests a high level of expertise in NLP development using the Hugging Face ecosystem.\\n   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                   This question is not related to machine learning, natural language processing, or the Hugging Face ecosystem. It is a geographical question about the Amazon rainforest.\\n   \n",
       "13  This question is not directly related to the Hugging Face ecosystem, which includes libraries for natural language processing and computer vision tasks. The Poplar SDK is a software development kit for Graphcore's Intelligence Processing Unit (IPU) hardware, which is a type of processor designed for machine learning workloads. While it's good to know about different hardware and software options for machine learning, this question is not particularly useful for developers building NLP applications with the Hugging Face ecosystem.\\n\\n   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                          This answer is direct and provides the requested information. It is useful for developers who want to use this model and need to know its size.\\n\\n   \n",
       "17                                                                                            This question is useful for Hugging Face developers because it asks about a specific feature of the Hugging Face ecosystem, the `pipeline` function, and how it can be used to build NLP applications. The question also asks about the `tokenizer` function, which is another important part of the Hugging Face ecosystem. By understanding how these functions work and how they can be used together, developers can build more effective NLP applications.\\n   \n",
       "\n",
       "    standalone_score  \\\n",
       "0                5.0   \n",
       "1                5.0   \n",
       "2                5.0   \n",
       "3                5.0   \n",
       "4                5.0   \n",
       "5                5.0   \n",
       "6                5.0   \n",
       "7                5.0   \n",
       "8                5.0   \n",
       "9                5.0   \n",
       "10               5.0   \n",
       "11               5.0   \n",
       "12               5.0   \n",
       "13               5.0   \n",
       "14               NaN   \n",
       "15               5.0   \n",
       "16               5.0   \n",
       "17               5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                             standalone_eval  \n",
       "0                                                                                              The question is asking about a specific dataset used in a tutorial, which is a clear and specific request. The tutorial in question is the Hugging Face Space tutorial on fine-tuning a DistilBERT model for sentiment analysis, so the context is clear.\\n\\n  \n",
       "1                                                                                                                                                                                                                                                                        The question is context-independant, as it is clear what the question is about.\\n\\n  \n",
       "2                                                                                                                                                                                                                                             The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n  \n",
       "3                                                                                               The question is asking for the number of parameters in a specific model, tf_efficientnet_es, which is a pre-trained model from TensorFlow. The question does not depend on any specific context or additional information, so it is context-independent.\\n\\n  \n",
       "4                                                                                                                         The question is asking about the corpus used to pre-train the CANINE model. The answer is not dependent on any specific context, as CANINE is a well-known model and its pre-training corpus is a fixed property of the model.\\n\\n  \n",
       "5                                                                                                                                                                                                                                             The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n  \n",
       "6                                             The question is clear and concise, and it is not dependent on any specific context or document. The question is asking about replacing the checkpoint with a Hugging Face model that supports large model loading, such as BLOOM, which is a well-known model in the field of natural language processing.\\n\\n  \n",
       "7   The question is about a specific model used for a specific task, and it is clear that the model is from Microsoft Research. The name of the model is not a common name, but it is a name that is specific to the model, and it is not a name that is used for other models. The question is clear and does not depend on any additional information.\\n\\n  \n",
       "8                                                                                                                                                                                                                                             The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n  \n",
       "9                                                                                                                                                                                                                                             The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n  \n",
       "10                                                                                                                                                                                                                                                       The question is asking for the default value of a variable, which is a self-contained question.\\n\\n  \n",
       "11                                                                                                     The question is clear and concise, and it does not depend on any additional information to be understood. It is asking about the number of new tokens added to a model, which is a technical concept that is well-defined and easy to understand.\\n\\n  \n",
       "12                                                                                                                                   The question is asking about the area of the Amazon rainforest that is covered by the basin. It is a context-independent question, as it is clear what the Amazon rainforest and the Amazon basin are referring to.\\n\\n  \n",
       "13                                                                                                                                 The question is asking about the name of the SDK that Graphcore offers for easy use with IPU. This question is context-independant, as it is clear to an operator with documentation what the question is asking about.\\n  \n",
       "14                                                                                                                                                                                                                                                                                                                                                       NaN  \n",
       "15                                                                                                                                                    The question is asking for the architecture of a specific model, gluon\\_resnext50\\_32x4d, and provides enough context for me to understand that it is a convolutional neural network architecture.\\n\\n  \n",
       "16                                                                                                                                           The question is asking for the number of parameters in a specific model, vit\\_giant\\_patch16\\_384. The answer is a number, not a range or a list of models, so it is clear what the question is asking for.\\n\\n  \n",
       "17                                                                                                                                                                                                                                            The question is asking for the name of the model, which is a common noun and does not depend on any context.\\n  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(len(outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Evaluations saved to ./updated_initial_QAs_evals.json\n"
     ]
    }
   ],
   "source": [
    "# Save outputs to a JSON file\n",
    "output_file_path = './updated_initial_QAs_evals.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Generated QA Evaluations saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQv36Y_f9jVO"
   },
   "source": [
    "Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "oBWuOu1b9jVO",
    "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n</td>\n",
       "      <td>The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much does a 48 cores machine cost per hour?\\n</td>\n",
       "      <td>4.848$/h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>The model [huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b) is released under the llama-license.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the number of parameters in tf_efficientnet_es?\\n</td>\n",
       "      <td>5440000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What corpus is CANINE pre-trained on?\\n</td>\n",
       "      <td>CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>A dictionary key must be used to index a key containing a number or special character in an AttributeDictionary.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What other library can I use to replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM?\\n</td>\n",
       "      <td>You can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM by specifying it in the `pipeline` function, like so: `pipeline(model=\"your_model_name\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the name of the model from Microsoft Research that is used for table extraction from unstructured documents?\\n</td>\n",
       "      <td>Table Transformer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>If you encounter any problems or have any questions about the Hugging Face Robust Speech Challenge, you should use the Hugging Face Discord server.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the default value for HF_HUB_</td>\n",
       "      <td>The default value for HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR is False.\\n\\n### HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR2\\n\\nOutput:::\\nFactoid question: What is the default value for HF_HUB_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How many new tokens are added to the model?\\n</td>\n",
       "      <td>27 new tokens are added to the model.\\n\\n\\n```python\\n\\n```</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How many square kilometers of the Amazon rainforest are covered by the basin?\\n</td>\n",
       "      <td>5,500,000 square kilometers of the Amazon rainforest are covered by the basin.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the name of the SDK that Graphcore offers for easy use with IPU?\\n</td>\n",
       "      <td>Poplar</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the format of the annotations expected by the image_processor?\\n</td>\n",
       "      <td>The annotations are expected in the format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the architecture of gluon\\_resnext50\\_32x4d?\\n</td>\n",
       "      <td>The architecture of gluon\\_resnext50\\_32x4d includes 1x1 Convolution, Batch Normalization, Convolution, Global Average Pooling, Grouped Convolution, Max Pooling, ReLU, ResNeXt Block, Residual Connection, and Softmax.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n</td>\n",
       "      <td>1,024,116,960</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>codeparrot/codeparrot-small</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        question  \\\n",
       "0                                                What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n   \n",
       "1                                                                                              How much does a 48 cores machine cost per hour?\\n   \n",
       "2                                                                                                                      (your factoid question)\\n   \n",
       "3                                                                                      What is the number of parameters in tf_efficientnet_es?\\n   \n",
       "4                                                                                                        What corpus is CANINE pre-trained on?\\n   \n",
       "5                                                                                                                      (your factoid question)\\n   \n",
       "6   What other library can I use to replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM?\\n   \n",
       "7                         What is the name of the model from Microsoft Research that is used for table extraction from unstructured documents?\\n   \n",
       "8                                                                                                                      (your factoid question)\\n   \n",
       "9                                                                                                                      (your factoid question)\\n   \n",
       "10                                                                                                         What is the default value for HF_HUB_   \n",
       "11                                                                                                 How many new tokens are added to the model?\\n   \n",
       "12                                                               How many square kilometers of the Amazon rainforest are covered by the basin?\\n   \n",
       "13                                                                    What is the name of the SDK that Graphcore offers for easy use with IPU?\\n   \n",
       "14                                                                      What is the format of the annotations expected by the image_processor?\\n   \n",
       "15                                                                                        What is the architecture of gluon\\_resnext50\\_32x4d?\\n   \n",
       "16                                                                     What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n   \n",
       "17                                                                                                                     (your factoid question)\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                  answer  \\\n",
       "0                                                                                                                                                                      The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "1                                                                                                                                                                                                                                                               4.848$/h   \n",
       "2                                                                                                                                                     The model [huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b) is released under the llama-license.   \n",
       "3                                                                                                                                                                                                                                                                5440000   \n",
       "4                                                                                     CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with   \n",
       "5                                                                                                                                                       A dictionary key must be used to index a key containing a number or special character in an AttributeDictionary.   \n",
       "6   You can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM by specifying it in the `pipeline` function, like so: `pipeline(model=\"your_model_name\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})`.   \n",
       "7                                                                                                                                                                                                                                                      Table Transformer   \n",
       "8                                                                                                                    If you encounter any problems or have any questions about the Hugging Face Robust Speech Challenge, you should use the Hugging Face Discord server.   \n",
       "9                                                                                                            The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.   \n",
       "10                                                                               The default value for HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR is False.\\n\\n### HF_HUB_USE_TQDM_INTERNAL_PROGRESSBAR2\\n\\nOutput:::\\nFactoid question: What is the default value for HF_HUB_   \n",
       "11                                                                                                                                                                                                           27 new tokens are added to the model.\\n\\n\\n```python\\n\\n```   \n",
       "12                                                                                                                                                                                        5,500,000 square kilometers of the Amazon rainforest are covered by the basin.   \n",
       "13                                                                                                                                                                                                                                                                Poplar   \n",
       "14                                                                                                                          The annotations are expected in the format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation.   \n",
       "15                                              The architecture of gluon\\_resnext50\\_32x4d includes 1x1 Convolution, Batch Normalization, Convolution, Global Average Pooling, Grouped Convolution, Max Pooling, ReLU, ResNeXt Block, Residual Connection, and Softmax.   \n",
       "16                                                                                                                                                                                                                                                         1,024,116,960   \n",
       "17                                                                                                                                                                                                                                           codeparrot/codeparrot-small   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                  5.0              4.0               5.0  \n",
       "1                  1.0              1.0               5.0  \n",
       "2                  1.0              5.0               5.0  \n",
       "3                  5.0              NaN               5.0  \n",
       "4                  5.0              5.0               5.0  \n",
       "5                  1.0              5.0               5.0  \n",
       "6                  5.0              NaN               5.0  \n",
       "7                  1.0              2.0               5.0  \n",
       "8                  NaN              5.0               5.0  \n",
       "9                  5.0              5.0               5.0  \n",
       "10                 NaN              NaN               5.0  \n",
       "11                 3.0              5.0               5.0  \n",
       "12                 5.0              1.0               5.0  \n",
       "13                 3.0              1.0               5.0  \n",
       "14                 3.0              NaN               NaN  \n",
       "15                 3.0              NaN               5.0  \n",
       "16                 5.0              5.0               5.0  \n",
       "17                 2.0              5.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n</td>\n",
       "      <td>The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What corpus is CANINE pre-trained on?\\n</td>\n",
       "      <td>CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(your factoid question)\\n</td>\n",
       "      <td>The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n</td>\n",
       "      <td>1,024,116,960</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           question  \\\n",
       "0   What dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis?\\n   \n",
       "4                                                           What corpus is CANINE pre-trained on?\\n   \n",
       "9                                                                         (your factoid question)\\n   \n",
       "16                        What is the number of parameters in the vit\\_giant\\_patch16\\_384 model?\\n   \n",
       "\n",
       "                                                                                                                                                                                answer  \\\n",
       "0                                                                                    The IMDB dataset is used in this tutorial to fine-tune a DistilBERT model for sentiment analysis.   \n",
       "4   CANINE is pre-trained on the CC100 corpus.\\n\\nThe CANINE model is a Transformer-based model that uses a downsampling strategy to handle the longer sequence lengths that come with   \n",
       "9                          The main opposition between pickle (PyTorch) and H5 (Tensorflow) formats is that pickle does not support zero-copy or lazy loading, while H5 supports both.   \n",
       "16                                                                                                                                                                       1,024,116,960   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                  5.0              4.0               5.0  \n",
       "4                  5.0              5.0               5.0  \n",
       "9                  5.0              5.0               5.0  \n",
       "16                 5.0              5.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(len(generated_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaOMZyu69jVO"
   },
   "source": [
    "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "We have generated only a few QA couples here to reduce time and cost. But let's kick start the next part by loading a pre-generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Q3RRz4W79jVO"
   },
   "outputs": [],
   "source": [
    "#eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5s19uTd9jVO"
   },
   "source": [
    "# 2. Build our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-mET8Dy9jVO"
   },
   "source": [
    "### 2.1. Preprocessing documents to build our vector database\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    "\n",
    "Many options exist for text splitting:\n",
    "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    "- split after `n` words / character, but only on sentence boundaries\n",
    "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    "\n",
    "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "üí° _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "H4fhm55Q9jVO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:00<00:00, 64416.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sz9Jw2_q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzBYfNG79jVO"
   },
   "source": [
    "### 2.2. Retriever - embeddings üóÇÔ∏è\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "üõ†Ô∏è __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "LqJlIDZR9jVO"
   },
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6y1mQJX9jVO"
   },
   "source": [
    "### 2.3. Reader - LLM üí¨\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "üõ†Ô∏è Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9PdpuWyP9jVP"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `HF_API_TOKEN = \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9SDqenld9jVP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35627/1783810473.py:7: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  READER_LLM = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QZ62CbcZ9jVP"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiygbqfT9jVP"
   },
   "source": [
    "# 3. Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    "\n",
    "To this end, __we setup a judge agent__. ‚öñÔ∏èü§ñ\n",
    "\n",
    "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
    "\n",
    "üí° _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
    "\n",
    "üí° _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VrlMh_ZI9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Ae-3KWzK9jVP"
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `OPEN_AI_KEY = \"\"`\n",
    "\n",
    "- Other chat models can be loaded following the documentation for [langchain here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/quick_start/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ia9Mvn859jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "eval_chat_model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", api_key=ANTHROPIC_API_KEY)\n",
    "evaluator_name = \"Claude-3-Sonnet\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXH-szLe9jVP"
   },
   "source": [
    "üöÄ Let's run the tests and evaluate answers!üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "jW2nnvUT9jVQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35627/2918242049.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found, generating it...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awkwabear/anaconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/awkwabear/anaconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.32it/s]\n",
      "/tmp/ipykernel_35627/1514433945.py:37: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm(final_prompt)\n",
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:04<00:14,  4.80s/it]/home/awkwabear/anaconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s]\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:09<00:09,  4.78s/it]/home/awkwabear/anaconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.11it/s]\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:14<00:04,  4.65s/it]/home/awkwabear/anaconda3/envs/hf/lib/python3.12/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:18<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:23<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:20<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "                if rerank\n",
    "                else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tytXV5-h9jVT"
   },
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "D4YDSfmr9jVT"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CdkXMNvS9jVT"
   },
   "outputs": [],
   "source": [
    "# Remove rows with N/A values before processing\n",
    "result = result[result[\"eval_score_Claude-3-Sonnet\"].str.strip().str.lower() != \"n/a\"]\n",
    "\n",
    "# Clean the string by removing non-numeric characters before converting\n",
    "result[\"eval_score_Claude-3-Sonnet\"] = result[\"eval_score_Claude-3-Sonnet\"].apply(\n",
    "    lambda x: int(''.join(filter(str.isdigit, str(x)))) if ''.join(filter(str.isdigit, str(x))) else 1\n",
    ")\n",
    "result[\"eval_score_Claude-3-Sonnet\"] = (result[\"eval_score_Claude-3-Sonnet\"] - 1) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lgxBpid29jVT",
    "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json                                                                                                                                1.0\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json     35008190938746880722365054927385957415890468782975680843178589832557389502595928731692804194219110874366581872355500670058496.0\n",
       "Name: eval_score_Claude-3-Sonnet, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_Claude-3-Sonnet\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['settings', 'eval_score_Claude-3-Sonnet'], dtype='object')\n",
      "                                                                                             settings  \\\n",
      "0  ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json   \n",
      "1   ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json   \n",
      "\n",
      "                                                                                                        eval_score_Claude-3-Sonnet  \n",
      "0                                                                                                                              1.0  \n",
      "1  35008190938746880722365054927385957415890468782975680843178589832557389502595928731692804194219110874366581872355500670058496.0  \n"
     ]
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[[\"eval_score_Claude-3-Sonnet\"]].mean().reset_index()\n",
    "print(average_scores.columns)\n",
    "print(average_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Accuracy=%{y}<br>Configuration=%{x}<extra></extra>",
         "legendgroup": "1.0",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "1.0",
         "offsetgroup": "1.0",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "./output/rag<br>Chunk Size: 200<br>Embeddings: thenlper/gte-small<br>Rerank: False<br>Reader Model: zephyr-7b-beta.json"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Accuracy=%{y}<br>Configuration=%{x}<extra></extra>",
         "legendgroup": "3.500819093874688e+124",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "3.500819093874688e+124",
         "offsetgroup": "3.500819093874688e+124",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "./output/rag<br>Chunk Size: 200<br>Embeddings: thenlper/gte-small<br>Rerank: True<br>Reader Model: zephyr-7b-beta.json"
         ],
         "xaxis": "x",
         "y": [
          3.500819093874688e+124
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "showscale": false
        },
        "font": {
         "size": 11
        },
        "height": 600,
        "legend": {
         "title": {
          "text": "Accuracy"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Accuracy of Different RAG Configurations for 70 Filtered Q/A Pairs evaluated by claude-3-sonnet-20240229</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "RAG Settings"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          3.500819093874688e+124,
          1
         ],
         "categoryorder": "array",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Function to format the settings\n",
    "def format_settings(settings):\n",
    "    formatted = settings.replace(\"chunk:\", \"Chunk Size: \") \\\n",
    "                        .replace(\"embeddings:\", \"Embeddings: \") \\\n",
    "                        .replace(\"rerank:\", \"Rerank: \") \\\n",
    "                        .replace(\"reader-model:\", \"Reader Model: \") \\\n",
    "                        .replace(\"evaluator-model:\", \"Evaluator Model: \") \\\n",
    "                        .replace(\"~\", \"/\")  # Replace any special characters as needed\n",
    "    return formatted.replace(\"_\", \"<br>\")  # Replace underscores with line breaks for better formatting\n",
    "\n",
    "# Assuming 'average_scores' is a DataFrame that contains 'settings' and 'eval_score_GPT4'\n",
    "# Create a new column with formatted settings\n",
    "average_scores['formatted_settings'] = average_scores['settings'].apply(format_settings)\n",
    "\n",
    "# Now use 'formatted_settings' in the plot\n",
    "fig = px.bar(\n",
    "    average_scores,\n",
    "    x='formatted_settings',  # Use the formatted settings for x-axis\n",
    "    y='eval_score_Claude-3-Sonnet',     # Use the evaluation scores for y-axis\n",
    "    labels={\n",
    "        \"eval_score_Claude-3-Sonnet\": \"Accuracy\",  # Y-axis label\n",
    "        \"formatted_settings\": \"Configuration\",  # X-axis label\n",
    "    },\n",
    "    color='eval_score_Claude-3-Sonnet',  # Color based on the evaluation score\n",
    "    color_continuous_scale=\"bluered\",  # Color scale\n",
    ")\n",
    "\n",
    "# Update layout settings\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 1],  # Adjusting range to [0, 1] since scores are normalized\n",
    "    title=\"<b>Accuracy of Different RAG Configurations for 70 Filtered Q/A Pairs evaluated by claude-3-sonnet-20240229</b>\",\n",
    "    xaxis_title=\"RAG Settings\",\n",
    "    font=dict(size=11),\n",
    ")\n",
    "\n",
    "# Add percentage suffix to the y-axis\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)  # Hide the color scale\n",
    "fig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"outside\")  # Display percentage on bars\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSPH9DYI9jVT"
   },
   "source": [
    "## Example results\n",
    "\n",
    "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
    "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
    "\n",
    "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
    "\n",
    "‚û°Ô∏è ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVOxatv99jVT"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqK0Dg2Q9jVT"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPUOMWGk9jVT"
   },
   "source": [
    "\n",
    "\n",
    "As you can see, these had varying impact on performance. In particular, tuning the chunk size is both easy and very impactful.\n",
    "\n",
    "But this is our case: your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options! üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
