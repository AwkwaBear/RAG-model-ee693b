{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YErqpfH9jVI"
   },
   "source": [
    "# CLAUDE RAG Evaluation\n",
    "Modified Notebook Authored by:\n",
    "\n",
    "- [Anthony Gasbarro](https://github.com/AwkwaBear/RAG-model-ee693b)\n",
    "- Chris Aguilar\n",
    "- Maxwell Pauly\n",
    "- (_Original by: [Aymeric Roucher](https://huggingface.co/m-ric)_)\n",
    "\n",
    "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "\n",
    "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
    "So let's see how to evaluate our RAG system.\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    "\n",
    "For our evaluation pipeline, we will need:\n",
    "1. An evaluation dataset with question - answer couples (QA couples)\n",
    "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    "\n",
    "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
    "1. The evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
    "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ü§ñ will then perform the evaluation on this synthetic dataset.\n",
    "\n",
    "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Conda Environment Setup\n",
    "- In command line:\n",
    "    - `conda create -n <PICK SOME ENVIRONMENT NAME>`\n",
    "    - `conda install python=3.12 pytorch pytorch-cuda transformers accelerate sentence-transformers faiss-gpu openpyxl python-dotenv -c pytorch -c nvidia -c conda-forge -y`\n",
    "- Create open this notebook in VScode and set the jupyter interpreter to `<PICK SOME ENVIRONMENT NAME>`\n",
    "\n",
    "## Run the pip install below for the rest of the required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bCKBvOcp9jVK"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain langchain-anthropic sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille ipywidgets jupyter plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k_lJFbYm9jVL"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm.auto import tqdm # importing a library for progress bars\n",
    "import pandas as pd # pandas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "from huggingface_hub import notebook_login \n",
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from huggingface_hub import InferenceClient\n",
    "import random\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import glob\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment variables from .env file\n",
    "- Loads API keys etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will print true if the .env file is loaded, false if not\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "print(\"Will print true if the .env file is loaded, false if not\")\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# print(f\"Hugging Face API Token: {os.getenv(\"HF_API_TOKEN\")} \")\n",
    "# print(f\"OpenAI API Key: {os.getenv(\"OPENAI_API_KEY\")} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oIlNZ1Mn9jVL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a library for data manipulation\n",
    "from typing import Optional, List, Tuple # importing some type hints\n",
    "import json # importing a library for working with json\n",
    "import datasets # importing the datasets library\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) # setting the maximum column width for pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Hugging Face Doc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# File path for the dataset\n",
    "path_hf_doc_dataset = \"./hf_docs_dataset/huggingface_doc_dataset.json\"\n",
    "\n",
    "\n",
    "def recreate_dataset():\n",
    "    # Log in to the Hugging Face Hub\n",
    "    hf_api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "    if not hf_api_token:\n",
    "        raise EnvironmentError(\"HF_API_TOKEN is not set in the environment.\")\n",
    "    login(hf_api_token)\n",
    "\n",
    "    # Load the dataset from Hugging Face\n",
    "    ds = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "    # Convert the dataset to a list of dictionaries\n",
    "    data_list = [doc for doc in ds]\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path_hf_doc_dataset), exist_ok=True)\n",
    "\n",
    "    # Write the dataset content to a JSON file\n",
    "    with open(path_hf_doc_dataset, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Dataset saved to {path_hf_doc_dataset}\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(path_hf_doc_dataset) and os.path.getsize(path_hf_doc_dataset) > 0:\n",
    "    try:\n",
    "        with open(path_hf_doc_dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "            data_list = json.load(f)\n",
    "    except (json.JSONDecodeError, OSError) as e:\n",
    "        print(f\"Error loading JSON file: {e}. Recreating dataset.\")\n",
    "        data_list = recreate_dataset()\n",
    "else:\n",
    "    data_list = recreate_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy9CKj0M9jVM"
   },
   "source": [
    "# 1. Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
    "\n",
    "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkoEgiDg9jVM"
   },
   "source": [
    "### 1.1. Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3gTOlRKO9jVM"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a0aade89a84484804370d7c3986f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(data_list)\n",
    "]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjrNhcCh9jVN"
   },
   "source": [
    "### 1.2. Setup agents for question generation\n",
    "\n",
    "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GoRySj3Q9jVN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "def call_llm(inference_client, prompt):\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": 1000},\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Decode the raw response bytes to a string\n",
    "        response_str = response.decode(\"utf-8\")\n",
    "\n",
    "        # Parse and validate the response JSON\n",
    "        response_data = json.loads(response_str)\n",
    "        if not isinstance(response_data, list) or not response_data or \"generated_text\" not in response_data[0]:\n",
    "            logging.error(f\"Unexpected response format: {response_data}\")\n",
    "            raise ValueError(f\"Invalid response format: {response_data}\")\n",
    "\n",
    "        # Return the generated text\n",
    "        return response_data[0][\"generated_text\"]\n",
    "\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        logging.error(f\"JSON decoding error: {json_err}. Response: {response}\")\n",
    "        raise\n",
    "    except AttributeError as attr_err:\n",
    "        logging.error(f\"Attribute error: {attr_err}. Ensure `response` is correctly handled.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in call_llm: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "#     response = inference_client.post(\n",
    "#         json={\n",
    "#             \"inputs\": prompt,\n",
    "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
    "#             \"task\": \"text-generation\",\n",
    "#         },\n",
    "#     )\n",
    "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hIM_DJRo9jVN"
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVFc-lVy9jVN"
   },
   "source": [
    "Now let's generate our QA couples.\n",
    "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
    "\n",
    "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8fteqDDD9jVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 300 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140c2c2f44c74fea8bbb7a1b14351209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:AssertionError: Answer is too long: The value of epsilon is 0.05 during the evaluation.\n",
      "\n",
      "    For step in the maximum number of steps per episode:\n",
      "\n",
      "        Get the current state\n",
      "        Choose an action based on the current state and epsilon\n",
      "        Take the action and get the reward and the new state\n",
      "        Update the total reward for the episode\n",
      "\n",
      "    Compute the average reward for the episode\n",
      "    Print the average reward for the episode\n",
      "```\n",
      "\n",
      "```python\n",
      "def evaluate_q_learning(env, Qtable, n_eval_episodes, max_steps, min_epsilon):\n",
      "    \"\"\"\n",
      "    This function evaluates the Q-learning algorithm.\n",
      "\n",
      "    Args:\n",
      "        env (gym.Env): The environment to evaluate the Q-learning algorithm. Context: return action\n",
      "```\n",
      "\n",
      "#### Solution\n",
      "\n",
      "```python\n",
      "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
      "    # Randomly generate a number between 0 and 1\n",
      "    random_num = random.uniform(0, 1)\n",
      "    # if random_num > greater than epsilon --> exploitation\n",
      "    if random_num > epsilon:\n",
      "        # Take the action with the highest value given a state\n",
      "        # np.argmax can be useful here\n",
      "        action = greedy_policy(Qtable, state)\n",
      "    # else --> exploration\n",
      "    else:\n",
      "        action = env.action_space.sample()\n",
      "\n",
      "    return action\n",
      "```\n",
      "\n",
      "## Define the hyperparameters ‚öôÔ∏è\n",
      "\n",
      "The exploration related hyperparamters are some of the most important ones.\n",
      "\n",
      "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
      "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem.\n",
      "\n",
      "```python\n",
      "# Training parameters\n",
      "n_training_episodes = 10000  # Total training episodes\n",
      "learning_rate = 0.7  # Learning rate\n",
      "\n",
      "# Evaluation parameters\n",
      "n_eval_episodes = 100  # Total number of test episodes\n",
      "\n",
      "# Environment parameters\n",
      "env_id = \"FrozenLake-v1\"  # Name of the environment\n",
      "max_steps = 99  # Max steps per episode\n",
      "gamma = 0.95  # Discounting rate\n",
      "eval_seed = []  # The evaluation seed of the environment\n",
      "\n",
      "# Exploration parameters\n",
      "max_epsilon = 1.0  # Exploration probability at start\n",
      "min_epsilon = 0.05  # Minimum exploration probability\n",
      "decay_rate = 0.0005  # Exponential decay rate for exploration prob\n",
      "```\n",
      "\n",
      "## Create the training loop method\n",
      "\n",
      "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
      "\n",
      "The training loop goes like this:\n",
      "\n",
      "```\n",
      "For episode in the total of training episodes:\n",
      "\n",
      "Reduce epsilon (since we need less and less exploration)\n",
      "Reset the environment\n",
      "ERROR:root:AssertionError: Answer is too long: The output directory of the training arguments is \"MariaK/vilt_finetuned_200\".\n",
      "\n",
      ">>> trainer = Trainer(\n",
      "...     model=model,\n",
      "...     args=training_args,\n",
      "...     train_dataset=processed_dataset,\n",
      "...     data_collator=data_collator,\n",
      "...     tokenizer=tokenizer,\n",
      "... )\n",
      "```\n",
      "\n",
      "3. Train the model.\n",
      "\n",
      "```py\n",
      ">>> trainer.train()\n",
      "```\n",
      "\n",
      "After training, you can evaluate your model on the validation set:\n",
      "\n",
      "```py\n",
      ">>> eval_results = trainer.evaluate()\n",
      ">>> print(eval_results)\n",
      "```\n",
      "\n",
      "You can also save the model to the hub:\n",
      "\n",
      "```py\n",
      ">>> trainer.push_to_hub(commit_message=\"End of training\")\n",
      "```\n",
      "\n",
      "Congratulations! You‚Äôve just fine-tuned a model for visual question answering!. Context: ...     encoding[\"labels\"] = targets\n",
      "    \n",
      "...     return encoding\n",
      "```\n",
      "\n",
      "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.map`] function. You can speed up `map` by \n",
      "setting `batched=True` to process multiple elements of the dataset at once. At this point, feel free to remove the columns you don't need.\n",
      "\n",
      "```py\n",
      ">>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])\n",
      ">>> processed_dataset\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],\n",
      "    num_rows: 200\n",
      "})\n",
      "```\n",
      "\n",
      "As a final step, create a batch of examples using [`DefaultDataCollator`]:\n",
      "\n",
      "```py\n",
      ">>> from transformers import DefaultDataCollator\n",
      "\n",
      ">>> data_collator = DefaultDataCollator()\n",
      "```\n",
      "\n",
      "## Train the model\n",
      "\n",
      "You‚Äôre ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels \n",
      "along with the label mappings:\n",
      "\n",
      "```py\n",
      ">>> from transformers import ViltForQuestionAnswering\n",
      "\n",
      ">>> model = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
      "```\n",
      "\n",
      "At this point, only three steps remain:\n",
      "\n",
      "1. Define your training hyperparameters in [`TrainingArguments`]:\n",
      "\n",
      "```py\n",
      ">>> from transformers import TrainingArguments\n",
      "\n",
      ">>> repo_id = \"MariaK/vilt_finetuned_200\"\n",
      "\n",
      ">>> training_args = TrainingArguments(\n",
      "...     output_dir=repo_id,\n",
      "...     per_device_train_batch_size=4,\n",
      "...     num_train_epochs=20,\n",
      "...     save_steps=200,\n",
      "...     logging_steps=50,\n",
      "...     learning_rate=5e-5,\n",
      "...     save_total_limit=2,\n",
      "...     remove_unused_columns=False,\n",
      "...     push_to_hub=True,\n",
      "... )\n",
      "```\n",
      "\n",
      "2. Pass the training arguments to [`Trainer`] along with the model, dataset, processor, and data collator.\n",
      "\n",
      "```py\n",
      ">>> from transformers import Trainer\n",
      "ERROR:root:AssertionError: Answer is too long: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: We can see it works with `Dataset.map()` without us needing to remove the old columns:\n",
      "\n",
      "```py\n",
      "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
      "tokenized_dataset\n",
      "```\n",
      "\n",
      "```python out\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
      "        num_rows: 206772\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
      "        num_rows: 68876\n",
      "    })\n",
      "})\n",
      "```\n",
      "\n",
      "We get the same number of training features as before, but here we've kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.\n",
      "\n",
      "You've now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs,\n",
      "there may be times when you'll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let's take a look at how this works.\n",
      "\n",
      "## From `Dataset`s to `DataFrame`s and back[[from-datasets-to-dataframes-and-back]]\n",
      "\n",
      "<Youtube id=\"tfcY1067A5Q\"/>\n",
      "\n",
      "To enable the conversion between various third-party libraries, ü§ó Datasets provides a `Dataset.set_format()` function. This function only changes the _output format_ of the dataset, so you can easily switch to another format without affecting the underlying _data format_, which is Apache Arrow. The formatting is done in place. To demonstrate, let's convert our dataset to Pandas:\n",
      "\n",
      "```py\n",
      "drug_dataset.set_format(\"pandas\")\n",
      "```\n",
      "\n",
      "Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary:\n",
      "\n",
      "Output:::\n",
      "\n",
      "```py\n",
      "drug_dataset[\"train\"][0]\n",
      "```\n",
      "\n",
      "```python out\n",
      "  attention_mask  condition  ...  review_length  token_type_ids  usefulCount\n",
      "0              1        0.0  ...               15              0             0\n",
      "\n",
      "[1 rows x 11 columns]\n",
      "```\n",
      "\n",
      "To convert back to the dictionary format, we can simply call `set_format()` again:\n",
      "\n",
      "```py\n",
      "drug_dataset.set_format(\"dict\")\n",
      "```\n",
      "\n",
      "Output:::\n",
      "\n",
      "```py\n",
      "drug_dataset[\"train\"][0]\n",
      "```\n",
      "\n",
      "```python out\n",
      "{'attention_mask': 1, 'condition': 0.0, 'date': '2018-01-01', 'drugName': 'acetaminophen', 'input_ids': tensor([   101,  15571,  14278,  19966,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,\n",
      "         14278,  14135,  14278,  14135,  14278,  14135,  14278,  14135,  14278,\n",
      "         14135,  14278,  14135,  14. Context: We can see it works with `Dataset.map()` without us needing to remove the old columns:\n",
      "\n",
      "```py\n",
      "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
      "tokenized_dataset\n",
      "```\n",
      "\n",
      "```python out\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
      "        num_rows: 206772\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
      "        num_rows: 68876\n",
      "    })\n",
      "})\n",
      "```\n",
      "\n",
      "We get the same number of training features as before, but here we've kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.\n",
      "\n",
      "You've now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs,\n",
      "there may be times when you'll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let's take a look at how this works.\n",
      "\n",
      "## From `Dataset`s to `DataFrame`s and back[[from-datasets-to-dataframes-and-back]]\n",
      "\n",
      "<Youtube id=\"tfcY1067A5Q\"/>\n",
      "\n",
      "To enable the conversion between various third-party libraries, ü§ó Datasets provides a `Dataset.set_format()` function. This function only changes the _output format_ of the dataset, so you can easily switch to another format without affecting the underlying _data format_, which is Apache Arrow. The formatting is done in place. To demonstrate, let's convert our dataset to Pandas:\n",
      "\n",
      "```py\n",
      "drug_dataset.set_format(\"pandas\")\n",
      "```\n",
      "\n",
      "Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary:\n",
      "ERROR:root:AssertionError: Answer is too long: To embed a W&B report in a Gradio application, you need to create a W&B report, make it public, and wrap the URL within an iFrame like this: `<iframe src={url} style=\"border:none;height:1024px;width:100%\">`. Then, you can use the `gr.HTML` block in Gradio to display the report. Here's an example:\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "def wandb_report(url):\n",
      "    iframe = f'<iframe src={url} style=\"border:none;height:1024px;width:100%\">'\n",
      "    return gr.HTML(iframe)\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n",
      "    report = wandb_report(report_url)\n",
      "\n",
      "demo.launch(share=True)\n",
      "```. Context: ```python\n",
      "\n",
      "   demo.integrate(wandb=wandb)\n",
      "   ```\n",
      "\n",
      "   Ë∞ÉÁî®ÈõÜÊàê‰πãÂêéÔºåÂ∞ÜÂàõÂª∫‰∏Ä‰∏™ÊºîÁ§∫ÔºåÊÇ®ÂèØ‰ª•Â∞ÜÂÖ∂ÈõÜÊàêÂà∞‰ª™Ë°®ÊùøÊàñÊä•Âëä‰∏≠\n",
      "\n",
      "   Âú® W&B ‰πãÂ§ñÔºå‰ΩøÁî® gradio-app Ê†áËÆ∞ÂÖÅËÆ∏‰ªª‰Ωï‰∫∫Áõ¥Êé•Â∞Ü Gradio ÊºîÁ§∫ÂµåÂÖ•Âà∞ÂÖ∂ÂçöÂÆ¢„ÄÅÁΩëÁ´ô„ÄÅÊñáÊ°£Á≠â‰∏≠ÁöÑ HF spaces ‰∏ä :\n",
      "\n",
      "   ```html\n",
      "   &lt;gradio-app space=\"akhaliq/JoJoGAN\"&gt; &lt;gradio-app&gt;\n",
      "   ```\n",
      "\n",
      "7.ÔºàÂèØÈÄâÔºâÂú® Gradio Â∫îÁî®Á®ãÂ∫è‰∏≠ÂµåÂÖ• W&B Âõæ\n",
      "\n",
      "    ‰πüÂèØ‰ª•Âú® Gradio Â∫îÁî®Á®ãÂ∫è‰∏≠ÂµåÂÖ• W&B Âõæ„ÄÇ‰∏∫Ê≠§ÔºåÊÇ®ÂèØ‰ª•ÂàõÂª∫‰∏Ä‰∏™ W&B Êä•ÂëäÔºåÂπ∂Âú®‰∏Ä‰∏™ `gr.HTML` Âùó‰∏≠Â∞ÜÂÖ∂ÂµåÂÖ•Âà∞ Gradio Â∫îÁî®Á®ãÂ∫è‰∏≠„ÄÇ\n",
      "\n",
      "    Êä•ÂëäÈúÄË¶ÅÊòØÂÖ¨ÂºÄÁöÑÔºåÊÇ®ÈúÄË¶ÅÂú® iFrame ‰∏≠ÂåÖË£Ö URLÔºåÂ¶Ç‰∏ãÊâÄÁ§∫ :\n",
      "    The Report will need to be public and you will need to wrap the URL within an iFrame like this:\n",
      "    ```python\n",
      "\n",
      "    import gradio as gr\n",
      "\n",
      "    def wandb_report(url):\n",
      "        iframe = f'&lt;iframe src={url} style=\"border:none;height:1024px;width:100%\"&gt;'\n",
      "        return gr.HTML(iframe)\n",
      "\n",
      "    with gr.Blocks() as demo:\n",
      "        report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n",
      "        report = wandb_report(report_url)\n",
      "\n",
      "    demo.launch(share=True)\n",
      "    ```\n",
      "\n",
      "## ÁªìËÆ∫\n",
      "\n",
      "Â∏åÊúõÊÇ®ÂñúÊ¨¢Ê≠§ÂµåÂÖ• Gradio ÊºîÁ§∫Âà∞ W&B Êä•ÂëäÁöÑÁÆÄÁü≠ÊºîÁ§∫ÔºÅÊÑüË∞¢ÊÇ®‰∏ÄÁõ¥ÈòÖËØªÂà∞ÊúÄÂêé„ÄÇÂõûÈ°æ‰∏Ä‰∏ã :\n",
      "\n",
      "- ‰ªÖÈúÄË¶Å‰∏Ä‰∏™Âçï‰∏ÄÂèÇËÄÉÂõæÂÉèÂç≥ÂèØÂØπ JoJoGAN ËøõË°åÂæÆË∞ÉÔºåÈÄöÂ∏∏Âú® GPU ‰∏äÈúÄË¶ÅÁ∫¶ 1 ÂàÜÈíü„ÄÇËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Â∞ÜÊ†∑ÂºèÂ∫îÁî®‰∫é‰ªª‰ΩïËæìÂÖ•ÂõæÂÉè„ÄÇÂú®ËÆ∫Êñá‰∏≠ÈòÖËØªÊõ¥Â§öÂÜÖÂÆπ„ÄÇ\n",
      "\n",
      "- W&B ÂèØ‰ª•ÈÄöËøáÊ∑ªÂä†Âá†Ë°å‰ª£Á†ÅÊù•Ë∑üË∏™ÂÆûÈ™åÔºåÊÇ®ÂèØ‰ª•Âú®Âçï‰∏™ÈõÜ‰∏≠ÁöÑ‰ª™Ë°®Êùø‰∏≠ÂèØËßÜÂåñ„ÄÅÊéíÂ∫èÂíåÁêÜËß£ÊÇ®ÁöÑÂÆûÈ™å„ÄÇ\n",
      "\n",
      "- Gradio ÂàôÂú®Áî®Êà∑ÂèãÂ•ΩÁöÑÁïåÈù¢‰∏≠ÊºîÁ§∫Ê®°ÂûãÔºåÂèØ‰ª•Âú®ÁΩëÁªú‰∏ä‰ªª‰ΩïÂú∞ÊñπÂÖ±‰∫´„ÄÇ\n",
      "\n",
      "## Â¶Ç‰ΩïÂú® Wandb ÁªÑÁªáÁöÑ HF spaces ‰∏ä Ë¥°ÁåÆ Gradio ÊºîÁ§∫\n",
      "\n",
      "- Âú® Hugging Face ‰∏äÂàõÂª∫‰∏Ä‰∏™Â∏êÊà∑[Ê≠§Â§Ñ](https://huggingface.co/join)„ÄÇ\n",
      "- Âú®ÊÇ®ÁöÑÁî®Êà∑Âêç‰∏ãÊ∑ªÂä† Gradio ÊºîÁ§∫ÔºåËØ∑ÂèÇÈòÖ[Ê≠§ÊïôÁ®ã](https://huggingface.co/course/chapter9/4?fw=pt) ‰ª•Âú® Hugging Face ‰∏äËÆæÁΩÆ Gradio ÊºîÁ§∫„ÄÇ\n",
      "- Áî≥ËØ∑Âä†ÂÖ• wandb ÁªÑÁªá[Ê≠§Â§Ñ](https://huggingface.co/wandb)„ÄÇ\n",
      "- ÊâπÂáÜÂêéÔºåÂ∞ÜÊ®°Âûã‰ªéËá™Â∑±ÁöÑÁî®Êà∑ÂêçËΩ¨ÁßªÂà∞ Wandb ÁªÑÁªá‰∏≠„ÄÇ\n",
      "ERROR:root:AssertionError: Answer is too long: The name of the paper is 'High Fidelity Neural Audio Compression'.\n",
      "\n",
      "The EnCodec model is a neural codec that combines the benefits of traditional codecs and neural codecs.\n",
      "\n",
      "Traditional codecs are designed to be efficient and robust, but they often lack the ability to capture the nuances of human speech and music.\n",
      "\n",
      "Neural codecs, on the other hand, are designed to capture the nuances of human speech and music, but they often lack the efficiency and robustness of traditional codecs.\n",
      "\n",
      "The EnCodec model addresses these limitations by combining the benefits of both types of codecs.\n",
      "\n",
      "The EnCodec model consists of an encoder, a quantizer, and a decoder.\n",
      "\n",
      "The encoder takes in a sequence of audio samples and maps them to a sequence of latent representations.\n",
      "\n",
      "The quantizer then quantizes the latent representations to a fixed-length code.\n",
      "\n",
      "The decoder then takes in the fixed-length code and maps it back to a sequence of audio samples.\n",
      "\n",
      "The EnCodec model is trained using a combination of supervised and unsupervised learning.\n",
      "\n",
      "In supervised learning, the model is trained to minimize the reconstruction loss between the input audio samples and the output audio samples.\n",
      "\n",
      "In unsupervised learning, the model is trained to learn a compact and informative representation of the audio data.\n",
      "\n",
      "The EnCodec model achieves state-of-the-art performance on several audio compression benchmarks.\n",
      "\n",
      "The EnCodec model is also able to capture the nuances of human speech and music, making it well-suited for applications such as music streaming and teleconferencing.\n",
      "\n",
      "## Model Architecture\n",
      "\n",
      "The EnCodec model consists of an encoder, a quantizer, and a decoder.\n",
      "\n",
      "The encoder takes in a sequence of audio samples and maps them to a sequence of latent representations.\n",
      "\n",
      "The quantizer then quantizes the latent representations to a fixed-length code.\n",
      "\n",
      "The decoder then takes in the fixed-length code and maps it back to a sequence of audio samples.\n",
      "\n",
      "The encoder and decoder are both implemented as Transformer-based models.\n",
      "\n",
      "The encoder consists of a stack of Transformer layers, each of which contains a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
      "\n",
      "The decoder also consists of a stack of Transformer layers, but it also includes an encoder-decoder attention mechanism that allows it to attend to the output of the encoder.\n",
      "\n",
      "The quantizer is implemented as a simple nearest-neighbor quantization function.\n",
      "\n",
      "The EnCodec model also includes a number of other components, such as a learnable upsampling function and a learnable downsampling function, that are used to manipulate the audio data.\n",
      "\n",
      "## Training\n",
      "\n",
      "The EnCodec model is trained using a combination of supervised and unsupervised learning.\n",
      "\n",
      "In supervised learning, the model is trained to minimize the reconstruction loss between the input audio samples and the output audio samples.\n",
      "\n",
      "The reconstruction loss is calculated using a mean squared error (MSE) loss function.\n",
      "\n",
      "In unsupervised learning, the model is trained to learn a compact and informative representation of the audio data.\n",
      "\n",
      "The EnCodec model is trained using a combination of real-world audio data and synthetic audio data.\n",
      "\n",
      "The real-world audio data is obtained from a variety of sources, including music streaming services and teleconferencing platforms.\n",
      "\n",
      "The synthetic audio data is generated using a variety of audio synthesis techniques, such as additive synthesis and granular synthesis.\n",
      "\n",
      "The EnCodec model is trained using a distributed training setup, with multiple GPUs and multiple machines.\n",
      "\n",
      "The model is trained using the Adam optimizer with a learning rate of 1e-4.\n",
      "\n",
      "The model is trained for a total of 1 million iterations, with a batch size of 256.\n",
      "\n",
      "## Evaluation\n",
      "\n",
      "The EnCodec model is evaluated using several audio compression benchmarks.\n",
      "\n",
      "The model is evaluated on both objective and subjective metrics.\n",
      "\n",
      "The objective metrics include the reconstruction loss, the signal-to-noise ratio (SNR), and the log-spectral distance (LSD).\n",
      "\n",
      "The subjective metrics include the mean opinion score (MOS) and the perceptual evaluation of speech quality (PESQ).\n",
      "\n",
      "The EnCodec model achieves state-of-the-art performance on several audio compression benchmarks.\n",
      "\n",
      "The EnCodec model is also able to. Context: !--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# EnCodec\n",
      "\n",
      "## Overview\n",
      "\n",
      "The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre D√©fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "ERROR:root:AssertionError: Answer is too long: You can upload a model to the Hub using the `Trainer` API by setting `push_to_hub=True` when you define your `TrainingArguments`. The `Trainer` will then upload your model to the Hub each time it is saved in a repository in your namespace. The repository will be named like the output directory you picked but you can choose a different name with `hub_model_id = \"a_different_name\"`. To upload your model to an organization you are a member of, just pass it with `hub_model_id = \"my_organization/my_repo_name\"`.. Context: ## Using the `push_to_hub` API[[using-the-pushtohub-api]]\n",
      "\n",
      "{#if fw === 'pt'}\n",
      "\n",
      "<Youtube id=\"Zh0FfmVrKX0\"/>\n",
      "\n",
      "{:else}\n",
      "\n",
      "<Youtube id=\"pUh5cGmNV8Y\"/>\n",
      "\n",
      "{/if}\n",
      "\n",
      "The simplest way to upload files to the Hub is by leveraging the `push_to_hub` API.\n",
      "\n",
      "Before going further, you'll need to generate an authentication token so that the `huggingface_hub` API knows who you are and what namespaces you have write access to. Make sure you are in an environment where you have `transformers` installed (see [Setup](/course/chapter0)). If you are in a notebook, you can use the following function to login:\n",
      "\n",
      "```python\n",
      "from huggingface_hub import notebook_login\n",
      "\n",
      "notebook_login()\n",
      "```\n",
      "\n",
      "In a terminal, you can run:\n",
      "\n",
      "```bash\n",
      "huggingface-cli login\n",
      "```\n",
      "\n",
      "In both cases, you should be prompted for your username and password, which are the same ones you use to log in to the Hub. If you do not have a Hub profile yet, you should create one [here](https://huggingface.co/join).\n",
      "\n",
      "Great! You now have your authentication token stored in your cache folder. Let's create some repositories!\n",
      "\n",
      "{#if fw === 'pt'}\n",
      "\n",
      "If you have played around with the `Trainer` API to train a model, the easiest way to upload it to the Hub is to set `push_to_hub=True` when you define your `TrainingArguments`:\n",
      "\n",
      "```py\n",
      "from transformers import TrainingArguments\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    \"bert-finetuned-mrpc\", save_strategy=\"epoch\", push_to_hub=True\n",
      ")\n",
      "```\n",
      "\n",
      "When you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time it is saved (here every epoch) in a repository in your namespace. That repository will be named like the output directory you picked (here `bert-finetuned-mrpc`) but you can choose a different name with `hub_model_id = \"a_different_name\"`.\n",
      "\n",
      "To upload your model to an organization you are a member of, just pass it with `hub_model_id = \"my_organization/my_repo_name\"`.\n",
      "ERROR:root:AssertionError: Answer is too long: Fetch reduced processing latency by 50%.\n",
      "\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Fetch‚Äôs new ML pipeline cut processing latency by 50% and increased accuracy by 20%. By using Amazon SageMaker and Hugging Face, Fetch was able to process receipts faster and with significantly higher accuracy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ". Context: --\n",
      "title: \"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\"\n",
      "thumbnail: /blog/assets/78_ml_director_insights/fetch.png\n",
      "authors:\n",
      "- user: Violette\n",
      "---\n",
      "\n",
      "# Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\n",
      "\n",
      "\n",
      "_This article is a cross-post from an originally published post on September 2023 [on AWS's website](https://aws.amazon.com/fr/solutions/case-studies/fetch-case-study/)._\n",
      "\n",
      "\n",
      "## Overview\n",
      "\n",
      "Consumer engagement and rewards company [Fetch](https://fetch.com/) offers an application that lets users earn rewards on their purchases by scanning their receipts. The company also parses these receipts to generate insights into consumer behavior and provides those insights to brand partners. As weekly scans rapidly grew, Fetch needed to improve its speed and precision.\n",
      "\n",
      "On Amazon Web Services (AWS), Fetch optimized its machine learning (ML) pipeline using Hugging Face and [Amazon SageMaker ](https://aws.amazon.com/sagemaker/), a service for building, training, and deploying ML models with fully managed infrastructure, tools, and workflows. Now, the Fetch app can process scans faster and with significantly higher accuracy.\n",
      "\n",
      "\n",
      "## Opportunity | Using Amazon SageMaker to Accelerate an ML Pipeline in 12 Months for Fetch\n",
      "\n",
      "Using the Fetch app, customers can scan receipts, receive points, and redeem those points for gift cards. To reward users for receipt scans instantaneously, Fetch needed to be able to capture text from a receipt, extract the pertinent data, and structure it so that the rest of its system can process and analyze it. With over 80 million receipts processed per week‚Äîhundreds of receipts per second at peak traffic‚Äîit needed to perform this process quickly, accurately, and at scale.\n",
      "ERROR:root:AssertionError: Answer is too long: The name of the tokenizer used for CodeParrot is not mentioned in the context.\n",
      "\n",
      "\n",
      "\n",
      "We'll use the [GPT2Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.GPT2Tokenizer) from Hugging Face's Transformers library to tokenize our dataset. This tokenizer is based on the GPT-2 architecture and is able to handle the specifics of Python code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ". Context: --\n",
      "title: Training CodeParrot ü¶ú from Scratch\n",
      "thumbnail: /blog/assets/40_codeparrot/thumbnail.png\n",
      "authors:\n",
      "- user: leandro\n",
      "---\n",
      "\n",
      "# Training CodeParrot ü¶ú from Scratch\n",
      "\n",
      "\n",
      "\n",
      "In this blog post we'll take a look at what it takes to build the technology behind [GitHub CoPilot](https://copilot.github.com/), an application that provides suggestions to programmers as they code. In this step by step guide, we'll learn how to train a large GPT-2 model called CodeParrot ü¶ú, entirely from scratch. CodeParrot can auto-complete your Python code - give it a spin [here](https://huggingface.co/spaces/lvwerra/codeparrot-generation). Let's get to building it from scratch!\n",
      "\n",
      "![codeparrot](assets/40_codeparrot/codeparrot.png)\n",
      "\n",
      "## Creating a Large Dataset of Source Code\n",
      "The first thing we need is a large training dataset. With the goal to train a Python code generation model, we accessed the GitHub dump available on Google's BigQuery and filtered for all Python files. The result is a 180 GB dataset with 20 million files (available [here](http://hf.co/datasets/transformersbook/codeparrot)). After initial training experiments, we found that the duplicates in the dataset severely impacted the model performance. Further investigating the dataset we found that:\n",
      "\n",
      "- 0.1% of the unique files make up 15% of all files\n",
      "- 1% of the unique files make up 35% of all files\n",
      "- 10% of the unique files make up 66% of all files\n",
      "\n",
      "You can learn more about our findings in [this Twitter thread](https://twitter.com/lvwerra/status/1458470994146996225). We removed the duplicates and applied the same cleaning heuristics found in the [Codex paper](https://arxiv.org/abs/2107.03374). Codex is the model behind CoPilot and is a GPT-3 model fine-tuned on GitHub code. \n",
      "\n",
      "The cleaned dataset is still 50GB big and available on the Hugging Face Hub: [codeparrot-clean](http://hf.co/datasets/lvwerra/codeparrot-clean). With that we can setup a new tokenizer and train a model.\n",
      "\n",
      "## Initializing the Tokenizer and Model\n",
      "ERROR:root:AssertionError: Answer is too long: `transformers.onnx` is a separate package that provides a similar functionality to `optimum.onnxruntime`, but with some differences in usage and features.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "`transformers.onnx` and `optimum.onnxruntime` are both packages that allow you to export ü§ó Transformers models to ONNX.\n",
      "However, there are some differences between the two packages:\n",
      "\n",
      "* `transformers.onnx` is a separate package that provides a similar functionality to `optimum.onnxruntime`, but with some differences in usage and features.\n",
      "* `transformers.onnx` provides a command line interface for exporting models, while `optimum.onnxruntime` does not.\n",
      "* `transformers.onnx` provides more control over the ONNX export options, such as the ONNX opset version and the input/output names.\n",
      "* `optimum. Context: ```python\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
      ">>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
      ">>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n",
      ">>> outputs = model(**inputs)\n",
      "```\n",
      "\n",
      "The process is identical for TensorFlow checkpoints on the Hub. For instance, here's how you would\n",
      "export a pure TensorFlow checkpoint from the [Keras organization](https://huggingface.co/keras-io):\n",
      "\n",
      "```bash\n",
      "optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/\n",
      "```\n",
      "\n",
      "### Exporting a ü§ó Transformers model to ONNX with `optimum.onnxruntime`\n",
      "\n",
      "Alternative to CLI, you can export a ü§ó Transformers model to ONNX programmatically like so: \n",
      "\n",
      "```python\n",
      ">>> from optimum.onnxruntime import ORTModelForSequenceClassification\n",
      ">>> from transformers import AutoTokenizer\n",
      "\n",
      ">>> model_checkpoint = \"distilbert_base_uncased_squad\"\n",
      ">>> save_directory = \"onnx/\"\n",
      "\n",
      ">>> # Load a model from transformers and export it to ONNX\n",
      ">>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
      "\n",
      ">>> # Save the onnx model and tokenizer\n",
      ">>> ort_model.save_pretrained(save_directory)\n",
      ">>> tokenizer.save_pretrained(save_directory)\n",
      "```\n",
      "\n",
      "### Exporting a model for an unsupported architecture\n",
      "\n",
      "If you wish to contribute by adding support for a model that cannot be currently exported, you should first check if it is\n",
      "supported in [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview),\n",
      "and if it is not, [contribute to ü§ó Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)\n",
      "directly.\n",
      "\n",
      "### Exporting a model with `transformers.onnx`\n",
      "\n",
      "<Tip warning={true}>\n",
      "ERROR:root:AssertionError: Answer is too long: The main idea of the MAE approach is to mask random patches of the input image and reconstruct the missing pixels. This is achieved through an asymmetric encoder-decoder architecture, where the encoder operates only on the visible subset of patches and the decoder reconstructs the original image from the latent representation and mask tokens. The authors find that masking a high proportion of the input image yields a nontrivial and meaningful self-supervisory task, which enables efficient and effective training of large models that generalize well.. Context: The abstract from the paper is the following:\n",
      "\n",
      "*This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the\n",
      "input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates\n",
      "only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask\n",
      "tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs\n",
      "enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity\n",
      "models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream\n",
      "tasks outperforms supervised pre-training and shows promising scaling behavior.*\n",
      "\n",
      "<img src=\"https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png\"\n",
      "alt=\"drawing\" width=\"600\"/> \n",
      "\n",
      "<small> MAE architecture. Taken from the <a href=\"https://arxiv.org/abs/2111.06377\">original paper.</a> </small>\n",
      "\n",
      "This model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [sayakpaul](https://github.com/sayakpaul) and \n",
      "[ariG23498](https://github.com/ariG23498) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/mae). \n",
      "\n",
      "## Usage tips\n",
      "ERROR:root:AssertionError: Answer is too long: You can resume training from a previous checkpoint in ü§ó Trainer by passing `--output_dir previous_output_dir` without `--overwrite_output_dir` to resume training from the latest checkpoint in `output_dir`, or by passing `--resume_from_checkpoint path_to_a_specific_checkpoint` to resume training from that checkpoint folder. If you want to turn an example into a notebook, you can use `trainer.train(resume_from_checkpoint)` where `resume_from_checkpoint` is `True` to look for the last checkpoint in the value of `output_dir` passed via `TrainingArguments`, or a path to a specific checkpoint to use that saved checkpoint folder to resume the training from.. Context: ## Running quick tests\n",
      "\n",
      "Most examples are equipped with a mechanism to truncate the number of dataset samples to the desired length. This is useful for debugging purposes, for example to quickly check that all stages of the programs can complete, before running the same setup on the full dataset which may take hours to complete.\n",
      "\n",
      "For example here is how to truncate all three splits to just 50 samples each:\n",
      "```\n",
      "examples/pytorch/token-classification/run_ner.py \\\n",
      "--max_train_samples 50 \\\n",
      "--max_eval_samples 50 \\\n",
      "--max_predict_samples 50 \\\n",
      "[...]\n",
      "```\n",
      "\n",
      "Most example scripts should have the first two command line arguments and some have the third one. You can quickly check if a given example supports any of these by passing a `-h` option, e.g.:\n",
      "```\n",
      "examples/pytorch/token-classification/run_ner.py -h\n",
      "```\n",
      "\n",
      "## Resuming training\n",
      "\n",
      "You can resume training from a previous checkpoint like this:\n",
      "\n",
      "1. Pass `--output_dir previous_output_dir` without `--overwrite_output_dir` to resume training from the latest checkpoint in `output_dir` (what you would use if the training was interrupted, for instance).\n",
      "2. Pass `--resume_from_checkpoint path_to_a_specific_checkpoint` to resume training from that checkpoint folder.\n",
      "\n",
      "Should you want to turn an example into a notebook where you'd no longer have access to the command\n",
      "line, ü§ó Trainer supports resuming from a checkpoint via `trainer.train(resume_from_checkpoint)`.\n",
      "\n",
      "1. If `resume_from_checkpoint` is `True` it will look for the last checkpoint in the value of `output_dir` passed via `TrainingArguments`.\n",
      "2. If `resume_from_checkpoint` is a path to a specific checkpoint it will use that saved checkpoint folder to resume the training from.\n",
      "\n",
      "\n",
      "### Upload the trained/fine-tuned model to the Hub\n",
      "ERROR:root:AssertionError: Answer is too long: The PEFT method type for PromptEncoderConfig is 'PTUNING'.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"base_model_name_or_path\": \"roberta-large\", #base model to apply p-tuning to\n",
      "  \"bias\": \"none\",\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_peft_weights\": true,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"PTUNING\", #PEFT method type\n",
      "  \"revision\": null,\n",
      "  \"task_type\": \"CAUSAL_LM\", #type of task to train model on\n",
      "  \"transformer_layers\": [\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"ln_1\",\n",
      "      \"module_type\": \"LayerNorm\",\n",
      "      \"num_params\": 64,\n",
      "      \"params_to_transform\": [\n",
      "        \"weight\",\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_dropout\",\n",
      "      \"module_type\": \"Dropout\",\n",
      "      \"num_params\": 0,\n",
      "      \"params_to_transform\": []\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_output\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 3072,\n",
      "      \"params_to_transform\": [\n",
      "        \"weight\",\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_output_bias\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 768,\n",
      "      \"params_to_transform\": [\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_query\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 3072,\n",
      "      \"params_to_transform\": [\n",
      "        \"weight\",\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_query_bias\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 768,\n",
      "      \"params_to_transform\": [\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_key\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 3072,\n",
      "      \"params_to_transform\": [\n",
      "        \"weight\",\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_key_bias\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 768,\n",
      "      \"params_to_transform\": [\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_value\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 3072,\n",
      "      \"params_to_transform\": [\n",
      "        \"weight\",\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"attn_value_bias\",\n",
      "      \"module_type\": \"Dense\",\n",
      "      \"num_params\": 768,\n",
      "      \"params_to_transform\": [\n",
      "        \"bias\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"layer_id\": 0,\n",
      "      \"module_name\": \"mlp_dropout\",\n",
      "      \"module_type\": \"Dropout\",\n",
      "      \"num_params\": 0,\n",
      "      \"params_to_transform\": []\n",
      "    },\n",
      "    {\n",
      "      \"layer. Context: ## PEFT configurations\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Learn more about the parameters you can configure for each PEFT method in their respective API reference page.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "A configuration stores important parameters that specify how a particular PEFT method should be applied.\n",
      "\n",
      "For example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json) for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json) for applying p-tuning (these configuration files are already JSON-serialized). Whenever you load a PEFT adapter, it is a good idea to check whether it has an associated adapter_config.json file which is required.\n",
      "\n",
      "<hfoptions id=\"config\">\n",
      "<hfoption id=\"LoraConfig\">\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"base_model_name_or_path\": \"facebook/opt-350m\", #base model to apply LoRA to\n",
      "  \"bias\": \"none\",\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_lora_weights\": true,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.05,\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"LORA\", #PEFT method type\n",
      "  \"r\": 16,\n",
      "  \"revision\": null,\n",
      "  \"target_modules\": [\n",
      "    \"q_proj\", #model modules to apply LoRA to (query and value projection layers)\n",
      "    \"v_proj\"\n",
      "  ],\n",
      "  \"task_type\": \"CAUSAL_LM\" #type of task to train model on\n",
      "}\n",
      "```\n",
      "\n",
      "You can create your own configuration for training by initializing a [`LoraConfig`].\n",
      "\n",
      "```py\n",
      "from peft import LoraConfig, TaskType\n",
      "\n",
      "lora_config = LoraConfig(\n",
      "    r=16,\n",
      "    target_modules=[\"q_proj\", \"v_proj\"],\n",
      "    task_type=TaskType.CAUSAL_LM,\n",
      "    lora_alpha=32,\n",
      "    lora_dropout=0.05\n",
      ")\n",
      "```\n",
      "\n",
      "</hfoption>\n",
      "<hfoption id=\"PromptEncoderConfig\">\n",
      "ERROR:root:AssertionError: Answer is too long: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
      "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
      "can be modified to enable quick research experiments.\n",
      "\n",
      "Why should I use transformers?\n",
      "\n",
      "1. Easy-to-use state-of-the-art models:\n",
      "  - High performance on NLU and NLG tasks.\n",
      "  - Low barrier to entry for educators and practitioners.\n",
      "  - Few user-facing abstractions with just three classes to learn.\n",
      "  - A unified API for using all our pretrained models.\n",
      "  - Lower compute costs, smaller carbon footprint:\n",
      "\n",
      "2. Researchers can share trained models instead of always retraining.\n",
      "  - Practitioners can reduce compute time and production costs.\n",
      "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
      "\n",
      "3. Choose the right framework for every part of a model's lifetime:\n",
      "  - Train state-of-the-art models in 3 lines of code.\n",
      "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
      "  - Seamlessly pick the right framework for training, evaluation and production.\n",
      "\n",
      "4. Easily customize a model or an example to your needs:\n",
      "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
      "  - Model internal [SEP]\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "This means the model will have a hard time picking the correct answer. To fix this, the `question-answering` pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don't split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks.\n",
      "\n",
      "We can have the tokenizer (fast or slow) do this for us by adding `return_overflowing_tokens=True`, and we can specify the overlap we want with the `stride` argument. Here is an example, using a smaller sentence:\n",
      "\n",
      "Output:::\n",
      "\n",
      "```python\n",
      "from transformers import pipeline, AutoTokenizer\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "sentence = \"Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub.\"\n",
      "\n",
      "# Tokenize the sentence\n",
      "tokens = tokenizer.tokenize(sentence)\n",
      "\n",
      "# Add special tokens\n",
      "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
      "\n",
      "# Create the chunks\n",
      "max_len = 64\n",
      "stride = 32\n",
      "\n",
      "chunks = [tokens[i:i + max_len] for i in range(0, len(tokens), stride)]\n",
      "\n",
      "# Now we can run the question-answering pipeline on each chunk\n",
      "nli = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
      "\n",
      "for chunk in chunks:\n",
      "    input_dict = tokenizer.encode_plus(\n",
      "        chunk,\n",
      "        \"Why should I use transformers?\",\n",
      "        return_tensors=\"pt\",\n",
      "        return_overflowing_tokens=True,\n",
      "        stride=stride,\n",
      "        padding=\"max_length\",\n",
      "        max_length=max_len,\n",
      "    )\n",
      "    output = nli(input_dict)\n",
      "    print(output)\n",
      "```\n",
      "\n",
      "Output:::\n",
      "\n",
      "```json\n",
      "[{'score': 0.5455055236816406, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, 'end': 11, 'answer': 'Transformers'}]\n",
      "[{'score': 0.9999999403953552, 'start': 1, '. Context: [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
      "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
      "can be modified to enable quick research experiments.\n",
      "\n",
      "Why should I use transformers?\n",
      "\n",
      "1. Easy-to-use state-of-the-art models:\n",
      "  - High performance on NLU and NLG tasks.\n",
      "  - Low barrier to entry for educators and practitioners.\n",
      "  - Few user-facing abstractions with just three classes to learn.\n",
      "  - A unified API for using all our pretrained models.\n",
      "  - Lower compute costs, smaller carbon footprint:\n",
      "\n",
      "2. Researchers can share trained models instead of always retraining.\n",
      "  - Practitioners can reduce compute time and production costs.\n",
      "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
      "\n",
      "3. Choose the right framework for every part of a model's lifetime:\n",
      "  - Train state-of-the-art models in 3 lines of code.\n",
      "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
      "  - Seamlessly pick the right framework for training, evaluation and production.\n",
      "\n",
      "4. Easily customize a model or an example to your needs:\n",
      "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
      "  - Model internal [SEP]\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "This means the model will have a hard time picking the correct answer. To fix this, the `question-answering` pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don't split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks.\n",
      "\n",
      "We can have the tokenizer (fast or slow) do this for us by adding `return_overflowing_tokens=True`, and we can specify the overlap we want with the `stride` argument. Here is an example, using a smaller sentence:\n",
      "ERROR:root:AssertionError: Answer is too long: DeepFloyd IF\n",
      "\n",
      "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
      "\n",
      "init_image = load_image(url)\n",
      "init_image = init_image.resize((512, 512))\n",
      "\n",
      "path = \"runwayml/deepfloyd-if-v1-0\"\n",
      "\n",
      "pipe = DiffusionPipeline.from_pretrained(\n",
      "    path, torch_dtype=torch.float16, use_safetensors=True\n",
      ").to(\"cuda\")\n",
      "\n",
      "pipe.unet.to(memory_format=torch.channels_last)\n",
      "\n",
      "if run_compile:\n",
      "    print(\"Run torch compile\")\n",
      "    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
      "\n",
      "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
      "\n",
      "for _ in range(3):\n",
      "    image = pipe(prompt=prompt, image=init_image).images[0]\n",
      "```. Context: prompt = \"ghibli style, a fantasy landscape with castles\"\n",
      "\n",
      "for _ in range(3):\n",
      "    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n",
      "```\n",
      "\n",
      "### ControlNet\n",
      "\n",
      "```python\n",
      "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
      "from diffusers.utils import load_image\n",
      "import torch\n",
      "\n",
      "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
      "\n",
      "init_image = load_image(url)\n",
      "init_image = init_image.resize((512, 512))\n",
      "\n",
      "path = \"runwayml/stable-diffusion-v1-5\"\n",
      "\n",
      "run_compile = True  # Set True / False\n",
      "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\n",
      "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
      "    path, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
      ")\n",
      "\n",
      "pipe = pipe.to(\"cuda\")\n",
      "pipe.unet.to(memory_format=torch.channels_last)\n",
      "pipe.controlnet.to(memory_format=torch.channels_last)\n",
      "\n",
      "if run_compile:\n",
      "    print(\"Run torch compile\")\n",
      "    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
      "    pipe.controlnet = torch.compile(pipe.controlnet, mode=\"reduce-overhead\", fullgraph=True)\n",
      "\n",
      "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
      "\n",
      "for _ in range(3):\n",
      "    image = pipe(prompt=prompt, image=init_image).images[0]\n",
      "```\n",
      "\n",
      "### DeepFloyd IF text-to-image + upscaling\n",
      "\n",
      "```python\n",
      "from diffusers import DiffusionPipeline\n",
      "import torch\n",
      "\n",
      "run_compile = True  # Set True / False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA couples saved to ./generated_QAs/initial_generated_qa_couples.json\n",
      "Below is the first QA couple generated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all</td>\n",
       "      <td>What is the name of the class for implementing a new pipeline?\\n</td>\n",
       "      <td>Implementing a new pipeline</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all   \n",
       "\n",
       "                                                           question  \\\n",
       "0  What is the name of the class for implementing a new pipeline?\\n   \n",
       "\n",
       "                        answer  \\\n",
       "0  Implementing a new pipeline   \n",
       "\n",
       "                                                                    source_doc  \n",
       "0  huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 300 # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "# Save outputs to a JSON file\n",
    "QA_initial_couples_path = './generated_QAs/initial_generated_qa_couples.json'\n",
    "\n",
    "\n",
    "def generate_initial_QA():\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "    QA_initial_outputs = []\n",
    "    for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "        try:\n",
    "            # Generate QA couple\n",
    "            output_QA_couple = call_llm(\n",
    "                llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "            )\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "            assert len(answer) < 500, f\"Answer is too long: {answer}\"\n",
    "            QA_initial_outputs.append(\n",
    "                {\n",
    "                    \"context\": sampled_context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "                }\n",
    "            )\n",
    "        except AssertionError as ae:\n",
    "            logging.error(f\"AssertionError: {ae}. Context: {sampled_context.page_content}\")\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"ValueError in call_llm: {ve}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during loop iteration: {e}. Context: {sampled_context.page_content}\")\n",
    "        \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(QA_initial_couples_path), exist_ok=True)\n",
    "\n",
    "    with open(QA_initial_couples_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(QA_initial_outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Generated QA couples saved to {QA_initial_couples_path}\")\n",
    "        \n",
    "    return QA_initial_outputs\n",
    "\n",
    "\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(QA_initial_couples_path) and os.path.getsize(QA_initial_couples_path) > 0:\n",
    "    try:\n",
    "        with open(QA_initial_couples_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            QA_initial_outputs = json.load(f)\n",
    "            print(f\"Successfully LOADED {len(QA_initial_outputs)} QA couples\")\n",
    "    except (json.JSONDecodeError, OSError) as e:\n",
    "        print(f\"Error loading JSON file: {e}. Recreating dataset.\")\n",
    "        QA_initial_outputs = generate_initial_QA()\n",
    "else:\n",
    "    QA_initial_outputs = generate_initial_QA()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Display the first QA couple to check the format\n",
    "print(f\"Below is the first QA couple generated:\")\n",
    "display(pd.DataFrame(QA_initial_outputs).head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KG4dNtg9jVN"
   },
   "source": [
    "### 1.3. Setup critique agents\n",
    "\n",
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
    "\n",
    "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "- **Groundedness:** can the question be answered from the given context?\n",
    "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
    "\n",
    "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
    "We also build a critique agent for this criteria:\n",
    "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
    "\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    "\n",
    "üí° ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
    "\n",
    "We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "05aSgTGs9jVO"
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b9tbk7ME9jVO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b95a5d04884464a9e32da352dfaffbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Xnguc2OZ16Xb8X72Fu3be)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Zx7PZRgXxASxHNx0LKhb5)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': \"Top 5 Accuracy: 93.2%\\n- Name: tv_densenet121\\n  In Collection: DenseNet\\n  Metadata:\\n    FLOPs: 3641843200\\n    Parameters: 7980000\\n    File Size: 32342954\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Block\\n    - Dense Connections\\n    - Dropout\\n    - Max Pooling\\n    - ReLU\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tv_densenet121\\n    LR: 0.1\\n    Epochs: 90\\n    Crop Pct: '0.875'\\n    LR Gamma: 0.1\\n    Momentum: 0.9\\n    Batch Size: 32\\n    Image Size: '224'\\n    LR Step Size: 30\\n    Weight Decay: 0.0001\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/densenet.py#L379\\n  Weights: https://download.pytorch.org/models/densenet121-a639ec97.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74.74%\\n      Top 5 Accuracy: 92.15%\\n-->\", 'question': 'What is the number of parameters in the tv_densenet121 model?\\n', 'answer': 'The number of parameters in the tv_densenet121 model is 7980000.', 'source_doc': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/densenet.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Xnguc2OZ16Xb8X72Fu3be)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '```py\\n>>> from huggingface_hub import HfApi\\n>>> api = HfApi()\\n\\n# Upload all the content from the local folder to your remote Space.\\n# By default, files are uploaded at the root of the repo\\n>>> api.upload_folder(\\n...     folder_path=\"/path/to/local/space\",\\n...     repo_id=\"username/my-cool-space\",\\n...     repo_type=\"space\",\\n... )\\n```\\n\\nBy default, the `.gitignore` file will be taken into account to know which files should be committed or not. By default we check if a `.gitignore` file is present in a commit, and if not, we check if it exists on the Hub. Please be aware that only a `.gitignore` file present at the root of the directory with be used. We do not check for `.gitignore` files in subdirectories.\\n\\nIf you don\\'t want to use an hardcoded `.gitignore` file, you can use the `allow_patterns` and `ignore_patterns` arguments to filter which files to upload. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented [here](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). If both `allow_patterns` and `ignore_patterns` are provided, both constraints apply.\\n\\nBeside the `.gitignore` file and allow/ignore patterns, any `.git/` folder present in any subdirectory will be ignored.\\n\\n```py\\n>>> api.upload_folder(\\n...     folder_path=\"/path/to/local/folder\",\\n...     path_in_repo=\"my-dataset/train\", # Upload to a specific folder\\n...     repo_id=\"username/test-dataset\",\\n...     repo_type=\"dataset\",\\n...     ignore_patterns=\"**/logs/*.txt\", # Ignore all text logs\\n... )\\n```\\n\\nYou can also use the `delete_patterns` argument to specify files you want to delete from the repo in the same commit.\\nThis can prove useful if you want to clean a remote folder before pushing files in it and you don\\'t know which files\\nalready exists.', 'question': 'How to ignore all text logs when uploading a folder to Hugging Face?\\n', 'answer': 'You can ignore all text logs by using the `ignore_patterns` argument and setting it to `\"**/logs/*.txt\"` when calling the `upload_folder` function.', 'source_doc': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/upload.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Zx7PZRgXxASxHNx0LKhb5)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: yVecEtQNzpEQhvc4z1vDJ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: tRywKDjTBjMMonnpgD7y4)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '<!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Textual Inversion\\n\\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.\\n\\nIf you\\'re training on a GPU with limited vRAM, you should try enabling the `gradient_checkpointing` and `mixed_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn\\'t support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster!\\n\\nThis guide will explore the [textual_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.\\n\\nBefore running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .\\n```', 'question': 'What is the technique used in the Textual Inversion method for personalizing image generation models?\\n', 'answer': 'The Textual Inversion technique personalizes image generation models by learning and updating the text embeddings to match the example images provided. The new embeddings are tied to a special word used in the prompt.', 'source_doc': 'huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: yVecEtQNzpEQhvc4z1vDJ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'his text generation demo takes in input text and returns generated text. It uses the Transformers library to set up the model and has two examples.', 'question': 'What does the text generation demo do?\\n', 'answer': 'The text generation demo takes in input text and returns generated text. It uses the Transformers library to set up the model.', 'source_doc': 'gradio-app/gradio/blob/main/demo/text_generation/DESCRIPTION.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: tRywKDjTBjMMonnpgD7y4)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4NeW-zH2r5nd_JCG96F_X)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: iD-0-PJGP0v_rOWlnSNm1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': \"Open licensing is one of the cornerstones of AI innovation. Licenses as social and legal institutions should be well taken care of. They should not be conceived as burdensome legal technical mechanisms, but rather as a communication instrument among AI communities bringing stakeholders together by sharing common messages on how the licensed artifact can be used.\\n\\nLet's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and impact depends on it, on all of us, on you.\\n\\nAuthor: Carlos Mu√±oz Ferrandis\\n\\nBlog acknowledgments: Yacine Jernite, Giada Pistilli, Irene Solaiman, Clementine Fourrier, Cl√©ment D√©lange\", 'question': 'What is one of the cornerstones of AI innovation?\\n', 'answer': 'Open licensing is one of the cornerstones of AI innovation.', 'source_doc': 'huggingface/blog/blob/main/open_rail.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4NeW-zH2r5nd_JCG96F_X)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_test = common_voice_test.map(replace_hatted_characters)\\n```\\n\\nIn CTC, it is common to classify speech chunks into letters, so we will do the same here.\\nLet\\'s extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\\n\\nWe write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars.\\nIt is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once.\\n\\n```python\\ndef extract_all_chars(batch):\\n  all_text = \" \".join(batch[\"sentence\"])\\n  vocab = list(set(all_text))\\n  return {\"vocab\": [vocab], \"all_text\": [all_text]}\\n```\\n\\n```python\\nvocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\\nvocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\\n```\\n\\nNow, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary.\\n\\n```python\\nvocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\\n```\\n\\n```python\\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\\nvocab_dict\\n```\\n\\n```bash\\n    {\\' \\': 0,\\n     \\'a\\': 1,\\n     \\'b\\': 2,\\n     \\'c\\': 3,\\n     \\'d\\': 4,\\n     \\'e\\': 5,\\n     \\'f\\': 6,\\n     \\'g\\': 7,\\n     \\'h\\': 8,\\n     \\'i\\': 9,\\n     \\'j\\': 10,\\n     \\'k\\': 11,\\n     \\'l\\': 12,\\n     \\'m\\': 13,\\n     \\'n\\': 14,\\n     \\'o\\': 15,\\n     \\'p\\': 16,\\n     \\'q\\': 17,\\n     \\'r\\': 18,\\n     \\'s\\': 19,\\n     \\'t\\': 20,\\n     \\'u\\': 21,\\n     \\'v\\': 22,\\n     \\'w\\': 23,\\n     \\'x\\': 24,\\n     \\'y\\': 25,\\n     \\'z\\': 26,\\n     \\'√ß\\': 27,\\n     \\'√´\\': 28,\\n     \\'√∂\\': 29,\\n     \\'√º\\': 30,\\n     \\'ƒü\\': 31,\\n     \\'ƒ±\\': 32,\\n     \\'≈ü\\': 33,\\n     \\'Ãá\\': 34}\\n```', 'question': 'What is the 27th letter in the sorted vocabulary dictionary?\\n', 'answer': '√ß', 'source_doc': 'huggingface/blog/blob/main/mms_adapters.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: iD-0-PJGP0v_rOWlnSNm1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: upwqfL5F_yfwxUP28aZtJ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: dy-ZPP9emSzNjFBK0IuLl)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'Here are the images:\\n\\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/concept_images/0.jpeg\" height=\"256\">\\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/concept_images/1.jpeg\" height=\"256\">\\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/concept_images/2.jpeg\" height=\"256\">\\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/concept_images/3.jpeg\" height=\"256\">\\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/concept_images/4.jpeg\" height=\"256\">\\n\\n\\nThe system setup is now complete. Let\\'s configure the training job.\\n\\n## Configuring the fine-tuning job\\n\\nThe [Accelerate](https://huggingface.co/docs/accelerate/index) library makes it very easy to run distributed training. We need to run it on each node and answer simple questions.\\n\\nHere\\'s a screenshot for the primary node. On the other nodes, you need to set the rank to 1, 2, and 3. All other answers are identical.\\n\\n<kbd>\\n  <img src=\"assets/stable-diffusion-finetuning-intel/screen01.png\">\\n</kbd>\\n\\nFinally, we need to set the environment on the primary node. It will be propagated to other nodes as the fine-tuning job starts. The first line sets the name of the network interface connected to the local network where all nodes run. You may need to adapt this using`ifconfig` to get the appropriate information.\\n\\n```\\nexport I_MPI_HYDRA_IFACE=ens786f1\\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\\nexport LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so\\nexport LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so\\nexport CCL_ATL_TRANSPORT=ofi\\nexport CCL_WORKER_COUNT=1\\n\\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\\nexport DATA_DIR=\"/home/devcloud/dicoo\"\\n```\\n\\nWe can now launch the fine-tuning job.\\n\\n## Fine-tuning the model', 'question': 'What is the name of the model being fine-tuned?\\n', 'answer': 'The name of the model being fine-tuned is \"runwayml/stable-diffusion-v1-5\".', 'source_doc': 'huggingface/blog/blob/main/stable-diffusion-finetuning-intel.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: upwqfL5F_yfwxUP28aZtJ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'Let\\'s fill the `push_to_hub` function:\\n\\n- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `\\n(repo_id = {username}/{repo_name})`\\nüí° A good `repo_id` is `{username}/q-{env_id}`\\n- `model`: our model dictionary containing the hyperparameters and the Qtable.\\n- `env`: the environment.\\n- `commit_message`: message of the commit\\n\\n```python\\nmodel\\n```\\n\\n```python\\nusername = \"\"  # FILL THIS\\nrepo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)\\n```\\n\\nCongrats ü•≥ you\\'ve just implemented from scratch, trained, and uploaded your first Reinforcement Learning agent.\\nFrozenLake-v1 no_slippery is very simple environment, let\\'s try a harder one üî•.\\n\\n# Part 2: Taxi-v3 üöñ\\n\\n## Create and understand [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\\n---\\n\\nüí° A good habit when you start to use an environment is to check its documentation\\n\\nüëâ https://gymnasium.farama.org/environments/toy_text/taxi/\\n\\n---\\n\\nIn `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\\n\\nWhen the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\\n\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\\n\\n\\n```python\\nenv = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\\n```\\n\\nThere are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\\n\\n\\n```python\\nstate_space = env.observation_space.n\\nprint(\"There are \", state_space, \" possible states\")\\n```', 'question': 'What is the reward for all other timesteps in the Taxi-v3 environment?\\n', 'answer': 'The reward for all other timesteps in the Taxi-v3 environment is -1.', 'source_doc': 'huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: dy-ZPP9emSzNjFBK0IuLl)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: OQdcdPVWdwoQ5rFUCtY-q)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 03JUcwZdb8uz9g8X9Zzh0)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'For now, Transformers supports SDPA inference and training for the following architectures:\\n* [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\\n* [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)\\n* [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\\n* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\\n* [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\\n* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\\n\\n<Tip>\\n\\nFlashAttention can only be used for models with the `fp16` or `bf16` torch type, so make sure to cast your model to the appropriate type first.\\n\\n</Tip>\\n\\nBy default, SDPA selects the most performant kernel available but you can check whether a backend is available in a given setting (hardware, problem size) with [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager:\\n\\n```diff\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\\n# convert the model to BetterTransformer\\nmodel.to_bettertransformer()\\n\\ninput_text = \"Hello my dog is cute and\"\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\\n\\n+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\\n    outputs = model.generate(**inputs)\\n\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\n\\nIf you see a bug with the traceback below, try using the nightly version of PyTorch which may have broader coverage for FlashAttention:\\n\\n```bash\\nRuntimeError: No available kernel. Aborting execution.', 'question': 'Which models support SDPA inference and training in Transformers?\\n', 'answer': 'Bart, GPTBigCode, Falcon, Llama, Idefics, and Whisper models support SDPA inference and training in Transformers.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: OQdcdPVWdwoQ5rFUCtY-q)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\npipeline = AutoPipelineForImage2Image.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    torch_dtype=torch.float16,\\n    use_safetensors=True,\\n).to(\"cuda\")\\nprompt = \"a portrait of a dog wearing a pearl earring\"\\n\\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"\\n\\nresponse = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage.thumbnail((768, 768))\\n\\nimage = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]\\nimage\\n```\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png\" alt=\"generated image of a vermeer portrait of a dog wearing a pearl earring\"/>\\n</div>\\n\\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\\n\\n```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_image\\nimport torch\\n\\npipeline = AutoPipelineForInpainting.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True\\n).to(\"cuda\")\\n\\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\\n\\ninit_image = load_image(img_url).convert(\"RGB\")\\nmask_image = load_image(mask_url).convert(\"RGB\")\\n\\nprompt = \"A majestic tiger sitting on a bench\"\\nimage = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]\\nimage\\n```', 'question': 'What is the prompt used for the inpainting pipeline?\\n', 'answer': 'The prompt used for the inpainting pipeline is \"A majestic tiger sitting on a bench\".', 'source_doc': 'huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 03JUcwZdb8uz9g8X9Zzh0)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 03aaEnxhiurqUg804sg59)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: DBYURbQCfEiuLFFvzmRJR)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '```py\\n>>> repo.git_add(\"path/to/file\")\\n>>> repo.git_commit(commit_message=\"add my first model config file :)\")\\n```\\n\\nWhen you\\'re ready, push the file to your repository with [`~Repository.git_push`]:\\n\\n```py\\n>>> repo.git_push()\\n```', 'question': 'How do you push a file to a repository using git?\\n', 'answer': \"You can push a file to a repository using the `git_push` method of a `Repository` object in Python. This method sends the repository's history to the remote repository. In the provided context, the `git_push` method is called with no arguments to push the repository's changes to the remote repository.\", 'source_doc': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/upload.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 03aaEnxhiurqUg804sg59)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '--\\ntitle: MAPE\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  Mean Absolute Percentage Error (MAPE) is the mean percentage error difference between the predicted and actual\\n  values.\\n---\\n\\n# Metric Card for MAPE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAPE) is the mean of the percentage error of difference between the predicted $x_i$ and actual $y_i$ numeric values:\\n![image](https://user-images.githubusercontent.com/8100/200005316-c3975d32-8978-40f3-b541-c2ef57ec7c5b.png)\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as inputs.\\n\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the estimated target values.\\n- `references`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the ground truth (correct) target values.\\n\\nOptional arguments:\\n- `sample_weight`: numeric array-like of shape (`n_samples,`) representing sample weights. The default is `None`.\\n- `multioutput`: `raw_values`, `uniform_average` or numeric array-like of shape (`n_outputs,`), which defines the aggregation of multiple output values. The default value is `uniform_average`.\\n  - `raw_values` returns a full set of errors in case of multioutput input.\\n  - `uniform_average` means that the errors of all outputs are averaged with uniform weight. \\n  - the array-like value defines weights used to average errors.', 'question': 'What is the formula for MAPE?\\n', 'answer': 'MAPE is the mean of the percentage error of difference between the predicted and actual numeric values, represented as the formula: MAPE = (1/n) * Œ£(|yi - xi| / |yi|) * 100%', 'source_doc': 'huggingface/evaluate/blob/main/metrics/mape/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: DBYURbQCfEiuLFFvzmRJR)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: rW6tSoHzcAmYDZx_RFvWC)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: wAUOY8Btn9lJMOK8HWRir)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'The abstract from the paper is the following:\\n\\n*Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*\\n\\nThis model was contributed by [xvjiarui](https://huggingface.co/xvjiarui). The TensorFlow version was contributed by [ariG23498](https://huggingface.co/ariG23498) with the help of [Yih-Dar SHIEH](https://huggingface.co/ydshieh), [Amy Roberts](https://huggingface.co/amyeroberts), and [Joao Gante](https://huggingface.co/joaogante).\\nThe original code can be found [here](https://github.com/NVlabs/GroupViT).\\n\\n## Usage tips\\n \\n- You may specify `output_segmentation=True` in the forward of `GroupViTModel` to get the segmentation logits of input texts. \\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with GroupViT.', 'question': 'What is the name of the model proposed in the paper?\\n', 'answer': 'The name of the model proposed in the paper is GroupViT.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/groupvit.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: rW6tSoHzcAmYDZx_RFvWC)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'Metric Card for SacreBLEU\\n\\n\\n## Metric Description\\nSacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. Inspired by Rico Sennrich\\'s `multi-bleu-detok.perl`, it produces the official Workshop on Machine Translation (WMT) scores but works with plain text. It also knows all the standard test sets and handles downloading, processing, and tokenization.\\n\\nSee the [README.md] file at https://github.com/mjpost/sacreBLEU for more information.\\n\\n## How to Use\\nThis metric takes a set of predictions and a set of references as input, along with various optional parameters.\\n\\n\\n```python\\n>>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\\n>>> references = [[\"hello there general kenobi\", \"hello there !\"],\\n...                 [\"foo bar foobar\", \"foo bar foobar\"]]\\n>>> sacrebleu = datasets.load_metric(\"sacrebleu\")\\n>>> results = sacrebleu.compute(predictions=predictions, \\n...                             references=references)\\n>>> print(list(results.keys()))\\n[\\'score\\', \\'counts\\', \\'totals\\', \\'precisions\\', \\'bp\\', \\'sys_len\\', \\'ref_len\\']\\n>>> print(round(results[\"score\"], 1))\\n100.0\\n```', 'question': 'What is the purpose of SacreBLEU?\\n', 'answer': 'SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. It produces the official Workshop on Machine Translation (WMT) scores but works with plain text, and it also knows all the standard test sets and handles downloading, processing, and tokenization.', 'source_doc': 'huggingface/datasets/blob/main/metrics/sacrebleu/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: wAUOY8Btn9lJMOK8HWRir)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: BnnPenxWcXehvSU62MuHo)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: mILjJGXXAcfubJim8EOXw)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '## Get the size of the dataset\\n\\nThe `/size` endpoint returns a JSON with the size (number of rows and size in bytes) of the dataset, and for every configuration and split:\\n\\n<inferencesnippet>\\n<python>\\n```python\\nimport requests\\nAPI_URL = \"https://datasets-server.huggingface.co/size?dataset=rotten_tomatoes\"\\ndef query():\\n    response = requests.get(API_URL)\\n    return response.json()\\ndata = query()\\n````\\n\\n</python>\\n<js>\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\n    const response = await fetch(\\n        \"https://datasets-server.huggingface.co/size?dataset=rotten_tomatoes\",\\n        {\\n            method: \"GET\"\\n        }\\n    );\\n    const result = await response.json();\\n    return result;\\n}\\nquery().then((response) => {\\n    console.log(JSON.stringify(response));\\n});\\n```\\n</js>\\n<curl>\\n```curl\\ncurl https://datasets-server.huggingface.co/size?dataset=rotten_tomatoes \\\\\\n        -X GET\\n```\\n</curl>\\n</inferencesnippet>\\n\\nThis returns a URL to the Parquet file for each split:', 'question': 'What is the number of rows in the rotten tomatoes dataset?\\n', 'answer': 'The number of rows in the rotten tomatoes dataset is 123456.', 'source_doc': 'huggingface/datasets-server/blob/main/docs/source/quick_start.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: BnnPenxWcXehvSU62MuHo)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of `generic_update` by shallow copying by [@gitgithan](https://github.com/gitgithan) in [PR 3405](https://github.com/gradio-app/gradio/pull/3405) to fix [#3282](https://github.com/gradio-app/gradio/issues/3282)\\n- Persist file names of files uploaded through any Gradio component by [@abidlabs](https://github.com/abidlabs) in [PR 3412](https://github.com/gradio-app/gradio/pull/3412)\\n- Fix markdown embedded component in docs by [@aliabd](https://github.com/aliabd) in [PR 3410](https://github.com/gradio-app/gradio/pull/3410)\\n- Clean up event listeners code by [@aliabid94](https://github.com/aliabid94) in [PR 3420](https://github.com/gradio-app/gradio/pull/3420)\\n- Fix css issue with spaces logo by [@aliabd](https://github.com/aliabd) in [PR 3422](https://github.com/gradio-app/gradio/pull/3422)\\n- Makes a few fixes to the `JSON` component (show_label parameter, icons) in [@abidlabs](https://github.com/abidlabs) in [PR 3451](https://github.com/gradio-app/gradio/pull/3451)\\n\\n### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.20.1\\n\\n### New Features:\\n\\n- Add `height` kwarg to style in `gr.Chatbot()` component by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3369](https://github.com/gradio-app/gradio/pull/3369)\\n\\n```python\\nchatbot = gr.Chatbot().style(height=500)\\n```\\n\\n### Bug Fixes:\\n\\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https://github.com/pngwn) in [PR 3386](https://github.com/gradio-app/gradio/pull/3386)\\n- Fixes bug where when if fn is a non-static class member, then self should be ignored as the first param of the fn by [@or25](https://github.com/or25) in [PR #3227](https://github.com/gradio-app/gradio/pull/3227)\\n\\n### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.', 'question': 'What is the new feature added to the `gr.Chatbot()` component in version 3.20.1?\\n', 'answer': 'The new feature added to the `gr.Chatbot()` component in version 3.20.1 is the `height` kwarg to style the component.', 'source_doc': 'gradio-app/gradio/blob/main/CHANGELOG.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: mILjJGXXAcfubJim8EOXw)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: ylbU9KNbElESPCP1AdhGG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4WGVlYNbum2R4wQWQ_15E)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '1. Get a list of ids to products which we can call `ids_to_products_dict`:\\n\\n```bash\\n{0: \\'RamPro 10\" All Purpose Utility Air Tires/Wheels with a 5/8\" Diameter Hole with Double Sealed Bearings (Pack of 2)\\',\\n 1: \\'MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tractor Tire with Yellow Rim, (3\" Centered Hub, 3/4\" Bushings )\\',\\n 2: \\'NEIKO 20601A 14.5 inch Steel Tire Spoon Lever Iron Tool Kit | Professional Tire Changing Tool for Motorcycle, Dirt Bike, Lawn Mower | 3 pcs Tire Spoons | 3 Rim Protector | Valve Tool | 6 Valve Cores\\',\\n 3: \\'2PK 13x5.00-6 13x5.00x6 13x5x6 13x5-6 2PLY Turf Mower Tractor Tire with Gray Rim\\',\\n 4: \\'(Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Wheel Assy .75\" Bearing\\',\\n 5: \\'MaxAuto 2 Pcs 16x6.50-8 Lawn Mower Tire for Garden Tractors Ridings, 4PR, Tubeless\\',\\n 6: \\'Dr.Roc Tire Spoon Lever Dirt Bike Lawn Mower Motorcycle Tire Changing Tools with Durable Bag 3 Tire Irons 2 Rim Protectors 1 Valve Stems Set TR412 TR413\\',\\n 7: \\'MARASTAR 21446-2PK 15x6.00-6\" Front Tire Assembly Replacement-Craftsman Mower, Pack of 2\\',\\n 8: \\'15x6.00-6\" Front Tire Assembly Replacement for 100 and 300 Series John Deere Riding Mowers - 2 pack\\',\\n 9: \\'Honda HRR Wheel Kit (2 Front 44710-VL0-L02ZB, 2 Back 42710-VE2-M02ZE)\\',\\n 10: \\'Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01ZE) Lawn Mower Rear Wheel Set of 2\\' ...\\n```\\n\\n2. Use the trained [smangrul/peft_lora_e5_ecommerce_semantic_search_colab](https://huggingface.co/smangrul/peft_lora_e5_ecommerce_semantic_search_colab) model to get the product embeddings:\\n\\n```py\\n# base model\\nmodel = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)\\n\\n# peft config and wrapping\\nmodel = PeftModel.from_pretrained(model, peft_model_id)\\n\\ndevice = \"cuda\"\\nmodel.to(device)\\nmodel.eval()\\nmodel = model.merge_and_unload()\\n\\nimport numpy as np\\nnum_products= len(dataset)\\nd = 1024', 'question': 'How many products are in the ids_to_products_dict?\\n', 'answer': 'There are 11 products in the ids_to_products_dict.\\n```', 'source_doc': 'huggingface/peft/blob/main/docs/source/task_guides/semantic-similarity-lora.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: ylbU9KNbElESPCP1AdhGG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '--cache_dir CACHE_DIR\\n                        Path indicating where to store cache.\\n  --trust-remote-code   Allows to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust and in which you have read the code, as it will execute on your local machine arbitrary code present in the model repository.\\n  --no-post-process     Allows to disable any post-processing done by default on the exported ONNX models. For example, the merging of decoder and decoder-with-past models into a single ONNX model file to reduce memory usage.\\n  --optimize {O1,O2,O3,O4}\\n                        Allows to run ONNX Runtime optimizations directly during the export. Some of these optimizations are specific to ONNX Runtime, and the resulting ONNX will not be usable with other runtime as OpenVINO or TensorRT. Possible options:\\n                            - O1: Basic general optimizations\\n                            - O2: Basic and extended general optimizations, transformers-specific fusions\\n                            - O3: Same as O2 with GELU approximation\\n                            - O4: Same as O3 with mixed precision (fp16, GPU-only, requires `--device cuda`)', 'question': 'What are the possible options for the --optimize flag?\\n', 'answer': 'The possible options for the --optimize flag are O1, O2, O3, and O4.', 'source_doc': 'huggingface/optimum/blob/main/docs/source/exporters/onnx/usage_guides/export_a_model.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4WGVlYNbum2R4wQWQ_15E)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 9qyOjvf4EBjneVYVEl1_S)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: VnWcdU3ajn7ZW5GLaO7UU)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': \"### Memory usage and data loading\\n\\nOne thing to note is that all data is loaded into memory in this script. Most question answering datasets are small\\nenough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\\ndata streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\\nrequired to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and \\nREADME, but for more information you can see the 'Input Datasets' section of \\n[this document](https://www.tensorflow.org/guide/tpu).\\n\\n### Example command\\n```\\npython run_qa.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--output_dir output \\\\\\n--dataset_name squad \\\\\\n--do_train \\\\\\n--do_eval \\\\\\n```\", 'question': 'How is data loaded in the script?\\n', 'answer': 'All data is loaded into memory in this script.', 'source_doc': 'huggingface/transformers/blob/main/examples/tensorflow/question-answering/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 9qyOjvf4EBjneVYVEl1_S)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'dataset = load_dataset(\"imdb\", split=\"train\")\\n\\nmodel_id = \"tiiuae/falcon-7b\"\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\\n\\ntrainer = SFTTrainer(\\n    model,\\n    tokenizer=tokenizer,\\n    train_dataset=dataset,\\n    dataset_text_field=\"text\",\\n    max_seq_length=512,\\n)\\ntrainer.train()\\n```\\n\\nCheck out the [original qlora repository](https://github.com/artidoro/qlora/) for additional details about evaluating the trained models.\\n\\n### Fine-tuning Resources\\n- **[Colab notebook to fine-tune Falcon-7B on Guanaco dataset using 4bit and PEFT](https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing)** \\n- **[Training code](https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14)** \\n- **[40B model adapters](https://huggingface.co/smangrul/falcon-40B-int4-peft-lora-sfttrainer)** ([logs](https://wandb.ai/smangrul/huggingface/runs/3hpqq08s/workspace?workspace=user-younesbelkada))\\n- **[7B model adapters](https://huggingface.co/ybelkada/falcon-7b-guanaco-lora)** ([logs](https://wandb.ai/younesbelkada/huggingface/runs/2x4zi72j?workspace=user-younesbelkada)) \\n\\n## Conclusion\\n\\nFalcon is an exciting new large language model which can be used for commercial applications. In this blog post we showed its capabilities, how to run it in your own environment and how easy to fine-tune on custom data within in the Hugging Face ecosystem. We are excited to see what the community will build with it!', 'question': 'What is the name of the model used in the fine-tuning process?\\n', 'answer': 'Falcon\\n```', 'source_doc': 'huggingface/blog/blob/main/falcon.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: VnWcdU3ajn7ZW5GLaO7UU)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: x9idU9Iz0EubEnNYTUv8m)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 26o-UWxEwfChquX2C3y_W)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '@gradio/chatbot\\n\\n## 0.5.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6), [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\\n  - @gradio/markdown@0.6.0\\n  - @gradio/client@0.9.3\\n  - @gradio/statustracker@0.4.3\\n  - @gradio/atoms@0.4.1\\n  - @gradio/upload@0.5.6\\n\\n## 0.5.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\\n  - @gradio/client@0.9.2\\n  - @gradio/upload@0.5.5\\n\\n## 0.5.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n\\n## 0.5.2\\n\\n### Features\\n\\n- [#6399](https://github.com/gradio-app/gradio/pull/6399) [`053bec9`](https://github.com/gradio-app/gradio/commit/053bec98be1127e083414024e02cf0bebb0b5142) - Improve CSS token documentation in Storybook. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.5.1\\n\\n### Fixes\\n\\n- [#6574](https://github.com/gradio-app/gradio/pull/6574) [`2b625ad`](https://github.com/gradio-app/gradio/commit/2b625ad9403c3449b34a8a3da68ae48c4347c2db) - Ensure Chatbot messages are properly aligned when `rtl` is true. Thanks [@hannahblair](https://github.com/hannahblair)!\\n- [#6572](https://github.com/gradio-app/gradio/pull/6572) [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50) - Improve like/dislike functionality. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.5.0\\n\\n### Features', 'question': 'What is the version of the @gradio/chatbot package?\\n', 'answer': '0.5.5', 'source_doc': 'gradio-app/gradio/blob/main/js/chatbot/CHANGELOG.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: x9idU9Iz0EubEnNYTUv8m)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '</Tip>\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\\n\\n## Sharded checkpoints\\n\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\\n\\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model.\\n\\n```py\\nfrom transformers import AutoModel\\n\\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\\n```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n\\n```py\\n>>> import os\\n>>> import tempfile\\n\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir)\\n...     print(sorted(os.listdir(tmp_dir)))\\n[\\'config.json\\', \\'pytorch_model.bin\\']\\n```\\n\\nNow let\\'s use a maximum shard size of 200MB:\\n\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     print(sorted(os.listdir(tmp_dir)))\\n[\\'config.json\\', \\'pytorch_model-00001-of-00003.bin\\', \\'pytorch_model-00002-of-00003.bin\\', \\'pytorch_model-00003-of-00003.bin\\', \\'pytorch_model.bin.index.json\\']\\n```\\n\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:', 'question': 'How many files are created when saving a model with a maximum shard size of 200MB?\\n', 'answer': '4 files are created when saving a model with a maximum shard size of 200MB.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/big_models.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 26o-UWxEwfChquX2C3y_W)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: D8Ncah_Es9aW8suk_0x_R)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: B2qUlHhME19R9lVWPbIw1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'This type of monitoring helps you quickly flag the currently deployed\\nEndpoint and make adjustments as necessary. It‚Äôs also possible to\\nrequest monitoring of model explanations. Refer\\n[<u>here</u>](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)\\nto learn more.\\n\\n# Local Load Testing\\n\\nWe conducted a local load test to better understand the limits of the\\nEndpoint with [<u>Locust</u>](https://locust.io/). The table below\\nsummarizes the request statistics:\\n\\n![](./assets/97_vertex_ai/image5.png)\\n\\nAmong all the different statistics shown in the table, `Average (ms)`\\nrefers to the average latency of the Endpoint. Locust fired off about\\n**17230 requests**, and the reported average latency is **646\\nMilliseconds**, which is impressive. In practice, you‚Äôd want to simulate\\nmore real traffic by conducting the load test in a distributed manner.\\nRefer [<u>here</u>](https://cloud.google.com/architecture/load-testing-and-monitoring-aiplatform-models)\\nto learn more.\\n\\n[<u>This directory</u>](https://github.com/sayakpaul/deploy-hf-tf-vision-models/tree/main/hf_vision_model_vertex_ai/locust)\\nhas all the information needed to know how we conducted the load test.\\n\\n# Pricing\\n\\nYou can use the [<u>GCP cost estimator</u>](https://cloud.google.com/products/calculator) to estimate the cost of usage, \\nand the exact hourly pricing table can be found [<u>here</u>](https://cloud.google.com/vertex-ai/pricing#custom-trained_models).\\nIt is worth noting that you are only charged when the node is processing\\nthe actual prediction requests, and you need to calculate the price with\\nand without GPUs.\\n\\nFor the Vertex Prediction for a custom-trained model, we can choose\\n[N1 machine types from `n1-standard-2` to `n1-highcpu-32`](https://cloud.google.com/vertex-ai/pricing#custom-trained_models).\\nYou used `n1-standard-8` for this post which is equipped with 8\\nvCPUs and 32GBs of RAM.\\n\\n<div align=\"center\">', 'question': 'What is the average latency of the Endpoint?\\n', 'answer': 'The average latency of the Endpoint is 646 Milliseconds.\\n\\n</div>', 'source_doc': 'huggingface/blog/blob/main/deploy-vertex-ai.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: D8Ncah_Es9aW8suk_0x_R)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'This model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md).\\n\\n## Usage tips\\n\\n- The checkpoints are named **mobilenet\\\\_v1\\\\_*depth*\\\\_*size***, for example **mobilenet\\\\_v1\\\\_1.0\\\\_224**, where **1.0** is the depth multiplier (sometimes also referred to as \"alpha\" or the width multiplier) and **224** is the resolution of the input images the model was trained on.\\n\\n- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32.\\n\\n- One can use [`MobileNetV1ImageProcessor`] to prepare images for the model.\\n\\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra ‚Äúbackground‚Äù class (index 0).\\n\\n- The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV1Config`] with `tf_padding = False`.\\n\\nUnsupported features:\\n\\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use a 7x7 average pooling layer with stride 2 instead of global pooling. For larger inputs, this gives a pooled output that is larger than 1x1 pixel. The HuggingFace implementation does not support this.\\n\\n- It is currently not possible to specify an `output_stride`. For smaller output strides, the original model invokes dilated convolution to prevent the spatial resolution from being reduced further. The output stride of the HuggingFace model is always 32.', 'question': 'What is the name of the class that prepares images for the model?\\n', 'answer': 'The name of the class that prepares images for the model is [`MobileNetV1ImageProcessor`].', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/mobilenet_v1.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: B2qUlHhME19R9lVWPbIw1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: _H9G8jo6e-r9y0N413Nzz)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Z14XzGO0Cl5CvTnEwkyYz)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': \"<!--\\nType: model-index\\nCollections:\\n- Name: FBNet\\n  Paper:\\n    Title: 'FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural\\n      Architecture Search'\\n    URL: https://paperswithcode.com/paper/fbnet-hardware-aware-efficient-convnet-design\\nModels:\\n- Name: fbnetc_100\\n  In Collection: FBNet\\n  Metadata:\\n    FLOPs: 508940064\\n    Parameters: 5570000\\n    File Size: 22525094\\n    Architecture:\\n    - 1x1 Convolution\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - FBNet Block\\n    - Global Average Pooling\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x GPUs\\n    ID: fbnetc_100\\n    LR: 0.1\\n    Epochs: 360\\n    Layers: 22\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0005\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L985\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetc_100-c345b898.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75.12%\\n      Top 5 Accuracy: 92.37%\\n-->\", 'question': 'What is the top 1 accuracy of the fbnetc_100 model on ImageNet?\\n', 'answer': 'The top 1 accuracy of the fbnetc\\\\_100 model on ImageNet is 75.12%.', 'source_doc': 'huggingface/pytorch-image-models/blob/main/docs/models/fbnet.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: _H9G8jo6e-r9y0N413Nzz)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as possible by staying as concise as possible. Remove all code / all information that is irrelevant to the issue. If you have found a bug, try to create the easiest code example you can to demonstrate your issue, do not just dump your whole workflow into the issue as soon as you have found a bug. E.g., if you train a model and get an error at some point during the training, you should first try to understand what part of the training code is responsible for the error and try to reproduce it with a couple of lines. Try to use dummy data instead of full datasets.\\n5. Add links. If you are referring to a certain naming, method, or model make sure to provide a link so that the reader can better understand what you mean. If you are referring to a specific PR or issue, make sure to link it to your issue. Do not assume that the reader knows what you are talking about. The more links you add to your issue the better.\\n6. Formatting. Make sure to nicely format your issue by formatting code into Python code syntax, and error messages into normal code syntax. See the [official GitHub formatting docs](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) for more information.\\n7. Think of your issue not as a ticket to be solved, but rather as a beautiful entry to a well-written encyclopedia. Every added issue is a contribution to publicly available knowledge. By adding a nicely written issue you not only make it easier for maintainers to solve your issue, but you are helping the whole community to better understand a certain aspect of the library.', 'question': 'What should you do if you find a bug in the code?\\n', 'answer': 'You should try to create the easiest code example possible to demonstrate the issue and use dummy data instead of full datasets.', 'source_doc': 'huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Z14XzGO0Cl5CvTnEwkyYz)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: w5ZJCD9VtDSxSMITz5ucW)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: eEgY4zkJNRWRNayyy6gcZ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '| Column | Type                  | Description                                                                                                                                                                                                                                                                                             |\\n|:-------|:----------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 1      | Document ID           | This is a variation on the document filename                                                                                                                                                                                                                                                            |\\n| 2      | Part number           | Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.                                                                                                                                                                                                                          |\\n| 3      | Word number           |                                                                                                                                                                                                                                                                                                         |\\n| 4      | Word                  | This is the token as segmented/tokenized in the Treebank. Initially the *_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release.                                                                                    |', 'question': 'What is the type of the first column in the table?\\n', 'answer': 'The type of the first column in the table is Document ID.', 'source_doc': 'huggingface/evaluate/blob/main/metrics/coval/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: w5ZJCD9VtDSxSMITz5ucW)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'Model Card components\\n\\n**Model Card Components** are special elements that you can inject directly into your Model Card markdown to display powerful custom components in your model page. These components are authored by us, feel free to share ideas about new Model Card component in [this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/17).\\n\\n## The Gallery component\\n\\nAdd the `<Gallery />` component to your text-to-image model card to showcase your images generation.\\n\\nFor example, \\n```md\\n\\n<Gallery />\\n\\n## Model description\\n\\nTintinIA is fine-tuned version of Stable-Diffusion-xl trained on 125 comics panels from Tintin album. \\n\\n```\\n\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gallery.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gallery-dark.png\"/>\\n</div>\\n\\nThe `<Gallery/>` component will use your Model Card [widget metadata](/docs/hub/models-widgets-examples#text-to-image) to display the images with each associated prompt. \\n\\n```yaml\\nwidget:\\n- text: \"drawing of tintin in a shop\"\\n  output:\\n    url: \"images/shop.png\"\\n- text: \"drawing of tintin watching rugby\"\\n  output:\\n    url: \"images/rugby.png\"\\n  parameters:\\n    negative_prompt: \"blurry\"\\n- text: \"tintin working at the office\"\\n  output:\\n    url: \"images/office.png\"\\n```\\n\\n> Hint: Support of Card Components through the GUI editor coming soon...', 'question': 'How do I add a gallery to my model card?\\n', 'answer': 'Add the `<Gallery />` component to your model card markdown to showcase your images generation.', 'source_doc': 'huggingface/hub-docs/blob/main/docs/hub/model-cards-components.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: eEgY4zkJNRWRNayyy6gcZ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4fozmty_CranwEG1rIE6_)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: AxpfiAl1VtAUcpAzzSxqu)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'Learning Rate Schedulers\\n\\nThis page contains the API reference documentation for learning rate schedulers included in `timm`.\\n\\n## Schedulers\\n\\n### Factory functions\\n\\n[[autodoc]] timm.scheduler.scheduler_factory.create_scheduler\\n[[autodoc]] timm.scheduler.scheduler_factory.create_scheduler_v2\\n\\n### Scheduler Classes\\n\\n[[autodoc]] timm.scheduler.cosine_lr.CosineLRScheduler\\n[[autodoc]] timm.scheduler.multistep_lr.MultiStepLRScheduler\\n[[autodoc]] timm.scheduler.plateau_lr.PlateauLRScheduler\\n[[autodoc]] timm.scheduler.poly_lr.PolyLRScheduler\\n[[autodoc]] timm.scheduler.step_lr.StepLRScheduler\\n[[autodoc]] timm.scheduler.tanh_lr.TanhLRScheduler', 'question': 'What are the names of the scheduler classes in timm?\\n', 'answer': 'The names of the scheduler classes in timm are CosineLRScheduler, MultiStepLRScheduler, PlateauLRScheduler, PolyLRScheduler, StepLRScheduler, and TanhLRScheduler.', 'source_doc': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/reference/schedulers.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 4fozmty_CranwEG1rIE6_)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': \"1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\\n1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\\n1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.\\n1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\", 'question': 'Which model was released by Meta AI?\\n', 'answer': 'X-MOD', 'source_doc': 'huggingface/transformers/blob/main/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: AxpfiAl1VtAUcpAzzSxqu)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 5ne6jteFS01mESR4w5DAA)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: vas-2ex6b8FO8Sw7SMK2f)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '# 3\\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\\n\\n# 4\\nmodel = A2C(policy = \"MultiInputPolicy\",\\n            env = env,\\n            verbose=1)\\n# 5\\nmodel.learn(1_000_000)\\n```\\n\\n```python\\n# 6\\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\\nmodel.save(model_name)\\nenv.save(\"vec_normalize.pkl\")\\n\\n# 7\\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\\n\\n# Load the saved statistics\\neval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\\n\\n#  do not update them at test time\\neval_env.training = False\\n# reward normalization is not needed at test time\\neval_env.norm_reward = False\\n\\n# Load the agent\\nmodel = A2C.load(model_name)\\n\\nmean_reward, std_reward = evaluate_policy(model, eval_env)\\n\\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\\n\\n# 8\\npackage_to_hub(\\n    model=model,\\n    model_name=f\"a2c-{env_id}\",\\n    model_architecture=\"A2C\",\\n    env_id=env_id,\\n    eval_env=eval_env,\\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # TODO: Change the username\\n    commit_message=\"Initial commit\",\\n)\\n```\\n\\nSee you on Unit 7! üî•\\n\\n## Keep learning, stay awesome ü§ó', 'question': 'What is the name of the model saved in the context?\\n', 'answer': 'The name of the model saved in the context is \"a2c-PandaPickAndPlace-v3\".', 'source_doc': 'huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 5ne6jteFS01mESR4w5DAA)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'Pretrained LLMs can also be specialized or adapted for a specific task after pretraining, particularly when the weights are openly released. They are then used as a starting point for use cases and applications through a process called **fine-tuning**. Fine-tuning involves applying additional training steps on the model on a different ‚Äìoften more specialized and smaller‚Äì dataset to optimize it for a specific application. Even though this step has a cost in terms of compute power needed, it is usually much less costly than training a model from scratch, both financially and environmentally. This is one reason high-quality open-source pretrained models are very interesting, as they can be freely used and built upon by the community even when the practitioners have only access to a limited computing budget. \\n\\n## üóùÔ∏è 2022, from a race for size to a race for data\\nWhat open models were available to the community before 2023?', 'question': 'What is the process of optimizing a pretrained LLM for a specific application called?\\n', 'answer': 'The process of optimizing a pretrained LLM for a specific application is called fine-tuning.', 'source_doc': 'huggingface/blog/blob/main/2023-in-llms.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: vas-2ex6b8FO8Sw7SMK2f)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Ty-1MK-vPF-_IrzwYrNzY)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 64Cd-UHeps-hEyIHIdBK9)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '<frameworkcontent>\\n<pt>\\n\\nWe will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\\n\\n```py\\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\\n>>> encoding = image_processor(image, return_tensors=\"pt\")\\n>>> pixel_values = encoding.pixel_values.to(device)\\n```\\n\\nPass your input to the model and return the `logits`:\\n\\n```py\\n>>> outputs = model(pixel_values=pixel_values)\\n>>> logits = outputs.logits.cpu()\\n```\\n\\nNext, rescale the logits to the original image size:\\n\\n```py\\n>>> upsampled_logits = nn.functional.interpolate(\\n...     logits,\\n...     size=image.size[::-1],\\n...     mode=\"bilinear\",\\n...     align_corners=False,\\n... )\\n\\n>>> pred_seg = upsampled_logits.argmax(dim=1)[0]\\n```\\n\\n</pt>\\n</frameworkcontent>\\n\\n<frameworkcontent>\\n<tf>\\nLoad an image processor to preprocess the image and return the input as TensorFlow tensors:\\n\\n```py\\n>>> from transformers import AutoImageProcessor\\n\\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\\n>>> inputs = image_processor(image, return_tensors=\"tf\")\\n```\\n\\nPass your input to the model and return the `logits`:\\n\\n```py\\n>>> from transformers import TFAutoModelForSemanticSegmentation\\n\\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\\n>>> logits = model(**inputs).logits\\n```\\n\\nNext, rescale the logits to the original image size and apply argmax on the class dimension:\\n```py\\n>>> logits = tf.transpose(logits, [0, 2, 3, 1])\\n\\n>>> upsampled_logits = tf.image.resize(\\n...     logits,\\n...     # We reverse the shape of `image` because `image.size` returns width and height.\\n...     image.size[::-1],\\n... )\\n\\n>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\\n```\\n\\n</tf>\\n</frameworkcontent>', 'question': 'How to load an image processor in TensorFlow?\\n', 'answer': 'The image processor can be loaded in TensorFlow using `AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")`.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: Ty-1MK-vPF-_IrzwYrNzY)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'The following code requires roughly 12GB of GPU RAM.\\n\\n```python\\nfrom io import BytesIO\\nimport requests\\nimport torch\\nfrom diffusers import DiffusionPipeline\\nfrom PIL import Image\\nfrom transformers import CLIPFeatureExtractor, CLIPModel\\nfeature_extractor = CLIPFeatureExtractor.from_pretrained(\\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\\n)\\nclip_model = CLIPModel.from_pretrained(\\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", torch_dtype=torch.float16\\n)\\nguided_pipeline = DiffusionPipeline.from_pretrained(\\n    \"CompVis/stable-diffusion-v1-4\",\\n    # custom_pipeline=\"clip_guided_stable_diffusion\",\\n    custom_pipeline=\"/home/njindal/diffusers/examples/community/clip_guided_stable_diffusion.py\",\\n    clip_model=clip_model,\\n    feature_extractor=feature_extractor,\\n    torch_dtype=torch.float16,\\n)\\nguided_pipeline.enable_attention_slicing()\\nguided_pipeline = guided_pipeline.to(\"cuda\")\\nprompt = \"fantasy book cover, full moon, fantasy forest landscape, golden vector elements, fantasy magic, dark light night, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Albert Bierstadt, masterpiece\"\\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage = guided_pipeline(\\n    prompt=prompt,\\n    num_inference_steps=30,\\n    image=init_image,\\n    strength=0.75,\\n    guidance_scale=7.5,\\n    clip_guidance_scale=100,\\n    num_cutouts=4,\\n    use_cutouts=False,\\n).images[0]\\ndisplay(image)\\n```\\n\\nInit Image\\n\\n![img2img_init_clip_guidance](https://huggingface.co/datasets/njindal/images/resolve/main/clip_guided_img2img_init.jpg)\\n\\nOutput Image\\n\\n![img2img_clip_guidance](https://huggingface.co/datasets/njindal/images/resolve/main/clip_guided_img2img.jpg)\\n\\n### TensorRT Text2Image Stable Diffusion Pipeline', 'question': 'How much GPU RAM does the provided code require?\\n', 'answer': 'The provided code requires roughly 12GB of GPU RAM.', 'source_doc': 'huggingface/diffusers/blob/main/examples/community/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 64Cd-UHeps-hEyIHIdBK9)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: VpqarohtO2uqQ4k23-NN1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: iOFAd3Z9S4_zjPx54ghlu)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'Training with the previously defined hyper-parameters yields the following results:\\n\\n```bash\\nf1 = 93.15\\nexact_match = 86.91\\n```\\n\\nThis fine-tuned model is available as a checkpoint under the reference\\n[`bert-large-uncased-whole-word-masking-finetuned-squad`](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad).\\n\\n## Results\\n\\nLarger batch size may improve the performance while costing more memory.\\n\\n##### Results for SQuAD1.0 with the previously defined hyper-parameters:\\n\\n```python\\n{\\n\"exact\": 85.45884578997162,\\n\"f1\": 92.5974600601065,\\n\"total\": 10570,\\n\"HasAns_exact\": 85.45884578997162,\\n\"HasAns_f1\": 92.59746006010651,\\n\"HasAns_total\": 10570\\n}\\n```\\n\\n##### Results for SQuAD2.0 with the previously defined hyper-parameters:\\n\\n```python\\n{\\n\"exact\": 80.4177545691906,\\n\"f1\": 84.07154997729623,\\n\"total\": 11873,\\n\"HasAns_exact\": 76.73751686909581,\\n\"HasAns_f1\": 84.05558584352873,\\n\"HasAns_total\": 5928,\\n\"NoAns_exact\": 84.0874684608915,\\n\"NoAns_f1\": 84.0874684608915,\\n\"NoAns_total\": 5945\\n}\\n```', 'question': 'What is the f1 score for SQuAD1.0 with the previously defined hyper-parameters?\\n', 'answer': '92.59746006010651', 'source_doc': 'huggingface/transformers/blob/main/examples/legacy/question-answering/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: VpqarohtO2uqQ4k23-NN1)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': \"**Requirements**\\n\\nBefore we start, make sure you have met the following requirements\\n\\n* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\\n* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\\n* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\\n\\n**Helpful Resources**\\n\\n* [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\\n* [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner)\\n* [Optimum Habana Documentation](https://huggingface.co/docs/optimum/habana/index)\\n* [Pre-training script](./scripts/run_mlm.py)\\n* [Code: pre-training-bert.ipynb](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/pre-training-bert.ipynb)\\n\\n\\n## What is BERT?\\n\\nBERT, short for Bidirectional Encoder Representations from Transformers, is a Machine Learning (ML) model for natural language processing. It was developed in 2018 by researchers at Google AI Language and serves as a swiss army knife solution to 11+ of the most common language tasks, such as sentiment analysis and named entity recognition.\\n\\nRead more about BERT in our [BERT 101 ü§ó State Of The Art NLP Model Explained](https://huggingface.co/blog/bert-101) blog.\\n\\n## What is a Masked Language Modeling (MLM)?\\n\\nMLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.\\n\\n**Masked Language Modeling Example:**\\n\\n```bash\\n‚ÄúDang! I‚Äôm out fishing and a huge trout just [MASK] my line!‚Äù\\n```\\nRead more about Masked Language Modeling [here](https://huggingface.co/blog/bert-101).\\n\\n---\\n\\nLet's get started. üöÄ\", 'question': 'What is the name of the ML model for natural language processing mentioned in the context?\\n', 'answer': 'BERT', 'source_doc': 'huggingface/blog/blob/main/pretraining-bert.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: iOFAd3Z9S4_zjPx54ghlu)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: n5MAiX6hVRwmgnpZ7CVIQ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: G3ndQoL_4J4J4ICChcgya)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '### Forced Alignment\\n\\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language.\\nInspired by [this](https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html) Pytorch tutorial.\\n\\n#### Input Formats\\n\\n    Input format in script.txt              Input format in wavs directroy\\n    0000    sentence1                       0000.wav\\n    0001    sentence2                       0001.wav\\n    \\n#### Output Format\\n\\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\\n\\n    char    score   start_ms    end_ms\\n    h       0.25    1440        1520\\n    \\n#### Run command\\n\\n```\\npython alignment.py  \\\\\\n--model_name=\"arijitx/wav2vec2-xls-r-300m-bengali\" \\\\\\n--wav_dir=\"./wavs\"\\n--text_file=\"script.txt\" \\\\\\n--input_wavs_sr=48000 \\\\\\n--output_dir=\"./out_alignment\" \\\\\\n--cuda\\n```', 'question': 'What is the input format for the wav files in the context?\\n', 'answer': 'The input format for the wav files is a directory containing .wav files with the same names as the lines in the text file.', 'source_doc': 'huggingface/transformers/blob/main/examples/research_projects/wav2vec2/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: n5MAiX6hVRwmgnpZ7CVIQ)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': \">>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\", 'question': 'What is the name of the model with the highest probability?\\n', 'answer': 'Samoyed', 'source_doc': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/mobilenet-v2.mdx'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: G3ndQoL_4J4J4ICChcgya)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: AAXP5m_aTWW7uG6bLTtHk)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: e4ORAz_-mAbKqDOFSXjr6)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': \"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir <output path> \\\\\\n    --train_batch_size <batch size> \\\\\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\\\\n    --learning_rate 1e-4 \\\\\\n    --use_lora \\\\\\n    --pretrained_model_name_or_path huggingface/amused-512 \\\\\\n    --instance_data_dataset  'monadical-labs/minecraft-preview' \\\\\\n    --prompt_prefix 'minecraft ' \\\\\\n    --image_key image \\\\\\n    --prompt_key text \\\\\\n    --resolution 512 \\\\\\n    --mixed_precision fp16 \\\\\\n    --lr_scheduler constant \\\\\\n    --validation_prompts \\\\\\n        'minecraft Avatar' \\\\\\n        'minecraft character' \\\\\\n        'minecraft' \\\\\\n        'minecraft president' \\\\\\n        'minecraft pig' \\\\\\n    --max_train_steps 10000 \\\\\\n    --checkpointing_steps 500 \\\\\\n    --validation_steps 250 \\\\\\n    --gradient_checkpointing\\n```\\n\\n### Styledrop\\n\\n[Styledrop](https://arxiv.org/abs/2306.00983) is an efficient finetuning method for learning a new style from just one or very few images. It has an optional first stage to generate human picked additional training samples. The additional training samples can be used to augment the initial images. Our examples exclude the optional additional image selection stage and instead we just finetune on a single image.\\n\\nThis is our example style image:\\n![example](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/A%20mushroom%20in%20%5BV%5D%20style.png)\\n\\nDownload it to your local directory with\\n```sh\\nwget https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/A%20mushroom%20in%20%5BV%5D%20style.png\\n```\\n\\n#### 256\\n\\nExample results:\\n\\n![glowing_256_1](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/glowing_256_1.png) ![glowing_256_2](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/glowing_256_2.png) ![glowing_256_3](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/glowing_256_3.png)\\n\\nLearning rate: 4e-4, Gives decent results in 1500-2000 steps\", 'question': 'What is the learning rate used for the 256 resolution Styledrop example?\\n', 'answer': 'The learning rate used for the 256 resolution Styledrop example is 4e-4.', 'source_doc': 'huggingface/diffusers/blob/main/examples/amused/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: AAXP5m_aTWW7uG6bLTtHk)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'CodeParrot ü¶ú\\n<p align=\"center\">\\n    <img src=\"https://huggingface.co/datasets/lvwerra/repo-images/raw/main/code-highlighting-streamlit.png\" alt=\"drawing\" width=\"350\"/>\\n</p>\\n\\n## What is this about?\\nThis is an open-source effort to train and evaluate code generation models. CodeParrot ü¶ú is a GPT-2 model trained from scratch on Python code. The highlights of this project are:\\n- initialize and train a GPT-2 language model from scratch for code generation\\n- train a custom tokenizer adapted for Python code\\n- clean and deduplicate a large (>100GB) dataset with `datasets`\\n- train with `accelerate` on multiple GPUs using data parallelism and mixed precision\\n- continuously push checkpoints to the hub with `huggingface_hub`\\n- stream the dataset with `datasets` during training to avoid disk bottlenecks\\n- apply the `code_eval` metric in `datasets` to evaluate on [OpenAI\\'s _HumanEval_ benchmark](https://huggingface.co/datasets/openai_humaneval)\\n- showcase examples for downstream tasks with code models in [examples](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot/examples) folder:\\n    - Algorithmic complexity prediction\\n    - Code generation from english text\\n    - Code explanation\\n    \\n## Installation\\nTo install the dependencies simply run the following command:\\n```bash\\npip install -r requirements.txt\\n```\\n\\nTo reproduce the results you can follow the scripts in the following sections. Note that we don\\'t always show all possible arguments to the scripts. To get the full list of arguments with descriptions you can run the following command on any script:\\n\\n```bash\\npython scripts/some_script.py --help\\n```\\n\\nBefore you run any of the scripts make sure you are logged in and can push to the hub:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nAdditionally, sure you have git-lfs installed. You can find instructions for how to install it [here](https://git-lfs.github.com/).', 'question': 'What is the name of the GPT-2 model trained from scratch for code generation?\\n', 'answer': 'CodeParrot ü¶ú', 'source_doc': 'huggingface/transformers/blob/main/examples/research_projects/codeparrot/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: e4ORAz_-mAbKqDOFSXjr6)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: DDUCN1smehWwa1XdO1H4Y)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: C4V6e8Bg8q1A04jlQn5Qv)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\\n1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\\n1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\\n1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.', 'question': 'Which model was released by Meta AI with the paper \"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models\"?\\n', 'answer': 'XLM-V', 'source_doc': 'huggingface/transformers/blob/main/README_ru.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: DDUCN1smehWwa1XdO1H4Y)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (‡§´‡•á‡§∏‡§¨‡•Å‡§ï ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ‡§™‡•á‡§™‡§∞ [‡§≤‡§æ‡§∞‡•ç‡§ú-‡§∏‡•ç‡§ï‡•á‡§≤ ‡§∏‡•á‡§≤‡•ç‡§´- ‡§è‡§Ç‡§° ‡§∏‡•á‡§Æ‡•Ä-‡§∏‡•Å‡§™‡§∞‡§µ‡§æ‡§á‡§ú‡•ç‡§° ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§´‡•â‡§∞ ‡§∏‡•ç‡§™‡•Ä‡§ö ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§≤‡•á‡§∂‡§®](https://arxiv.org/abs/2104.06678) ‡§ö‡§æ‡§Ç‡§ó‡§π‡§æ‡§® ‡§µ‡§æ‡§Ç‡§ó, ‡§ê‡§®‡•Ä ‡§µ‡•Ç, ‡§ú‡•Å‡§Ü‡§® ‡§™‡§ø‡§®‡•ã, ‡§è‡§≤‡•á‡§ï‡•ç‡§∏‡•Ä ‡§¨‡•á‡§µ‡§∏‡•ç‡§ï‡•Ä, ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§î‡§≤‡•Ä, ‡§è‡§≤‡•á‡§ï‡•ç‡§∏‡§ø‡§∏ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ Conneau ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ã‡§∏‡•ç‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§\\n1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (‡§§‡•á‡§≤ ‡§Ö‡§µ‡•Ä‡§µ ‡§Ø‡•Ç‡§®‡§ø‡§µ‡§∞‡•ç‡§∏‡§ø‡§ü‡•Ä ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ‡§™‡•á‡§™‡§∞ [‡§∏‡•ç‡§™‡•à‡§® ‡§∏‡§ø‡§≤‡•á‡§ï‡•ç‡§∂‡§® ‡§ï‡•ã ‡§™‡•ç‡§∞‡•Ä-‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡§∞‡§ï‡•á ‡§ï‡•Å‡§õ-‡§∂‡•â‡§ü ‡§ï‡•ç‡§µ‡•á‡§∂‡•ç‡§ö‡§® ‡§Ü‡§Ç‡§∏‡§∞‡§ø‡§Ç‡§ó](https:// arxiv.org/abs/2101.00438) ‡§ì‡§∞‡§ø ‡§∞‡§æ‡§Æ, ‡§Ø‡•Å‡§µ‡§≤ ‡§ï‡§∞‡•ç‡§∏‡•ç‡§ü‡§®, ‡§ú‡•ã‡§®‡§æ‡§•‡§® ‡§¨‡•á‡§∞‡•á‡§Ç‡§ü, ‡§Ö‡§Æ‡•Ä‡§∞ ‡§ó‡•ç‡§≤‡•ã‡§¨‡§∞‡•ç‡§∏‡§®, ‡§ì‡§Æ‡§∞ ‡§≤‡•á‡§µ‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡•§\\n1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (‡§¨‡§∞‡•ç‡§ï‡§≤‡•á ‡§∏‡•á) ‡§ï‡§æ‡§ó‡§ú ‡§ï‡•á ‡§∏‡§æ‡§• [SqueezeBERT: ‡§ï‡•Å‡§∂‡§≤ ‡§§‡§Ç‡§§‡•ç‡§∞‡§ø‡§ï‡§æ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç NLP ‡§ï‡•ã ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§µ‡§ø‡§ú‡§º‡§® ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§ø‡§ñ‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à?](https: //arxiv.org/abs/2006.11316) ‡§´‡•â‡§∞‡•á‡§∏‡•ç‡§ü ‡§è‡§®. ‡§á‡§®‡§°‡•ã‡§≤‡§æ, ‡§Ö‡§≤‡•ç‡§¨‡§∞‡•ç‡§ü ‡§à. ‡§∂‡•â, ‡§∞‡§µ‡§ø ‡§ï‡•É‡§∑‡•ç‡§£‡§æ, ‡§î‡§∞ ‡§ï‡§∞‡•ç‡§ü ‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç. ‡§ï‡•á‡§ü‡§ú‡§º‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡•§\\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (MBZUAI ‡§∏‡•á) Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ\\n1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§∏‡•â‡§´‡•ç‡§ü ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§ó‡§ú [‡§∏‡•ç‡§µ‡§æ‡§á‡§® ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞: ‡§∂‡§ø‡§´‡•ç‡§ü‡•á‡§° ‡§µ‡§ø‡§Ç‡§°‡•ã‡§ú ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§™‡§¶‡§æ‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§ ‡§µ‡§ø‡§ú‡§® ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞](https://arxiv .org/abs/2103.14030) ‡§ú‡§º‡•Ä ‡§≤‡§ø‡§Ø‡•Ç, ‡§Ø‡•Å‡§ü‡•ã‡§Ç‡§ó ‡§≤‡§ø‡§®, ‡§Ø‡•Ç ‡§ï‡§æ‡§ì, ‡§π‡§æ‡§® ‡§π‡•Ç, ‡§Ø‡§ø‡§ï‡•ç‡§∏‡•Å‡§Ü‡§® ‡§µ‡•á‡§à, ‡§ù‡•á‡§Ç‡§ó ‡§ù‡§æ‡§Ç‡§ó, ‡§∏‡•ç‡§ü‡•Ä‡§´‡§® ‡§≤‡§ø‡§®, ‡§¨‡•à‡§®‡§ø‡§Ç‡§ó ‡§ó‡•Å‡§ì ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡•§\\n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (Microsoft ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§µ‡§æ‡§≤‡§æ ‡§™‡•á‡§™‡§∞ [Swin Transformer V2: ‡§∏‡•ç‡§ï‡•á‡§≤‡§ø‡§Ç‡§ó ‡§Ö‡§™ ‡§ï‡•à‡§™‡•á‡§∏‡§ø‡§ü‡•Ä ‡§è‡§Ç‡§° ‡§∞‡•á‡§ú‡•ã‡§≤‡•ç‡§Ø‡•Ç‡§∂‡§®](https:// ‡§ú‡§º‡•Ä ‡§≤‡§ø‡§Ø‡•Ç, ‡§π‡§æ‡§® ‡§π‡•Ç, ‡§Ø‡•Å‡§ü‡•ã‡§Ç‡§ó ‡§≤‡§ø‡§®, ‡§ú‡§º‡•Å‡§≤‡§ø‡§Ü‡§Ç‡§ó ‡§Ø‡§æ‡§ì, ‡§ú‡§º‡•á‡§Ç‡§°‡§æ ‡§ú‡§º‡•Ä, ‡§Ø‡§ø‡§ï‡•ç‡§∏‡•Å‡§Ü‡§® ‡§µ‡•á‡§à, ‡§ú‡§ø‡§Ø‡§æ ‡§®‡§ø‡§Ç‡§ó, ‡§Ø‡•Ç ‡§ï‡§æ‡§ì, ‡§ù‡•á‡§Ç‡§ó ‡§ù‡§æ‡§Ç‡§ó, ‡§≤‡•Ä ‡§°‡•ã‡§Ç‡§ó, ‡§´‡•Å‡§∞‡•Å ‡§µ‡•á‡§à, ‡§¨‡•à‡§®‡§ø‡§Ç‡§ó ‡§ó‡•Å‡§ì ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ arxiv.org/abs/2111.09883‡•§', 'question': 'Which model is from Microsoft?\\n', 'answer': 'Swin Transformer V2', 'source_doc': 'huggingface/transformers/blob/main/README_hd.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: C4V6e8Bg8q1A04jlQn5Qv)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: wgqnsfjiqfcpFSNVEhkZN)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 16MWyRTGqDi5lgAxeJ5BA)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'const response = await fetch(INFERENCE_URL, {\\n\\t\\tmethod: \"POST\",\\n\\t\\tbody: JSON.stringify({ inputs: PROMPT + req.body.comment.content }),\\n\\t});\\n\\tif (response.ok) {\\n\\t\\tconst output = await response.json();\\n\\t\\tconst continuationText = output[0].generated_text.replace(\\n\\t\\t\\tPROMPT + req.body.comment.content,\\n\\t\\t\\t\"\"\\n\\t\\t);\\n\\t\\t...\\n```\\n\\nThis is the coolest part: we call the Inference API for the BLOOM model, prompting it with `PROMPT`, and we get the continuation text, i.e., the part generated by the model.\\n\\nFinally, we will post it as a reply in the same discussion thread:\\n\\n```ts\\n\\tconst commentUrl = req.body.discussion.url.api + \"/comment\";\\n\\n\\tconst commentApiResponse = await fetch(commentUrl, {\\n\\t\\tmethod: \"POST\",\\n\\t\\theaders: {\\n\\t\\t\\tAuthorization: `Bearer ${process.env.HF_TOKEN}`,\\n\\t\\t\\t\"Content-Type\": \"application/json\",\\n\\t\\t},\\n\\t\\tbody: JSON.stringify({ comment: continuationText }),\\n\\t});\\n\\n\\tconst apiOutput = await commentApiResponse.json();\\n```\\n\\n## Configure your Webhook to send events to your Space\\n\\nLast but not least, you\\'ll need to configure your Webhook to send POST requests to your Space.\\n\\nLet\\'s first grab our Space\\'s \"direct URL\" from the contextual menu. Click on \"Embed this Space\" and copy the \"Direct URL\".\\n\\n![embed this Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/embed-space.png)\\n![direct URL](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/direct-url.png)\\n\\nUpdate your webhook to send requests to that URL:\\n\\n![webhook settings](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/webhook-creation.png)\\n\\n\\n## Result\\n\\n![discussion-result](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/discussion-result.png)', 'question': 'What is the method used to send requests to the Space?\\n', 'answer': 'The method used to send requests to the Space is POST.', 'source_doc': 'huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: wgqnsfjiqfcpFSNVEhkZN)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n## Language generation\\n\\nBased on the script [`run_generation.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py).\\n\\nConditional text generation using the auto-regressive models of the library: GPT, GPT-2, GPTJ, Transformer-XL, XLNet, CTRL, BLOOM, LLAMA, OPT.\\nA similar script is used for our official demo [Write With Transfomer](https://transformer.huggingface.co), where you\\ncan try out the different models available in the library.\\n\\nExample usage:\\n\\n```bash\\npython run_generation.py \\\\\\n    --model_type=gpt2 \\\\\\n    --model_name_or_path=gpt2\\n```', 'question': 'What is the name of the script used for conditional text generation in the transformers library?\\n', 'answer': 'The name of the script is `run_generation.py`.', 'source_doc': 'huggingface/transformers/blob/main/examples/pytorch/text-generation/README.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 16MWyRTGqDi5lgAxeJ5BA)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 49lYNWbSGkkoqyl78itmG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: idJKOP_vDb7TBJS4OUfLG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '- [#5312](https://github.com/gradio-app/gradio/pull/5312) [`f769cb67`](https://github.com/gradio-app/gradio/commit/f769cb67149d8e209091508f06d87014acaed965) - only start listening for events after the components are mounted.  Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5254](https://github.com/gradio-app/gradio/pull/5254) [`c39f06e1`](https://github.com/gradio-app/gradio/commit/c39f06e16b9feea97984e4822df35a99c807461c) - Fix `.update()` for `gr.Radio()` and `gr.CheckboxGroup()`.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n- [#5231](https://github.com/gradio-app/gradio/pull/5231) [`87f1c2b4`](https://github.com/gradio-app/gradio/commit/87f1c2b4ac7c685c43477215fa5b96b6cbeffa05) - Allow `gr.Interface.from_pipeline()` and `gr.load()` to work within `gr.Blocks()`.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n- [#5238](https://github.com/gradio-app/gradio/pull/5238) [`de23e9f7`](https://github.com/gradio-app/gradio/commit/de23e9f7d67e685e791faf48a21f34121f6d094a) - Improve audio streaming.  Thanks [@aliabid94](https://github.com/aliabid94)!/n  - Proper audio streaming with WAV files. We now do the proper processing to stream out wav files as a single stream of audio without any cracks in the seams./n  - Audio streaming with bytes. Stream any audio type by yielding out bytes, and it should work flawlessly.\\n- [#5313](https://github.com/gradio-app/gradio/pull/5313) [`54bcb724`](https://github.com/gradio-app/gradio/commit/54bcb72417b2781ad9d7500ea0f89aa9d80f7d8f) - Restores missing part of bottom border on file component.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n- [#5235](https://github.com/gradio-app/gradio/pull/5235) [`1ecf88ac`](https://github.com/gradio-app/gradio/commit/1ecf88ac5f20bc5a1c91792d1a68559575e6afd7) - fix #5229.  Thanks [@breengles](https://github.com/breengles)!', 'question': 'Which Gradio components were fixed to have their bottom border restored?\\n', 'answer': 'The file component', 'source_doc': 'gradio-app/gradio/blob/main/gradio/CHANGELOG.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 49lYNWbSGkkoqyl78itmG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': \"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Fixed importing gradio can cause PIL.Image.registered_extensions() to break by `[@aliencaocao](https://github.com/aliencaocao)` in `[PR 2846](https://github.com/gradio-app/gradio/pull/2846)`\\n- Fix css glitch and navigation in docs by [@aliabd](https://github.com/aliabd) in [PR 2856](https://github.com/gradio-app/gradio/pull/2856)\\n- Added the ability to set `x_lim`, `y_lim` and legend positions for `gr.ScatterPlot` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2807](https://github.com/gradio-app/gradio/pull/2807)\\n- Remove footers and min-height the correct way by [@aliabd](https://github.com/aliabd) in [PR 2860](https://github.com/gradio-app/gradio/pull/2860)\\n\\n### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.14.0\\n\\n### New Features:\\n\\n###### Add Waveform Visual Support to Audio\\n\\nAdds a `gr.make_waveform()` function that creates a waveform video by combining an audio and an optional background image by [@dawoodkhan82](http://github.com/dawoodkhan82) and [@aliabid94](http://github.com/aliabid94) in [PR 2706](https://github.com/gradio-app/gradio/pull/2706. Helpful for making audio outputs much more shareable.\\n\\n![waveform screenrecording](https://user-images.githubusercontent.com/7870876/206062396-164a5e71-451a-4fe0-94a7-cbe9269d57e6.gif)\\n\\n###### Allows Every Component to Accept an `every` Parameter\\n\\nWhen a component's initial value is a function, the `every` parameter re-runs the function every `every` seconds. By [@abidlabs](https://github.com/abidlabs) in [PR 2806](https://github.com/gradio-app/gradio/pull/2806). Here's a code example:\\n\\n```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    df = gr.DataFrame(run_query, every=60*60)\\n\\ndemo.queue().launch()\\n```\\n\\n### Bug Fixes:\", 'question': 'What is the name of the function that creates a waveform video by combining an audio and an optional background image?\\n', 'answer': '`gr.make_waveform()`', 'source_doc': 'gradio-app/gradio/blob/main/gradio/CHANGELOG.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: idJKOP_vDb7TBJS4OUfLG)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: tgJlBKt3YJfb7M-TXuFq7)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: TpmXWQDDd2jkwDtAYW25W)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '--\\n{card_data}\\n---\\n\\n# {{ pretty_name | default(\"Dataset Name\", true)}}\\n\\n{{ some_data }}', 'question': 'What is the name of the dataset?\\n', 'answer': '{{ pretty_name | default(\"Dataset Name\", true)}}', 'source_doc': 'huggingface/huggingface_hub/blob/main/tests/fixtures/cards/sample_datasetcard_template.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: tgJlBKt3YJfb7M-TXuFq7)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': 'Working with Keras and Tensorflow\\n\\n\\n\\nEvaluate can be easily intergrated into your Keras and Tensorflow workflow. We\\'ll demonstrate two ways of incorporating Evaluate into model training, using the Fashion MNIST example dataset. We\\'ll train a standard classifier to predict two classes from this dataset, and show how to use a metric as a callback during training or afterwards for evaluation. \\n\\n\\n```python\\nimport numpy as np\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nimport evaluate\\n\\n# We pull example code from Keras.io\\'s guide on classifying with MNIST\\n# Located here: https://keras.io/examples/vision/mnist_convnet/\\n\\n# Model / data parameters\\ninput_shape = (28, 28, 1)\\n\\n# Load the data and split it between train and test sets\\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\\n\\n\\n# Only select tshirts/tops and trousers, classes 0 and 1\\ndef get_tshirts_tops_and_trouser(x_vals, y_vals):\\n    mask = np.where((y_vals == 0) | (y_vals == 1))\\n    return x_vals[mask], y_vals[mask]\\n\\nx_train, y_train = get_tshirts_tops_and_trouser(x_train, y_train)\\nx_test, y_test = get_tshirts_tops_and_trouser(x_test, y_test)\\n\\n\\n# Scale images to the [0, 1] range\\nx_train = x_train.astype(\"float32\") / 255\\nx_test = x_test.astype(\"float32\") / 255\\n\\nx_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\n\\n\\nmodel = keras.Sequential(\\n    [\\n        keras.Input(shape=input_shape),\\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Flatten(),\\n        layers.Dropout(0.5),\\n        layers.Dense(1, activation=\"sigmoid\"),\\n    ]\\n)\\n```\\n\\n## Callbacks\\n\\nSuppose we want to keep track of model metrics while a model is training. We can use a Callback in order to calculate this metric during training, after an epoch ends.', 'question': 'What is a Callback in Keras?\\n', 'answer': 'In Keras, a Callback is a class that can be used to perform actions at various stages of the training process, such as after each epoch. It can be used to calculate and keep track of model metrics during training.', 'source_doc': 'huggingface/evaluate/blob/main/docs/source/keras_integrations.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: TpmXWQDDd2jkwDtAYW25W)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: LOfeiN33vxK3XKcJ8wOUW)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: UbqUJtZ2Hcgo472inJDf9)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': 'Gated datasets\\n\\nTo give more control over how datasets are used, the Hub allows datasets authors to enable **access requests** for their datasets. Users must agree to share their contact information (username and email address) with the datasets authors to access the datasets files when enabled. Datasets authors can configure this request with additional fields. A dataset with access requests enabled is called a **gated dataset**. Access requests are always granted to individual users rather than to entire organizations. A common use case of gated datasets is to provide access to early research datasets before the wider release.\\n\\n## Manage gated datasets as a dataset author\\n\\n<a id=\"manual-approval\"></a> <!-- backward compatible anchor -->\\n<a id=\"notifications-settings\"></a> <!-- backward compatible anchor -->\\n\\n\\nTo enable access requests, go to the dataset settings page. By default, the dataset is not gated. Click on **Enable Access request** in the top-right corner.\\n\\n\\n<div class=\"flex justify-center\">\\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-disabled.png\"/>\\n    <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-disabled-dark.png\"/>\\n</div>\\n\\nBy default, access to the dataset is automatically granted to the user when requesting it. This is referred to as **automatic approval**. In this mode, any user can access your dataset once they\\'ve shared their personal information with you.\\n\\n<div class=\"flex justify-center\">\\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-enabled.png\"/>\\n    <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-enabled-dark.png\"/>\\n</div>', 'question': 'How does a user access a gated dataset?\\n', 'answer': 'A user accesses a gated dataset by sharing their contact information (username and email address) with the dataset author.', 'source_doc': 'huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: LOfeiN33vxK3XKcJ8wOUW)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': \"The abstract from the paper is the following:\\n\\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\\n\\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\\n\\n\\n<Tip warning={true}>\\n\\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\\n\\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\\n\\n</Tip>\\n\\n\\n## IdeficsConfig\\n\\n[[autodoc]] IdeficsConfig\\n\\n## IdeficsModel\\n\\n[[autodoc]] IdeficsModel\\n    - forward\\n\\n## IdeficsForVisionText2Text\\n\\n[[autodoc]] IdeficsForVisionText2Text\\n    - forward\\n\\n## IdeficsImageProcessor\\n\\n[[autodoc]] IdeficsImageProcessor\\n    - preprocess\\n\\n## IdeficsProcessor\\n\\n[[autodoc]] IdeficsProcessor\\n    - __call__\", 'question': 'How many parameters does the vision and language model trained on the OBELISC dataset have?\\n', 'answer': 'The vision and language model trained on the OBELISC dataset has 80 billion parameters.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/idefics.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: UbqUJtZ2Hcgo472inJDf9)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: HArhP_Is5ipiFQpnlzyjh)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: ekwFz0qgEVlc7TqdfmTqp)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# OWLv2\\n\\n## Overview\\n\\nOWLv2 was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up [OWL-ViT](owlvit) using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection.\\n\\nThe abstract from the paper is the following:', 'question': 'What are the gains of OWLv2 over the previous state-of-the-art for zero-shot object detection?\\n', 'answer': 'OWLv2 results in large gains over the previous state-of-the-art for zero-shot object detection.', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/owlv2.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: HArhP_Is5ipiFQpnlzyjh)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Unexpected error for output {'context': '```python\\n# forward diffusion (using the nice property)\\ndef q_sample(x_start, t, noise=None):\\n    if noise is None:\\n        noise = torch.randn_like(x_start)\\n\\n    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\\n    sqrt_one_minus_alphas_cumprod_t = extract(\\n        sqrt_one_minus_alphas_cumprod, t, x_start.shape\\n    )\\n\\n    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\\n```\\n\\nLet\\'s test it on a particular time step:\\n\\n```python\\ndef get_noisy_image(x_start, t):\\n  # add noise\\n  x_noisy = q_sample(x_start, t=t)\\n\\n  # turn back into PIL image\\n  noisy_image = reverse_transform(x_noisy.squeeze())\\n\\n  return noisy_image\\n```\\n\\n```python\\n# take time step\\nt = torch.tensor([40])\\n\\nget_noisy_image(x_start, t)\\n```\\n\\n<img src=\"assets/78_annotated-diffusion/output_cats_noisy.png\" width=\"100\" />\\n\\nLet\\'s visualize this for various time steps:\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# use seed for reproducability\\ntorch.manual_seed(0)\\n\\n# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\\ndef plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\\n    if not isinstance(imgs[0], list):\\n        # Make a 2d grid even if there\\'s just 1 row\\n        imgs = [imgs]\\n\\n    num_rows = len(imgs)\\n    num_cols = len(imgs[0]) + with_orig\\n    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\\n    for row_idx, row in enumerate(imgs):\\n        row = [image] + row if with_orig else row\\n        for col_idx, img in enumerate(row):\\n            ax = axs[row_idx, col_idx]\\n            ax.imshow(np.asarray(img), **imshow_kwargs)\\n            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\\n\\n    if with_orig:\\n        axs[0, 0].set(title=\\'Original image\\')\\n        axs[0, 0].title.set_size(8)\\n    if row_title is not None:\\n        for row_idx in range(num_rows):\\n            axs[row_idx, 0].set(ylabel=row_title[row_idx])', 'question': 'What is the name of the function that adds noise to the original image?\\n', 'answer': 'The name of the function is `get_noisy_image`.', 'source_doc': 'huggingface/blog/blob/main/annotated-diffusion.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: ekwFz0qgEVlc7TqdfmTqp)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected error in call_llm: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: uUSTjuIIjveicJQfa7VMO)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for output {'context': '```py\\nfrom transformers import BitsAndBytesConfig\\n\\nnf4_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n)\\n\\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\\n```\\n\\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype` and `torch_dtype` values.\\n\\n#### Nested quantization\\n\\nNested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an addition 0.4 bits/parameter. For example, with nested quantization, you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b) model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps.\\n\\n```py\\nfrom transformers import BitsAndBytesConfig\\n\\ndouble_quant_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n)\\n\\nmodel_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b\", quantization_config=double_quant_config)\\n```\\n\\n## Optimum\\n\\nThe [Optimum](https://huggingface.co/docs/optimum/index) library supports quantization for Intel, Furiosa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions. Consider using Optimum for quantization if you\\'re using specific and optimized hardware like Intel CPUs, Furiosa NPUs or a model accelerator like ONNX Runtime.\\n\\n## Benchmarks', 'question': 'What is the technique that can save additional memory at no additional performance cost?\\n', 'answer': 'Nested quantization', 'source_doc': 'huggingface/transformers/blob/main/docs/source/en/quantization.md'}: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: uUSTjuIIjveicJQfa7VMO)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset daily). Please subscribe to PRO or Enterprise Hub to get a higher limit: https://hf.co/pricing\n",
      "Generated QA couples saved to ./generated_QAs/critiqued_qa_couples.json\n",
      "Below is the CRITIQUED QA couple:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>groundedness_eval</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_eval</th>\n",
       "      <th>standalone_score</th>\n",
       "      <th>standalone_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all</td>\n",
       "      <td>What is the name of the class for implementing a new pipeline?\\n</td>\n",
       "      <td>Implementing a new pipeline</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nContext: Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all\\n\\nAnswer::: \\nEvaluation: The context provides a clear example of how to implement a new pipeline by subclassing an existing pipeline. The context also provides a link to the documentation for implementing a new pipeline. Therefore, the question is clearly and unambiguously answerable with the context.\\nTotal rating: 5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nAnswer::: \\nThe class for implementing a new pipeline is `Pipeline`.\\n\\nEvaluation: This question is useful for developers who want to build custom pipelines using the Hugging Face ecosystem. The `Pipeline` class is a fundamental component of the Hugging Face Transformers library, and understanding how to implement a new pipeline can help developers create more sophisticated NLP applications.\\n\\nTotal rating: 4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nAnswer::: \\nThe name of the class for implementing a new pipeline is `Pipeline`.\\n\\nEvaluation: The question is asking about a class, which is a fundamental concept in object-oriented programming. The name of the class is provided, which is `Pipeline`. This question is context-independent and can be answered without any additional information.\\n\\nTotal rating: 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all   \n",
       "\n",
       "                                                           question  \\\n",
       "0  What is the name of the class for implementing a new pipeline?\\n   \n",
       "\n",
       "                        answer  \\\n",
       "0  Implementing a new pipeline   \n",
       "\n",
       "                                                                    source_doc  \\\n",
       "0  huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md   \n",
       "\n",
       "   groundedness_score  \\\n",
       "0                 5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        groundedness_eval  \\\n",
       "0  (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nContext: Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\\ncases, so `transformers` could maybe support your use case.\\n\\n\\nIf you want to try simply you can:\\n\\n- Subclass your pipeline of choice\\n\\n```python\\nclass MyPipeline(TextClassificationPipeline):\\n    def postprocess():\\n        # Your code goes here\\n        scores = scores * 100\\n        # And here\\n\\n\\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\\n# or if you use *pipeline* function, then:\\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\\n```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Implementing a new pipeline](../add_new_pipeline)\\n\\n## Audio\\n\\nPipelines available for audio tasks include the following.\\n\\n### AudioClassificationPipeline\\n\\n[[autodoc]] AudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n### AutomaticSpeechRecognitionPipeline\\n\\n[[autodoc]] AutomaticSpeechRecognitionPipeline\\n    - __call__\\n    - all\\n\\n### TextToAudioPipeline\\n\\n[[autodoc]] TextToAudioPipeline\\n    - __call__\\n    - all\\n\\n\\n### ZeroShotAudioClassificationPipeline\\n\\n[[autodoc]] ZeroShotAudioClassificationPipeline\\n    - __call__\\n    - all\\n\\n## Computer vision\\n\\nPipelines available for computer vision tasks include the following.\\n\\n### DepthEstimationPipeline\\n[[autodoc]] DepthEstimationPipeline\\n    - __call__\\n    - all\\n\\n### ImageClassificationPipeline\\n\\n[[autodoc]] ImageClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ImageSegmentationPipeline\\n\\n[[autodoc]] ImageSegmentationPipeline\\n    - __call__\\n    - all\\n\\n### ImageToImagePipeline\\n\\n[[autodoc]] ImageToImagePipeline\\n    - __call__\\n    - all\\n\\n### ObjectDetectionPipeline\\n\\n[[autodoc]] ObjectDetectionPipeline\\n    - __call__\\n    - all\\n\\n### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPipeline\\n\\n[[autodoc]] ZeroShotImageClassificationPipeline\\n    - __call__\\n    - all\\n\\nAnswer::: \\nEvaluation: The context provides a clear example of how to implement a new pipeline by subclassing an existing pipeline. The context also provides a link to the documentation for implementing a new pipeline. Therefore, the question is clearly and unambiguously answerable with the context.\\nTotal rating: 5   \n",
       "\n",
       "   relevance_score  \\\n",
       "0              4.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         relevance_eval  \\\n",
       "0  (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nAnswer::: \\nThe class for implementing a new pipeline is `Pipeline`.\\n\\nEvaluation: This question is useful for developers who want to build custom pipelines using the Hugging Face ecosystem. The `Pipeline` class is a fundamental component of the Hugging Face Transformers library, and understanding how to implement a new pipeline can help developers create more sophisticated NLP applications.\\n\\nTotal rating: 4.5   \n",
       "\n",
       "   standalone_score  \\\n",
       "0               5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    standalone_eval  \n",
       "0  (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the name of the class for implementing a new pipeline?\\n\\n\\nAnswer::: \\nThe name of the class for implementing a new pipeline is `Pipeline`.\\n\\nEvaluation: The question is asking about a class, which is a fundamental concept in object-oriented programming. The name of the class is provided, which is `Pipeline`. This question is context-independent and can be answered without any additional information.\\n\\nTotal rating: 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re # importing the regex library\n",
    "\n",
    "# Save outputs to a JSON file\n",
    "QA_critiqued_couples_path = './generated_QAs/critiqued_qa_couples.json'\n",
    "\n",
    "# This is a helper function to parse the evaluation response\n",
    "def parse_evaluation(evaluation):\n",
    "    try:\n",
    "        # Use regex to extract 'Evaluation' and 'Total rating'\n",
    "        eval_match = re.search(r\"Evaluation:\\s*(.+)\", evaluation, re.DOTALL)\n",
    "        score_match = re.search(r\"Total rating:\\s*(\\d+)\", evaluation)\n",
    "        \n",
    "        # Ensure both matches were found\n",
    "        if not eval_match or not score_match:\n",
    "            raise ValueError(\"Missing 'Evaluation' or 'Total rating' in response\")\n",
    "        \n",
    "        eval_text = eval_match.group(1).strip()\n",
    "        score = int(score_match.group(1))\n",
    "        return score, eval_text\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing evaluation: {e}\")\n",
    "\n",
    "# This is the main function to generate critiques for each QA couple\n",
    "def generate_critiqued_QA(QA_initial_outputs):\n",
    "    \n",
    "    # Create a new list to store the critiqued QA couples \n",
    "    # initialize it with the initial QA couples\n",
    "    QA_critiqued_outputs = QA_initial_outputs\n",
    "    \n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    \n",
    "    for output in tqdm(QA_critiqued_outputs):\n",
    "        try:\n",
    "            evaluations = {\n",
    "                \"groundedness\": call_llm(\n",
    "                    llm_client,\n",
    "                    question_groundedness_critique_prompt.format(\n",
    "                        context=output[\"context\"], question=output[\"question\"]\n",
    "                    ),\n",
    "                ),\n",
    "                \"relevance\": call_llm(\n",
    "                    llm_client,\n",
    "                    question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "                ),\n",
    "                \"standalone\": call_llm(\n",
    "                    llm_client,\n",
    "                    question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "                ),\n",
    "            }\n",
    "            for criterion, evaluation in evaluations.items():\n",
    "                try:\n",
    "                    score, eval_text = parse_evaluation(evaluation)\n",
    "                    output.update(\n",
    "                        {\n",
    "                            f\"{criterion}_score\": score,\n",
    "                            f\"{criterion}_eval\": eval_text,\n",
    "                        }\n",
    "                    )\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Error parsing score or evaluation for criterion '{criterion}': {ve}\")\n",
    "        except Exception as e:\n",
    "                print(f\"Unexpected error for output {output}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(QA_critiqued_couples_path), exist_ok=True)\n",
    "\n",
    "    with open(QA_critiqued_couples_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(QA_critiqued_outputs, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Generated QA couples saved to {QA_critiqued_couples_path}\")\n",
    "\n",
    "    return QA_critiqued_outputs\n",
    "\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(QA_critiqued_couples_path) and os.path.getsize(QA_critiqued_couples_path) > 0:\n",
    "    try:\n",
    "        with open(QA_critiqued_couples_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            QA_critiqued_outputs = json.load(f)\n",
    "            print(f\"Successfully LOADED {len(QA_critiqued_outputs)} QA Critiques\")\n",
    "    except (json.JSONDecodeError, OSError) as e:\n",
    "        print(f\"Error loading JSON file: {e}. Recreating dataset.\")\n",
    "        QA_critiqued_outputs = generate_critiqued_QA(QA_initial_outputs)\n",
    "else:\n",
    "    QA_critiqued_outputs = generate_critiqued_QA(QA_initial_outputs)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Display the first QA couple to check the format\n",
    "print(f\"Below is the CRITIQUED QA couple:\")\n",
    "display(pd.DataFrame(QA_critiqued_outputs).head(1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQv36Y_f9jVO"
   },
   "source": [
    "Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oBWuOu1b9jVO",
    "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the class for implementing a new pipeline?\\n</td>\n",
       "      <td>Implementing a new pipeline</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which Gradio component can be used to identify the specific verse that was found by the semantic search?\\n</td>\n",
       "      <td>The HighlightedText Gradio component</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the LLM powered by Hugging Face Inference Endpoints and used for translation?\\n</td>\n",
       "      <td>falcon-7b-instruct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the paper that provides more detail and context for the AI Act?\\n</td>\n",
       "      <td>The name of the paper is \"supporting_OS_in_the_AIAct.pdf\".</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When does the Hugging Face Ambassador Program end?\\n</td>\n",
       "      <td>The Hugging Face Ambassador Program ends on December 31, 2022.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>How does a user access a gated dataset?\\n</td>\n",
       "      <td>A user accesses a gated dataset by sharing their contact information (username and email address) with the dataset author.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>How many parameters does the vision and language model trained on the OBELISC dataset have?\\n</td>\n",
       "      <td>The vision and language model trained on the OBELISC dataset has 80 billion parameters.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>What are the gains of OWLv2 over the previous state-of-the-art for zero-shot object detection?\\n</td>\n",
       "      <td>OWLv2 results in large gains over the previous state-of-the-art for zero-shot object detection.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>What is the name of the function that adds noise to the original image?\\n</td>\n",
       "      <td>The name of the function is `get_noisy_image`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>What is the technique that can save additional memory at no additional performance cost?\\n</td>\n",
       "      <td>Nested quantization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       question  \\\n",
       "0                                              What is the name of the class for implementing a new pipeline?\\n   \n",
       "1    Which Gradio component can be used to identify the specific verse that was found by the semantic search?\\n   \n",
       "2           What is the name of the LLM powered by Hugging Face Inference Endpoints and used for translation?\\n   \n",
       "3                         What is the name of the paper that provides more detail and context for the AI Act?\\n   \n",
       "4                                                          When does the Hugging Face Ambassador Program end?\\n   \n",
       "..                                                                                                          ...   \n",
       "281                                                                   How does a user access a gated dataset?\\n   \n",
       "282               How many parameters does the vision and language model trained on the OBELISC dataset have?\\n   \n",
       "283            What are the gains of OWLv2 over the previous state-of-the-art for zero-shot object detection?\\n   \n",
       "284                                   What is the name of the function that adds noise to the original image?\\n   \n",
       "285                  What is the technique that can save additional memory at no additional performance cost?\\n   \n",
       "\n",
       "                                                                                                                         answer  \\\n",
       "0                                                                                                   Implementing a new pipeline   \n",
       "1                                                                                          The HighlightedText Gradio component   \n",
       "2                                                                                                            falcon-7b-instruct   \n",
       "3                                                                    The name of the paper is \"supporting_OS_in_the_AIAct.pdf\".   \n",
       "4                                                                The Hugging Face Ambassador Program ends on December 31, 2022.   \n",
       "..                                                                                                                          ...   \n",
       "281  A user accesses a gated dataset by sharing their contact information (username and email address) with the dataset author.   \n",
       "282                                     The vision and language model trained on the OBELISC dataset has 80 billion parameters.   \n",
       "283                             OWLv2 results in large gains over the previous state-of-the-art for zero-shot object detection.   \n",
       "284                                                                              The name of the function is `get_noisy_image`.   \n",
       "285                                                                                                         Nested quantization   \n",
       "\n",
       "     groundedness_score  relevance_score  standalone_score  \n",
       "0                   5.0              4.0               5.0  \n",
       "1                   2.0              4.0               5.0  \n",
       "2                   1.0              5.0               5.0  \n",
       "3                   4.0              4.0               5.0  \n",
       "4                   5.0              1.0               5.0  \n",
       "..                  ...              ...               ...  \n",
       "281                 NaN              NaN               NaN  \n",
       "282                 NaN              NaN               NaN  \n",
       "283                 NaN              NaN               NaN  \n",
       "284                 NaN              NaN               NaN  \n",
       "285                 NaN              NaN               NaN  \n",
       "\n",
       "[286 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA couples saved to ./generated_QAs/filtered_qa_couples.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Save outputs to a JSON file\n",
    "filtered_QA_couples_path = './generated_QAs/filtered_qa_couples.json'\n",
    "\n",
    "# this is the main function of actually filtering the QA couples\n",
    "# any scores that are less than a 4 in each category are filtered out\n",
    "def filter_questions(dataframe_QA_critiqued_outputs):\n",
    "    # Ensure input is a DataFrame\n",
    "    if not isinstance(dataframe_QA_critiqued_outputs, pd.DataFrame):\n",
    "        filtered_QA_questions = pd.DataFrame.from_dict(dataframe_QA_critiqued_outputs)\n",
    "    else:\n",
    "        filtered_QA_questions = dataframe_QA_critiqued_outputs.copy()\n",
    "\n",
    "    print(\"Evaluation dataset before filtering:\")\n",
    "    display(\n",
    "        filtered_QA_questions[\n",
    "            [\n",
    "                \"question\",\n",
    "                \"answer\",\n",
    "                \"groundedness_score\",\n",
    "                \"relevance_score\",\n",
    "                \"standalone_score\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_QA_questions = filtered_QA_questions.loc[\n",
    "        (filtered_QA_questions[\"groundedness_score\"] >= 4)\n",
    "        & (filtered_QA_questions[\"relevance_score\"] >= 4)\n",
    "        & (filtered_QA_questions[\"standalone_score\"] >= 4)\n",
    "    ]\n",
    "\n",
    "    # Ensure the target directory path exists\n",
    "    os.makedirs(os.path.dirname(filtered_QA_couples_path), exist_ok=True)\n",
    " \n",
    "    # Write the filtered QA couples to a JSON file\n",
    "    # Convert to list of dictionaries to ensure JSON serialization\n",
    "    filtered_QA_questions_json = filtered_QA_questions.to_dict(orient='records')\n",
    "    \n",
    "    with open(filtered_QA_couples_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(filtered_QA_questions_json, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Generated QA couples saved to {filtered_QA_couples_path}\")\n",
    "\n",
    "    return filtered_QA_questions\n",
    "\n",
    "# Check if file exists and has content\n",
    "if os.path.exists(filtered_QA_couples_path) and os.path.getsize(filtered_QA_couples_path) > 0:\n",
    "    try:\n",
    "        # Load JSON and convert directly to DataFrame\n",
    "        with open(filtered_QA_couples_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            filtered_QA_questions = pd.DataFrame(json.load(f))\n",
    "            print(f\"Successfully LOADED {len(filtered_QA_questions)} Filtered and Finalized QA couples\")\n",
    "    except (json.JSONDecodeError, OSError) as e:\n",
    "        print(f\"Error loading JSON file: {e}. Running QA Filtering on critiqued outputs.\")\n",
    "        filtered_QA_questions = filter_questions(QA_critiqued_outputs)\n",
    "else:\n",
    "    filtered_QA_questions = filter_questions(QA_critiqued_outputs)\n",
    "\n",
    "# Create dataset\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    filtered_QA_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the class for implementing a new pipeline?\\n</td>\n",
       "      <td>Implementing a new pipeline</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the paper that provides more detail and context for the AI Act?\\n</td>\n",
       "      <td>The name of the paper is \"supporting_OS_in_the_AIAct.pdf\".</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the functionality of the `dtype` parameter in the `T5ForConditionalGeneration.from_pretrained` method?\\n</td>\n",
       "      <td>The `dtype` parameter in the `T5ForConditionalGeneration.from_pretrained` method is used to specify the data type of the model's weights. It is only available for floating dtypes.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does Double DQN help reduce the overestimation of Q values?\\n</td>\n",
       "      <td>Double DQN helps reduce the overestimation of Q values by using two networks to decouple the action selection from the target Q value generation. The DQN network is used to select the best action to take for the next state, and the Target network is used to calculate the target Q value of taking that action at the next state.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the purpose of the `allowed_paths` parameter in `launch()` in Gradio 4.0?\\n</td>\n",
       "      <td>The `allowed_paths` parameter in `launch()` in Gradio 4.0 is used to explicitly allow access to local files that are referenced within CSS or in a `gr.HTML` component using the `/file=` route. This is necessary because the working directory is not served by default in Gradio 4.0.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the extra reward calculated by the advantage function?\\n</td>\n",
       "      <td>The extra reward is what's beyond the expected value of that state.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the class used to define the training arguments?\\n</td>\n",
       "      <td>The class used to define the training arguments is `TrainingArguments`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the template for single sentences in the code snippet?\\n</td>\n",
       "      <td>The template for single sentences in the code snippet is `\"[CLS] $A [SEP]\"` where `$A` represents the sentence.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the prompt template for the first turn in Llama 2?\\n</td>\n",
       "      <td>The prompt template for the first turn in Llama 2 is `[INST] &lt;&lt;SYS&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ user_message }}`. The `system_prompt` specifies the behavior of the chat assistant and the `user_message` is the text entered by the user.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the main difference between ViT and ConvNeXT for image classification?\\n</td>\n",
       "      <td>The main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the time complexity of the ProbSparse self-attention mechanism?\\n</td>\n",
       "      <td>The time complexity of the ProbSparse self-attention mechanism is O(L logL).</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>How to define a Textbox outside of the Blocks() scope in Gradio?\\n</td>\n",
       "      <td>Define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>How do you load a Gradio Blocks app as a regular python function?\\n</td>\n",
       "      <td>You can load a Gradio Blocks app as a regular python function by calling the `Blocks.load` class method in your source file.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>What type of model is best suited for summarizing texts?\\n</td>\n",
       "      <td>A sequence-to-sequence model is best suited for summarizing texts.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Which model was released by the BigScience Workshop?\\n</td>\n",
       "      <td>BLOOM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>What is the default name of the database used for storing the migrations history?\\n</td>\n",
       "      <td>The default name of the database used for storing the migrations history is `\"datasets_server_maintenance\"`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>What is the expected improvement in future versions of the tool?\\n</td>\n",
       "      <td>In future versions, users can expect tools to measure the \"usefulness\" of parameters to be able to optimize the sparsity pattern, and NVIDIA Ampere 50% sparse pattern within blocks will probably yield another significant performance gain.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What is the mechanism of naive pipeline parallelism?\\n</td>\n",
       "      <td>The mechanism of naive pipeline parallelism involves spreading groups of model layers across multiple GPUs and moving data along from GPU to GPU as if it were one large composite GPU. This is done by switching the desired layers to the desired devices and modifying the data to be on the same device as the layer when it goes in and out those layers.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>What is the name of the problem that occurs when the node representation includes more and more nodes at each new layer?\\n</td>\n",
       "      <td>The over-smoothing problem</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Which model was developed by Meta AI and introduced in the research paper \"LLaMA: Open and Efficient Foundation Language Models\"?\\n</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>How many languages can the fastText model detect?\\n</td>\n",
       "      <td>The fastText model can detect 217 languages.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>How do I display the logs with the Aim UI in my Space?\\n</td>\n",
       "      <td>You need to compress the `.aim` folder to a `tar.gz` file and upload it to your Space using `git` or the Files and Versions sections of your Space.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>How can I report a comment on Hugging Face?\\n</td>\n",
       "      <td>You can report a comment on Hugging Face by clicking the three dots at the top right of a comment. This will submit a request for the Hugging Face team to review the comment.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>How do I load a dataset with a local loading script?\\n</td>\n",
       "      <td>You can load a dataset with a local loading script by passing the path to the loading script to the `load_dataset` function. The path can be either a local file path or a URL. For example, `load_dataset(\"path/to/local/loading/script.py\")` or `load_dataset(\"https://example.com/loading_script.py\")`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>What is the probability of whole word masking in the given context?\\n</td>\n",
       "      <td>The probability of whole word masking is 0.2.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>What are the possible values for the `block_type` argument in the `ResnetConfig` class?\\n</td>\n",
       "      <td>The possible values for the `block_type` argument in the `ResnetConfig` class are 'basic' and 'bottleneck'.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>What is the latest version of gradio?\\n</td>\n",
       "      <td>The latest version of gradio is 0.3.1.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is the name of the last markdown editor?\\n</td>\n",
       "      <td>Dillinger</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>How can I fine-tune a model on text classification?\\n</td>\n",
       "      <td>You can fine-tune a model on text classification by following the instructions in the notebook \"How to fine-tune a model on text classification\" available at &lt;https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb&gt;.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What is the format of the logs.csv file for the calculator example?\\n</td>\n",
       "      <td>The logs.csv file for the calculator example contains the columns 'num1', 'operation', 'num2', 'Output', and 'timestamp'.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>What is the name of the class for RoCBert's causal language model?\\n</td>\n",
       "      <td>RoCBertForCausalLM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>What is the task of text-to-video models?\\n</td>\n",
       "      <td>The task of text-to-video models is to generate a sequence of images from text descriptions that are both temporally and spatially consistent.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>What is the problem with RNN-based encoder-decoder models?\\n</td>\n",
       "      <td>RNN-based encoder-decoder models suffer from the vanishing gradient problem, making it very difficult to capture long-range dependencies, and the inherent recurrent architecture of RNNs prevents efficient parallelization when encoding.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>What is the main function of the pre-tokenization step in the tokenization pipeline?\\n</td>\n",
       "      <td>The main function of the pre-tokenization step in the tokenization pipeline is to apply rules that do not need to be learned to perform a first division of the text.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>How many unique datasets does the Datasets library include?\\n</td>\n",
       "      <td>The Datasets library includes more than 650 unique datasets.\\n```</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>What is the accuracy of the model \"lvwerra/distilbert-imdb\" on the IMDB dataset?\\n</td>\n",
       "      <td>The accuracy of the model \"lvwerra/distilbert-imdb\" on the IMDB dataset is 0.918.\\n```</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Which model was released by EleutherAI with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model?\\n</td>\n",
       "      <td>GPT NeoX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>What is one feature of Enterprise Hub?\\n</td>\n",
       "      <td>One feature of Enterprise Hub is SSO (Single Sign-On).</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>What is the type of the `value` property in the `BaseExample` component?\\n</td>\n",
       "      <td>The type of the `value` property in the `BaseExample` component is `string`.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>What is the default threshold value for 8-bit quantization?\\n</td>\n",
       "      <td>A good default threshold value is 6.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Who should I report instances of abusive, harassing, or otherwise unacceptable behavior to?\\n</td>\n",
       "      <td>You should report instances of abusive, harassing, or otherwise unacceptable behavior to the community leaders responsible for enforcement at feedback@huggingface.co.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>What are the supported audio extensions in the folder-based builder?\\n</td>\n",
       "      <td>The supported audio extensions in the folder-based builder are wav, mp3, and others as listed [here](https://github.com/huggingface/datasets/blob/b5672a956d5de864e6f5550e493527d962d6ae55/src/datasets/packaged_modules/audiofolder/audiofolder.py#L39).</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>What should not be changed in the code?\\n</td>\n",
       "      <td>The input arguments `\"--model_id\"`, `\"--dataset\"`, `\"--config\"`, `\"--split\"` and the function `def log_results(result: Dataset, args: Dict[str, str])` should not be changed in the code. Additionally, the code written under `if __name__ == \"__main__\":` should also not be changed. Participants are invited to change the `def normalize_text(text: str) -&gt; str:` function.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>How can I find fastai models on the Hugging Face model hub?\\n</td>\n",
       "      <td>You can find fastai models by filtering at the left of the [models page](https://huggingface.co/models?library=fastai&amp;sort=downloads) on the Hugging Face model hub.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>What is the purpose of the Private Hub?\\n</td>\n",
       "      <td>The Private Hub allows companies to use Hugging Face‚Äôs complete ecosystem in their own private and compliant environment to accelerate their machine learning development. It brings ML tools for every step of the ML lifecycle together in one place to make collaborating in ML simpler and more productive, while having a compliant environment that companies need for building ML securely.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>What is the first step in the 3D Gaussian Splatting procedure?\\n</td>\n",
       "      <td>The first step in the 3D Gaussian Splatting procedure is to use the Structure from Motion (SfM) method to estimate a point cloud from a set of images.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>How do I export the Stable Diffusion XL model to ONNX format?\\n</td>\n",
       "      <td>Use the `optimum-cli export onnx` command and specify the model and task as `stabilityai/stable-diffusion-xl-base-1.0` and `stable-diffusion-xl`, respectively. The exported model will be saved in the specified directory.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>What is the task of the PerceiverForOpticalFlow model?\\n</td>\n",
       "      <td>The PerceiverForOpticalFlow model estimates the 2D displacement for each pixel in the first image, given two images of the same scene.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>What is the task of predicting a caption for a given image called?\\n</td>\n",
       "      <td>Image captioning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>What is the name of the JavaScript library that allows running Transformers in the browser?\\n</td>\n",
       "      <td>Transformers.js</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>What is the official metric to evaluate models on the WikiSplit dataset?\\n</td>\n",
       "      <td>The official metric to evaluate models on the WikiSplit dataset is not specified in the given context. The context only mentions that the metric used in the given code snippet is not the official metric.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>What is the key insight of the MaskFormer model?\\n</td>\n",
       "      <td>The key insight of the MaskFormer model is that mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>How can I use my custom dataset with the datasets package offline?\\n</td>\n",
       "      <td>You can use the `load_dataset` function to load your custom dataset script, which should be located in a python file. For example, if your dataset script is located at `./my_dataset/my_dataset.py`, you can load it using `load_dataset(\"./my_dataset\")`. This will generate your dataset once and for all.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>What should be included in the metadata file?\\n</td>\n",
       "      <td>The metadata file should include a `file_name` column to link an audio file to its metadata.\\n\\n&lt;/Tip&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Who should I report instances of abusive, harassing, or otherwise unacceptable behavior to?\\n</td>\n",
       "      <td>You should report instances of abusive, harassing, or otherwise unacceptable behavior to the community leaders responsible for enforcement at feedback@huggingface.co.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>What is the backend model for HuggingChat?\\n</td>\n",
       "      <td>The backend model for HuggingChat is one of OpenAssistant's models.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>What is the method in charge of defining the dataset attributes in `DatasetBuilder`?\\n</td>\n",
       "      <td>[`DatasetBuilder._info`]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>What is the name of the service that allows you to run accelerated inference on Hugging Face's infrastructure for free?\\n</td>\n",
       "      <td>The Inference API</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>What is the number of parameters in the ssl\\_resnet50 model?\\n</td>\n",
       "      <td>25560000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                question  \\\n",
       "0                                                                       What is the name of the class for implementing a new pipeline?\\n   \n",
       "3                                                  What is the name of the paper that provides more detail and context for the AI Act?\\n   \n",
       "7                       What is the functionality of the `dtype` parameter in the `T5ForConditionalGeneration.from_pretrained` method?\\n   \n",
       "8                                                                      How does Double DQN help reduce the overestimation of Q values?\\n   \n",
       "9                                                    What is the purpose of the `allowed_paths` parameter in `launch()` in Gradio 4.0?\\n   \n",
       "11                                                                      What is the extra reward calculated by the advantage function?\\n   \n",
       "14                                                                            What is the class used to define the training arguments?\\n   \n",
       "15                                                                      What is the template for single sentences in the code snippet?\\n   \n",
       "18                                                                          What is the prompt template for the first turn in Llama 2?\\n   \n",
       "26                                                      What is the main difference between ViT and ConvNeXT for image classification?\\n   \n",
       "28                                                             What is the time complexity of the ProbSparse self-attention mechanism?\\n   \n",
       "40                                                                    How to define a Textbox outside of the Blocks() scope in Gradio?\\n   \n",
       "46                                                                   How do you load a Gradio Blocks app as a regular python function?\\n   \n",
       "47                                                                            What type of model is best suited for summarizing texts?\\n   \n",
       "50                                                                                Which model was released by the BigScience Workshop?\\n   \n",
       "54                                                   What is the default name of the database used for storing the migrations history?\\n   \n",
       "59                                                                    What is the expected improvement in future versions of the tool?\\n   \n",
       "65                                                                                What is the mechanism of naive pipeline parallelism?\\n   \n",
       "67            What is the name of the problem that occurs when the node representation includes more and more nodes at each new layer?\\n   \n",
       "73   Which model was developed by Meta AI and introduced in the research paper \"LLaMA: Open and Efficient Foundation Language Models\"?\\n   \n",
       "75                                                                                   How many languages can the fastText model detect?\\n   \n",
       "80                                                                              How do I display the logs with the Aim UI in my Space?\\n   \n",
       "82                                                                                         How can I report a comment on Hugging Face?\\n   \n",
       "84                                                                                How do I load a dataset with a local loading script?\\n   \n",
       "89                                                                 What is the probability of whole word masking in the given context?\\n   \n",
       "90                                             What are the possible values for the `block_type` argument in the `ResnetConfig` class?\\n   \n",
       "93                                                                                               What is the latest version of gradio?\\n   \n",
       "97                                                                                       What is the name of the last markdown editor?\\n   \n",
       "105                                                                                How can I fine-tune a model on text classification?\\n   \n",
       "109                                                                What is the format of the logs.csv file for the calculator example?\\n   \n",
       "110                                                                 What is the name of the class for RoCBert's causal language model?\\n   \n",
       "111                                                                                          What is the task of text-to-video models?\\n   \n",
       "123                                                                         What is the problem with RNN-based encoder-decoder models?\\n   \n",
       "127                                               What is the main function of the pre-tokenization step in the tokenization pipeline?\\n   \n",
       "132                                                                        How many unique datasets does the Datasets library include?\\n   \n",
       "136                                                   What is the accuracy of the model \"lvwerra/distilbert-imdb\" on the IMDB dataset?\\n   \n",
       "137                  Which model was released by EleutherAI with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model?\\n   \n",
       "145                                                                                             What is one feature of Enterprise Hub?\\n   \n",
       "155                                                           What is the type of the `value` property in the `BaseExample` component?\\n   \n",
       "157                                                                        What is the default threshold value for 8-bit quantization?\\n   \n",
       "166                                        Who should I report instances of abusive, harassing, or otherwise unacceptable behavior to?\\n   \n",
       "175                                                               What are the supported audio extensions in the folder-based builder?\\n   \n",
       "176                                                                                            What should not be changed in the code?\\n   \n",
       "177                                                                        How can I find fastai models on the Hugging Face model hub?\\n   \n",
       "182                                                                                            What is the purpose of the Private Hub?\\n   \n",
       "186                                                                     What is the first step in the 3D Gaussian Splatting procedure?\\n   \n",
       "196                                                                      How do I export the Stable Diffusion XL model to ONNX format?\\n   \n",
       "197                                                                             What is the task of the PerceiverForOpticalFlow model?\\n   \n",
       "198                                                                 What is the task of predicting a caption for a given image called?\\n   \n",
       "199                                        What is the name of the JavaScript library that allows running Transformers in the browser?\\n   \n",
       "200                                                           What is the official metric to evaluate models on the WikiSplit dataset?\\n   \n",
       "207                                                                                   What is the key insight of the MaskFormer model?\\n   \n",
       "208                                                                 How can I use my custom dataset with the datasets package offline?\\n   \n",
       "210                                                                                      What should be included in the metadata file?\\n   \n",
       "213                                        Who should I report instances of abusive, harassing, or otherwise unacceptable behavior to?\\n   \n",
       "217                                                                                         What is the backend model for HuggingChat?\\n   \n",
       "222                                               What is the method in charge of defining the dataset attributes in `DatasetBuilder`?\\n   \n",
       "230            What is the name of the service that allows you to run accelerated inference on Hugging Face's infrastructure for free?\\n   \n",
       "231                                                                       What is the number of parameters in the ssl\\_resnet50 model?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                 answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Implementing a new pipeline   \n",
       "3                                                                                                                                                                                                                                                                                                                                            The name of the paper is \"supporting_OS_in_the_AIAct.pdf\".   \n",
       "7                                                                                                                                                                                                                   The `dtype` parameter in the `T5ForConditionalGeneration.from_pretrained` method is used to specify the data type of the model's weights. It is only available for floating dtypes.   \n",
       "8                                                               Double DQN helps reduce the overestimation of Q values by using two networks to decouple the action selection from the target Q value generation. The DQN network is used to select the best action to take for the next state, and the Target network is used to calculate the target Q value of taking that action at the next state.   \n",
       "9                                                                                                              The `allowed_paths` parameter in `launch()` in Gradio 4.0 is used to explicitly allow access to local files that are referenced within CSS or in a `gr.HTML` component using the `/file=` route. This is necessary because the working directory is not served by default in Gradio 4.0.   \n",
       "11                                                                                                                                                                                                                                                                                                                                  The extra reward is what's beyond the expected value of that state.   \n",
       "14                                                                                                                                                                                                                                                                                                                              The class used to define the training arguments is `TrainingArguments`.   \n",
       "15                                                                                                                                                                                                                                                                                      The template for single sentences in the code snippet is `\"[CLS] $A [SEP]\"` where `$A` represents the sentence.   \n",
       "18                                                                                                                                                      The prompt template for the first turn in Llama 2 is `[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }}`. The `system_prompt` specifies the behavior of the chat assistant and the `user_message` is the text entered by the user.   \n",
       "26                                                                                                                                                                                                                                                                                                        The main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.   \n",
       "28                                                                                                                                                                                                                                                                                                                         The time complexity of the ProbSparse self-attention mechanism is O(L logL).   \n",
       "40                                                                                                                                                                                                                                                       Define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.   \n",
       "46                                                                                                                                                                                                                                                                         You can load a Gradio Blocks app as a regular python function by calling the `Blocks.load` class method in your source file.   \n",
       "47                                                                                                                                                                                                                                                                                                                                   A sequence-to-sequence model is best suited for summarizing texts.   \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                BLOOM   \n",
       "54                                                                                                                                                                                                                                                                                         The default name of the database used for storing the migrations history is `\"datasets_server_maintenance\"`.   \n",
       "59                                                                                                                                                       In future versions, users can expect tools to measure the \"usefulness\" of parameters to be able to optimize the sparsity pattern, and NVIDIA Ampere 50% sparse pattern within blocks will probably yield another significant performance gain.   \n",
       "65                                       The mechanism of naive pipeline parallelism involves spreading groups of model layers across multiple GPUs and moving data along from GPU to GPU as if it were one large composite GPU. This is done by switching the desired layers to the desired devices and modifying the data to be on the same device as the layer when it goes in and out those layers.   \n",
       "67                                                                                                                                                                                                                                                                                                                                                                           The over-smoothing problem   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                LLaMA   \n",
       "75                                                                                                                                                                                                                                                                                                                                                         The fastText model can detect 217 languages.   \n",
       "80                                                                                                                                                                                                                                                  You need to compress the `.aim` folder to a `tar.gz` file and upload it to your Space using `git` or the Files and Versions sections of your Space.   \n",
       "82                                                                                                                                                                                                                       You can report a comment on Hugging Face by clicking the three dots at the top right of a comment. This will submit a request for the Hugging Face team to review the comment.   \n",
       "84                                                                                           You can load a dataset with a local loading script by passing the path to the loading script to the `load_dataset` function. The path can be either a local file path or a URL. For example, `load_dataset(\"path/to/local/loading/script.py\")` or `load_dataset(\"https://example.com/loading_script.py\")`.   \n",
       "89                                                                                                                                                                                                                                                                                                                                                        The probability of whole word masking is 0.2.   \n",
       "90                                                                                                                                                                                                                                                                                          The possible values for the `block_type` argument in the `ResnetConfig` class are 'basic' and 'bottleneck'.   \n",
       "93                                                                                                                                                                                                                                                                                                                                                               The latest version of gradio is 0.3.1.   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                            Dillinger   \n",
       "105                                                                                                                                           You can fine-tune a model on text classification by following the instructions in the notebook \"How to fine-tune a model on text classification\" available at <https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb>.   \n",
       "109                                                                                                                                                                                                                                                                           The logs.csv file for the calculator example contains the columns 'num1', 'operation', 'num2', 'Output', and 'timestamp'.   \n",
       "110                                                                                                                                                                                                                                                                                                                                                                                  RoCBertForCausalLM   \n",
       "111                                                                                                                                                                                                                                                      The task of text-to-video models is to generate a sequence of images from text descriptions that are both temporally and spatially consistent.   \n",
       "123                                                                                                                                                         RNN-based encoder-decoder models suffer from the vanishing gradient problem, making it very difficult to capture long-range dependencies, and the inherent recurrent architecture of RNNs prevents efficient parallelization when encoding.   \n",
       "127                                                                                                                                                                                                                               The main function of the pre-tokenization step in the tokenization pipeline is to apply rules that do not need to be learned to perform a first division of the text.   \n",
       "132                                                                                                                                                                                                                                                                                                                                   The Datasets library includes more than 650 unique datasets.\\n```   \n",
       "136                                                                                                                                                                                                                                                                                                              The accuracy of the model \"lvwerra/distilbert-imdb\" on the IMDB dataset is 0.918.\\n```   \n",
       "137                                                                                                                                                                                                                                                                                                                                                                                            GPT NeoX   \n",
       "145                                                                                                                                                                                                                                                                                                                                              One feature of Enterprise Hub is SSO (Single Sign-On).   \n",
       "155                                                                                                                                                                                                                                                                                                                        The type of the `value` property in the `BaseExample` component is `string`.   \n",
       "157                                                                                                                                                                                                                                                                                                                                                                A good default threshold value is 6.   \n",
       "166                                                                                                                                                                                                                              You should report instances of abusive, harassing, or otherwise unacceptable behavior to the community leaders responsible for enforcement at feedback@huggingface.co.   \n",
       "175                                                                                                                                           The supported audio extensions in the folder-based builder are wav, mp3, and others as listed [here](https://github.com/huggingface/datasets/blob/b5672a956d5de864e6f5550e493527d962d6ae55/src/datasets/packaged_modules/audiofolder/audiofolder.py#L39).   \n",
       "176                    The input arguments `\"--model_id\"`, `\"--dataset\"`, `\"--config\"`, `\"--split\"` and the function `def log_results(result: Dataset, args: Dict[str, str])` should not be changed in the code. Additionally, the code written under `if __name__ == \"__main__\":` should also not be changed. Participants are invited to change the `def normalize_text(text: str) -> str:` function.   \n",
       "177                                                                                                                                                                                                                                You can find fastai models by filtering at the left of the [models page](https://huggingface.co/models?library=fastai&sort=downloads) on the Hugging Face model hub.   \n",
       "182  The Private Hub allows companies to use Hugging Face‚Äôs complete ecosystem in their own private and compliant environment to accelerate their machine learning development. It brings ML tools for every step of the ML lifecycle together in one place to make collaborating in ML simpler and more productive, while having a compliant environment that companies need for building ML securely.   \n",
       "186                                                                                                                                                                                                                                              The first step in the 3D Gaussian Splatting procedure is to use the Structure from Motion (SfM) method to estimate a point cloud from a set of images.   \n",
       "196                                                                                                                                                                        Use the `optimum-cli export onnx` command and specify the model and task as `stabilityai/stable-diffusion-xl-base-1.0` and `stable-diffusion-xl`, respectively. The exported model will be saved in the specified directory.   \n",
       "197                                                                                                                                                                                                                                                              The PerceiverForOpticalFlow model estimates the 2D displacement for each pixel in the first image, given two images of the same scene.   \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                    Image captioning   \n",
       "199                                                                                                                                                                                                                                                                                                                                                                                     Transformers.js   \n",
       "200                                                                                                                                                                                         The official metric to evaluate models on the WikiSplit dataset is not specified in the given context. The context only mentions that the metric used in the given code snippet is not the official metric.   \n",
       "207                                                                                                                                                             The key insight of the MaskFormer model is that mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure.   \n",
       "208                                                                                       You can use the `load_dataset` function to load your custom dataset script, which should be located in a python file. For example, if your dataset script is located at `./my_dataset/my_dataset.py`, you can load it using `load_dataset(\"./my_dataset\")`. This will generate your dataset once and for all.   \n",
       "210                                                                                                                                                                                                                                                                                              The metadata file should include a `file_name` column to link an audio file to its metadata.\\n\\n</Tip>   \n",
       "213                                                                                                                                                                                                                              You should report instances of abusive, harassing, or otherwise unacceptable behavior to the community leaders responsible for enforcement at feedback@huggingface.co.   \n",
       "217                                                                                                                                                                                                                                                                                                                                 The backend model for HuggingChat is one of OpenAssistant's models.   \n",
       "222                                                                                                                                                                                                                                                                                                                                                                            [`DatasetBuilder._info`]   \n",
       "230                                                                                                                                                                                                                                                                                                                                                                                   The Inference API   \n",
       "231                                                                                                                                                                                                                                                                                                                                                                                            25560000   \n",
       "\n",
       "     groundedness_score  relevance_score  standalone_score  \n",
       "0                   5.0              4.0               5.0  \n",
       "3                   4.0              4.0               5.0  \n",
       "7                   5.0              4.0               5.0  \n",
       "8                   5.0              5.0               5.0  \n",
       "9                   5.0              4.0               5.0  \n",
       "11                  5.0              5.0               5.0  \n",
       "14                  5.0              5.0               5.0  \n",
       "15                  5.0              4.0               5.0  \n",
       "18                  5.0              4.0               5.0  \n",
       "26                  5.0              4.0               5.0  \n",
       "28                  5.0              4.0               5.0  \n",
       "40                  5.0              4.0               5.0  \n",
       "46                  5.0              4.0               5.0  \n",
       "47                  4.0              5.0               5.0  \n",
       "50                  5.0              4.0               5.0  \n",
       "54                  5.0              4.0               5.0  \n",
       "59                  5.0              4.0               5.0  \n",
       "65                  4.0              4.0               5.0  \n",
       "67                  4.0              4.0               5.0  \n",
       "73                  5.0              4.0               5.0  \n",
       "75                  5.0              4.0               5.0  \n",
       "80                  5.0              4.0               5.0  \n",
       "82                  5.0              4.0               5.0  \n",
       "84                  5.0              5.0               5.0  \n",
       "89                  5.0              4.0               4.0  \n",
       "90                  5.0              4.0               5.0  \n",
       "93                  5.0              4.0               5.0  \n",
       "97                  5.0              5.0               5.0  \n",
       "105                 5.0              5.0               5.0  \n",
       "109                 5.0              4.0               5.0  \n",
       "110                 5.0              4.0               5.0  \n",
       "111                 5.0              5.0               5.0  \n",
       "123                 5.0              4.0               5.0  \n",
       "127                 5.0              5.0               5.0  \n",
       "132                 5.0              4.0               5.0  \n",
       "136                 5.0              4.0               5.0  \n",
       "137                 5.0              4.0               5.0  \n",
       "145                 5.0              4.0               5.0  \n",
       "155                 4.0              5.0               5.0  \n",
       "157                 5.0              4.0               5.0  \n",
       "166                 5.0              5.0               5.0  \n",
       "175                 5.0              4.0               5.0  \n",
       "176                 5.0              4.0               4.0  \n",
       "177                 5.0              4.0               5.0  \n",
       "182                 5.0              5.0               5.0  \n",
       "186                 5.0              4.0               5.0  \n",
       "196                 5.0              5.0               5.0  \n",
       "197                 5.0              4.0               5.0  \n",
       "198                 5.0              4.0               5.0  \n",
       "199                 5.0              5.0               5.0  \n",
       "200                 5.0              5.0               4.0  \n",
       "207                 5.0              5.0               5.0  \n",
       "208                 5.0              5.0               5.0  \n",
       "210                 5.0              5.0               4.0  \n",
       "213                 5.0              5.0               5.0  \n",
       "217                 5.0              4.0               5.0  \n",
       "222                 5.0              5.0               5.0  \n",
       "230                 5.0              5.0               5.0  \n",
       "231                 5.0              4.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we print the final evaluation dataset\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    filtered_QA_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(len(filtered_QA_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaOMZyu69jVO"
   },
   "source": [
    "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "We have generated only a few QA couples here to reduce time and cost. But let's kick start the next part by loading a pre-generated dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5s19uTd9jVO"
   },
   "source": [
    "# 2. Build our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-mET8Dy9jVO"
   },
   "source": [
    "### 2.1. Preprocessing documents to build our vector database\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    "\n",
    "Many options exist for text splitting:\n",
    "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    "- split after `n` words / character, but only on sentence boundaries\n",
    "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    "\n",
    "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
    "\n",
    "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "üí° _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4fhm55Q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(data_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz9Jw2_q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzBYfNG79jVO"
   },
   "source": [
    "### 2.2. Retriever - embeddings üóÇÔ∏è\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "üõ†Ô∏è __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqJlIDZR9jVO"
   },
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6y1mQJX9jVO"
   },
   "source": [
    "### 2.3. Reader - LLM üí¨\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "üõ†Ô∏è Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PdpuWyP9jVP"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `HF_API_TOKEN = \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SDqenld9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ62CbcZ9jVP"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiygbqfT9jVP"
   },
   "source": [
    "# 3. Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    "\n",
    "To this end, __we setup a judge agent__. ‚öñÔ∏èü§ñ\n",
    "\n",
    "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
    "\n",
    "üí° _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
    "\n",
    "üí° _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrlMh_ZI9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae-3KWzK9jVP"
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE YOU CREATE A HUGGING FACE API KEY AND ADD IT BELOW IN `OPEN_AI_KEY = \"\"`\n",
    "\n",
    "- Other chat models can be loaded following the documentation for [langchain here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/quick_start/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia9Mvn859jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "eval_chat_model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", api_key=ANTHROPIC_API_KEY)\n",
    "evaluator_name = \"Claude-3-Sonnet\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXH-szLe9jVP"
   },
   "source": [
    "üöÄ Let's run the tests and evaluate answers!üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW2nnvUT9jVQ"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "                if rerank\n",
    "                else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tytXV5-h9jVT"
   },
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4YDSfmr9jVT"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "reader_judge_outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    reader_judge_outputs.append(output)\n",
    "result = pd.concat(reader_judge_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdkXMNvS9jVT"
   },
   "outputs": [],
   "source": [
    "# Remove rows with N/A values before processing\n",
    "result = result[result[\"eval_score_Claude-3-Sonnet\"].str.strip().str.lower() != \"n/a\"]\n",
    "\n",
    "# Filter out non-integer values between 1-5\n",
    "result = result[result[\"eval_score_Claude-3-Sonnet\"].str.contains(r'^[1-5]$')]\n",
    "\n",
    "# Convert to integer\n",
    "result[\"eval_score_Claude-3-Sonnet\"] = result[\"eval_score_Claude-3-Sonnet\"].astype(int)\n",
    "\n",
    "result[\"eval_score_Claude-3-Sonnet\"] = (result[\"eval_score_Claude-3-Sonnet\"] - 1) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgxBpid29jVT",
    "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
   },
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_Claude-3-Sonnet\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[[\"eval_score_Claude-3-Sonnet\"]].mean().reset_index()\n",
    "print(average_scores.columns)\n",
    "print(average_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Function to format the settings\n",
    "def format_settings(settings):\n",
    "    formatted = settings.replace(\"chunk:\", \"Chunk Size: \") \\\n",
    "                        .replace(\"embeddings:\", \"Embeddings: \") \\\n",
    "                        .replace(\"rerank:\", \"Rerank: \") \\\n",
    "                        .replace(\"reader-model:\", \"Reader Model: \") \\\n",
    "                        .replace(\"evaluator-model:\", \"Evaluator Model: \") \\\n",
    "                        .replace(\"~\", \"/\")  # Replace any special characters as needed\n",
    "    return formatted.replace(\"_\", \"<br>\")  # Replace underscores with line breaks for better formatting\n",
    "\n",
    "# Assuming 'average_scores' is a DataFrame that contains 'settings' and 'eval_score_GPT4'\n",
    "# Create a new column with formatted settings\n",
    "average_scores['formatted_settings'] = average_scores['settings'].apply(format_settings)\n",
    "\n",
    "# Multiply the scores by 100 to convert to percentage\n",
    "average_scores['eval_score_percentage'] = average_scores['eval_score_Claude-3-Sonnet'] * 100\n",
    "\n",
    "# Now use 'formatted_settings' in the plot\n",
    "fig = px.bar(\n",
    "    average_scores,\n",
    "    x='formatted_settings',  # Use the formatted settings for x-axis\n",
    "    y='eval_score_percentage',     # Use the percentage scores for y-axis\n",
    "    labels={\n",
    "        \"eval_score_percentage\": \"Accuracy\",  # Y-axis label\n",
    "        \"formatted_settings\": \"Configuration\",  # X-axis label\n",
    "    },\n",
    "    color='eval_score_percentage',  # Color based on the evaluation score\n",
    "    color_continuous_scale=\"bluered\",  # Color scale\n",
    ")\n",
    "\n",
    "# Update layout settings\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],  # Adjusting range to [0, 100] for percentage\n",
    "    title=\"<b>Accuracy of Different RAG Configurations for 70 Filtered Q/A Pairs evaluated by claude-3-sonnet-20240229</b>\",\n",
    "    xaxis_title=\"RAG Settings\",\n",
    "    font=dict(size=11),\n",
    ")\n",
    "\n",
    "# Add percentage suffix to the y-axis\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)  # Hide the color scale\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}%\", textposition=\"outside\")  # Display percentage on bars\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSPH9DYI9jVT"
   },
   "source": [
    "## Example results\n",
    "\n",
    "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
    "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
    "\n",
    "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
    "\n",
    "‚û°Ô∏è ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVOxatv99jVT"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqK0Dg2Q9jVT"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPUOMWGk9jVT"
   },
   "source": [
    "\n",
    "\n",
    "As you can see, these had varying impact on performance. In particular, tuning the chunk size is both easy and very impactful.\n",
    "\n",
    "But this is our case: your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options! üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
